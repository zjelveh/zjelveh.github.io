{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0c34f-a94d-4bd2-8481-477d0a10f3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import tenacity\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import ast # Keep for potential fallback parsing if needed\n",
    "\n",
    "# --- Configuration for Deductive Coding ---\n",
    "\n",
    "# Define models TO USE FOR THE CODING TASK\n",
    "# The script will iterate through these models to code each abstract\n",
    "CODING_MODELS = {\n",
    "    \"llama-3.3-70b-versatile\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_name\": \"llama-3.3-70b-versatile\",\n",
    "        \"max_tokens\": 8000, # Max tokens for the coding response\n",
    "        \"temperature\": 0.1, # Low temperature for consistent coding\n",
    "    },\n",
    "    \"llama-3.1-8b-instant\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_name\": \"llama-3.1-8b-instant\",\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "    # Add other models intended for coding here\n",
    "}\n",
    "\n",
    "# API configuration (using keys for the CODING models)\n",
    "# Ensure the key here has access to the models defined in CODING_MODELS\n",
    "API_CONFIG = {\n",
    "    \"groq\": {\n",
    "        \"api_key\": os.environ.get(\"GROQ_API_KEY_CODING\") or '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', # Use a dedicated key if needed\n",
    "        \"api_url\": \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    },\n",
    "    # Add configs for other providers if used in CODING_MODELS\n",
    "}\n",
    "\n",
    "# Input file from the synthetic generation script\n",
    "INPUT_ABSTRACTS_FILE = \"synthetic_abstracts_multi_model.feather\"\n",
    "\n",
    "# Output file for coding results\n",
    "OUTPUT_CODING_RESULTS_FILE = \"deductive_coding_results_multi_model.json\"\n",
    "\n",
    "# Max samples to process (set to None to process all)\n",
    "MAX_SAMPLES_TO_CODE = None # e.g., 100 for testing, None for all\n",
    "\n",
    "# --- Definitions from Synthetic Data Generation (for Codebook) ---\n",
    "# Ensure these match the definitions used during generation\n",
    "TOPICS = {\n",
    "    \"T1\": {\n",
    "        \"name\": \"Machine Learning\",\n",
    "        \"subtopics\": [\"Neural Networks\", \"Reinforcement Learning\", \"Supervised Learning\", \"Unsupervised Learning\", \"Transfer Learning\"],\n",
    "        \"description\": \"Machine Learning involves developing algorithms and models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed.\"\n",
    "    },\n",
    "    \"T7\": {\n",
    "        \"name\": \"Sustainable Development\",\n",
    "        \"subtopics\": [\"Renewable Energy\", \"Climate Change Mitigation\", \"Resource Management\", \"Environmental Monitoring\", \"Sustainable Cities\"],\n",
    "        \"description\": \"Sustainable Development focuses on meeting present needs without compromising future generations, balancing economic growth, environmental protection, and social equity.\"\n",
    "    },\n",
    "    \"T8\": {\n",
    "        \"name\": \"Behavioral Economics\",\n",
    "        \"subtopics\": [\"Decision Making\", \"Cognitive Biases\", \"Risk Assessment\", \"Social Preferences\", \"Intertemporal Choice\"],\n",
    "        \"description\": \"Behavioral Economics studies how psychological, social, cognitive, and emotional factors influence economic decisions, challenging the assumption of perfect rationality.\"\n",
    "    },\n",
    "    \"T9\": {\n",
    "        \"name\": \"Digital Security\",\n",
    "        \"subtopics\": [\"Cybersecurity\", \"Privacy Enhancing Technologies\", \"Authentication Methods\", \"Threat Detection\", \"Security Policy\"],\n",
    "        \"description\": \"Digital Security encompasses technologies, protocols, and practices designed to protect computers, networks, programs, and data from attacks, damage, or unauthorized access.\"\n",
    "    },\n",
    "    \"T10\": {\n",
    "        \"name\": \"Public Health\",\n",
    "        \"subtopics\": [\"Epidemiology\", \"Health Promotion\", \"Disease Prevention\", \"Health Equity\", \"Health Systems\"],\n",
    "        \"description\": \"Public Health focuses on protecting and improving health at the population level through organized efforts, education, policies, and research.\"\n",
    "    }\n",
    "}\n",
    "# Note: DOMAINS list is not directly used in the codebook creation here,\n",
    "# but the domain value is extracted from diversity_params later.\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def create_codebook():\n",
    "    \"\"\"Create a structured codebook for deductive coding based on TOPICS\"\"\"\n",
    "    codebook = {\n",
    "        \"topics\": {k: {\n",
    "            \"name\": v[\"name\"],\n",
    "            \"id\": k,\n",
    "            \"subtopics\": v[\"subtopics\"],\n",
    "            \"description\": v[\"description\"]\n",
    "        } for k, v in TOPICS.items()},\n",
    "    }\n",
    "\n",
    "    # Add disambiguation guidelines (same as before)\n",
    "    codebook[\"disambiguation_guidelines\"] = \"\"\"\n",
    "        When coding abstracts, carefully distinguish between topics and between topics and subtopics:\n",
    "\n",
    "    TOPICS refer to the academic fields, subjects, or methodologies that form the theoretical or methodological foundation of the research. They answer \"what knowledge area is being studied or applied?\"\n",
    "\n",
    "    SUBTOPICS refer a finer-grained version of the relevant TOPIC in the abstract\n",
    "\n",
    "    For example, an abstract might describe using Machine Learning (TOPIC) to performed supervised learning (SUBTOPIC).\n",
    "    Here, Machine Learning is the broad academic subject being discussed and supervised learning is the more fine-graied topic being discussed.\n",
    "\n",
    "    Evidence for topics typically includes:\n",
    "    - Specific methodologies, theories, or frameworks from that academic field\n",
    "    - Technical terminology associated with the discipline\n",
    "    - Citations or references to literature in that field\n",
    "\n",
    "\n",
    "    Evidence for subtopics typically includes:\n",
    "    - Evidence of less granular topic\n",
    "    - Specific methodologies, theories, or frameworks from that academic field\n",
    "    - Technical terminology associated with the discipline\n",
    "    - Citations or references to literature in that field\n",
    "\n",
    "    Be aware that sometimes terminology can overlap.\n",
    "    \"\"\"\n",
    "    return codebook\n",
    "\n",
    "def create_deductive_coding_prompt(codebook, abstract_text):\n",
    "    \"\"\"Create a prompt for deductive coding given the codebook and abstract text.\"\"\"\n",
    "    # Prepare JSON representations of codebook parts for the prompt\n",
    "    topics_json = json.dumps([{\"id\": t_id, \"name\": t_info[\"name\"], \"description\": t_info[\"description\"], \"subtopics\": t_info[\"subtopics\"]}\n",
    "                              for t_id, t_info in codebook[\"topics\"].items()], indent=2)\n",
    "\n",
    "    # Construct the prompt using an f-string\n",
    "    # IMPORTANT: Use double curly braces `{{` and `}}` to escape literal braces\n",
    "    # needed for the example JSON format within the f-string.\n",
    "    prompt = f\"\"\"\n",
    "You are a highly skilled research methodologist performing deductive coding on academic abstracts. Your task is to analyze the following abstract and systematically identify both TOPICS and SUBTOPICS based on a predefined codebook.\n",
    "\n",
    "# CODEBOOK\n",
    "\n",
    "## Topics\n",
    "```json\n",
    "{topics_json}\n",
    "```\n",
    "\n",
    "## DISAMBIGUATION GUIDELINES\n",
    "{codebook[\"disambiguation_guidelines\"]}\n",
    "\n",
    "# CODING TASK\n",
    "\n",
    "Analyze the following abstract:\n",
    "\n",
    "---\n",
    "{abstract_text}\n",
    "---\n",
    "\n",
    "Perform the following analysis:\n",
    "\n",
    "1.  **TOPIC IDENTIFICATION**:\n",
    "    * Identify ALL topics (from the codebook) that are present in the abstract.\n",
    "    * For EACH identified topic, estimate the proportion (percentage) of the abstract text devoted to it. Proportions must sum to 100%. If a topic is present but minor, assign a small percentage (e.g., 5%). If no topics are identifiable, return an empty list for \"topics\".\n",
    "    * Rate your confidence for EACH topic identification on a scale of 1 (low) to 5 (high).\n",
    "\n",
    "2.  **SUBTOPIC IDENTIFICATION**:\n",
    "    * Identify ONE subtopic per identified topic (from the codebook topics' subtopic lists) that are represented in the abstract. List the subtopic name and the ID of the parent topic it belongs to.\n",
    "    * Rate your confidence for EACH subtopic identification on a scale of 1 (low) to 5 (high). If no subtopics are identifiable, return an empty list for \"subtopics\".\n",
    "\n",
    "3.  **DISAMBIGUATION EXPLANATION**:\n",
    "    * Briefly explain your reasoning for distinguishing between the identified topics and subtopics, especially in potentially ambiguous cases, referencing the disambiguation guidelines. If only one topic/subtopic is found, state that. If none are found, state that.\n",
    "\n",
    "Return your analysis **ONLY** in the following valid JSON format. Do **NOT** include any text outside this JSON structure, comments, or markdown formatting (like ```json).\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"topics\": [\n",
    "    {{\n",
    "      \"topic_id\": \"topic_identifier_from_codebook\",\n",
    "      \"topic_name\": \"topic_name_from_codebook\",\n",
    "      \"proportion\": <percentage_as_number_0_to_100>,\n",
    "      \"confidence\": <rating_from_1_to_5>\n",
    "    }}\n",
    "  ],\n",
    "  \"subtopics\": [\n",
    "    {{\n",
    "      \"parent_topic_id\": \"topic_identifier_from_codebook\",\n",
    "      \"subtopic_name\": \"subtopic_name_from_codebook\",\n",
    "      \"confidence\": <rating_from_1_to_5>\n",
    "    }}\n",
    "  ],\n",
    "  \"disambiguation_explanation\": \"Your explanation here, referencing the guidelines.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Important Instructions:**\n",
    "* Base your analysis STRICTLY on the provided abstract text. Do not infer information not present.\n",
    "* Ensure the topic proportions sum to 100%.\n",
    "* If you cannot identify any topics or subtopics, return empty lists `[]` for the respective keys.\n",
    "* Provide a concise `disambiguation_explanation`.\n",
    "* Output **only** the valid JSON object.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# API request handlers with retry logic\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "       retry=retry_if_exception_type((requests.RequestException, json.JSONDecodeError)))\n",
    "def query_groq_api(prompt, model_config, api_key, api_url):\n",
    "    \"\"\"Send a request to Groq API with retry logic\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_config[\"model_name\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": model_config[\"max_tokens\"],\n",
    "        \"temperature\": model_config[\"temperature\"],\n",
    "        \"response_format\": {\"type\": \"json_object\"} # Request JSON output directly\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload, timeout=90) # Increased timeout\n",
    "    response.raise_for_status() # Raise an exception for 4XX/5XX responses\n",
    "\n",
    "    result = response.json()\n",
    "    if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "        # Check if content is already a JSON object due to response_format\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Try parsing, as sometimes it might still be a string despite the request\n",
    "        try:\n",
    "            # If it's already parsed by requests, it might be a dict\n",
    "            if isinstance(content, dict):\n",
    "                return json.dumps(content) # Return as JSON string for consistency\n",
    "            else:\n",
    "                 # Attempt to parse if it's a string\n",
    "                 parsed_content = json.loads(content)\n",
    "                 return json.dumps(parsed_content) # Return as JSON string\n",
    "        except json.JSONDecodeError:\n",
    "             # If parsing fails, return the raw string content for later parsing attempt\n",
    "             print(f\"Warning: Groq response content was not valid JSON despite request: {content[:100]}...\")\n",
    "             return content # Return raw string\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format from Groq: {result}\")\n",
    "\n",
    "\n",
    "def query_model(prompt: str, coding_model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the appropriate model API based on the coding model name.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt string for the coding task.\n",
    "        coding_model_name: The name of the model (key in CODING_MODELS) to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw response content from the API (expected to be JSON or contain JSON).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provider or model configuration is invalid.\n",
    "        Exception: Propagates exceptions from the underlying API call functions.\n",
    "    \"\"\"\n",
    "    if coding_model_name not in CODING_MODELS:\n",
    "        raise ValueError(f\"Model '{coding_model_name}' not found in CODING_MODELS configuration.\")\n",
    "\n",
    "    model_config = CODING_MODELS[coding_model_name]\n",
    "    provider = model_config[\"provider\"]\n",
    "\n",
    "    if provider == \"groq\":\n",
    "        if provider not in API_CONFIG or not API_CONFIG[provider].get(\"api_key\"):\n",
    "            raise ValueError(f\"API key for provider '{provider}' not found in API_CONFIG.\")\n",
    "        api_key = API_CONFIG[provider][\"api_key\"]\n",
    "        api_url = API_CONFIG[provider].get(\"api_url\", \"[https://api.groq.com/openai/v1/chat/completions](https://api.groq.com/openai/v1/chat/completions)\")\n",
    "        return query_groq_api(prompt, model_config, api_key, api_url)\n",
    "    # Add elif blocks for other providers here if needed\n",
    "    # elif provider == \"openai\":\n",
    "    #     # Call your OpenAI query function\n",
    "    #     pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider configured for model '{coding_model_name}': {provider}\")\n",
    "\n",
    "\n",
    "def extract_json_from_response(response_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts JSON object from a string, handling potential surrounding text or markdown.\n",
    "\n",
    "    Args:\n",
    "        response_text: The raw string response from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the parsed JSON data, or an error dictionary if parsing fails.\n",
    "    \"\"\"\n",
    "    if not isinstance(response_text, str):\n",
    "        return {\"error\": \"Invalid input: response_text must be a string.\", \"raw_response\": str(response_text)}\n",
    "\n",
    "    # 1. Try direct JSON parsing first (ideal case, especially with response_format)\n",
    "    try:\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # print(\"Direct JSON parsing failed, attempting extraction...\") # Optional debug info\n",
    "        pass # Continue to extraction logic\n",
    "\n",
    "    # 2. Remove potential markdown wrappers (```json ... ``` or ``` ... ```)\n",
    "    cleaned_text = re.sub(r'^```(?:json)?\\s*', '', response_text.strip(), flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'\\s*```$', '', cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    # 3. Find the outermost curly braces\n",
    "    start_index = cleaned_text.find('{')\n",
    "    end_index = cleaned_text.rfind('}')\n",
    "\n",
    "    if start_index != -1 and end_index != -1 and end_index > start_index:\n",
    "        json_str = cleaned_text[start_index : end_index + 1]\n",
    "        try:\n",
    "            # 4. Attempt to parse the extracted string\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            # print(f\"Extraction parsing failed: {e}\") # Optional debug info\n",
    "            # Attempt to fix trailing commas as a common issue\n",
    "            json_str_fixed = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
    "            try:\n",
    "                return json.loads(json_str_fixed)\n",
    "            except json.JSONDecodeError:\n",
    "                 return {\"error\": f\"Could not parse extracted JSON: {e}\", \"extracted_snippet\": json_str[:200], \"raw_response\": response_text[:500]}\n",
    "    else:\n",
    "        # No clear JSON structure found after cleaning\n",
    "        return {\"error\": \"No valid JSON structure found in response\", \"raw_response\": response_text[:500]}\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "print(\"Starting Deductive Coding Process...\")\n",
    "\n",
    "# Load the generated abstracts data\n",
    "try:\n",
    "    abstracts_df = pd.read_feather(INPUT_ABSTRACTS_FILE)\n",
    "    print(f\"Successfully loaded {len(abstracts_df)} rows from {INPUT_ABSTRACTS_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Input file not found at {INPUT_ABSTRACTS_FILE}. Please ensure the generation script ran successfully.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load or read {INPUT_ABSTRACTS_FILE}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Verify essential columns exist from the generation script\n",
    "required_cols = ['id', 'provider', 'model', 'topic_mix', 'selected_subtopics', 'diversity_params']\n",
    "missing_cols = [col for col in required_cols if col not in abstracts_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"ERROR: Input DataFrame is missing required columns: {', '.join(missing_cols)}. Check the generation script output.\")\n",
    "    exit()\n",
    "\n",
    "# Data Type Check/Conversion (Optional but recommended)\n",
    "# Feather usually preserves types, but check `diversity_params`\n",
    "if not all(isinstance(x, (dict, type(None))) for x in abstracts_df['diversity_params'] if pd.notna(x)):\n",
    "     print(\"Warning: 'diversity_params' column contains non-dictionary elements. Attempting safe parsing...\")\n",
    "     # Define a safe parsing function if needed (e.g., using ast.literal_eval)\n",
    "     def safe_parse_dict(val):\n",
    "         if isinstance(val, dict) or pd.isna(val):\n",
    "             retMstance(val, str):\n",
    "             try:\n",
    "                 parsed = ast.literal_eval(val)\n",
    "                 return parsed if isinstance(parsed, dict) else None\n",
    "             except:\n",
    "                 return None\n",
    "         return None\n",
    "     abstracts_df['diversity_params'] = abstracts_df['diversity_params'].apply(safe_parse_dict)\n",
    "     print(\"Completed safe parsing attempt for 'diversity_params'.\")\n",
    "\n",
    "\n",
    "# Create the codebook\n",
    "codebook = create_codebook()\n",
    "print(\"Codebook created.\")\n",
    "\n",
    "# Select samples (all or a subset)\n",
    "if MAX_SAMPLES_TO_CODE is not None and MAX_SAMPLES_TO_CODE < len(abstracts_df):\n",
    "    selected_df = abstracts_df.sample(n=MAX_SAMPLES_TO_CODE, random_state=42).copy()\n",
    "    print(f\"Selected {len(selected_df)} random samples for coding.\")\n",
    "else:\n",
    "    selected_df = abstracts_df.copy()\n",
    "    print(f\"Selected all {len(selected_df)} rows for coding.\")\n",
    "\n",
    "\n",
    "coding_results_list = []\n",
    "\n",
    "# Process each selected abstract row\n",
    "print(\"Starting coding loop...\")\n",
    "for idx, row in tqdm(selected_df.iterrows(), total=len(selected_df), desc=\"Coding Abstracts\"):\n",
    "        generating_provider = row.get('provider')\n",
    "        generating_model = row.get('model')\n",
    "        abstract_id = row.get('id') # Original variation ID\n",
    "\n",
    "        # Basic check for provider info\n",
    "        if pd.isna(generating_provider):\n",
    "            # print(f\"Skipping row index {idx} due to missing provider.\") # Optional verbose logging\n",
    "            continue\n",
    "\n",
    "        # --- Check if the abstract generation was successful ---\n",
    "        error_col = f\"{generating_provider}_error\"\n",
    "        # Check if error column exists and if it has a non-NA value indicating an error\n",
    "        if error_col in row and not pd.isna(row[error_col]):\n",
    "            # print(f\"Skipping row index {idx} (ID: {abstract_id}, GenModel: {generating_model}) due to generation error.\")\n",
    "            continue\n",
    "\n",
    "        # --- Get the generated abstract text ---\n",
    "        abstract_col = f\"{generating_provider}_abstract\"\n",
    "        if abstract_col not in row or pd.isna(row[abstract_col]):\n",
    "            # print(f\"Skipping row index {idx} (ID: {abstract_id}, GenModel: {generating_model}) due to missing/empty abstract text.\")\n",
    "            continue\n",
    "        abstract_text = row[abstract_col]\n",
    "\n",
    "        # --- Prepare Ground Truth Information ---\n",
    "        # Ensure complex types like dicts are handled correctly (Feather should help)\n",
    "        diversity_params_val = row.get('diversity_params')\n",
    "        domain_val = \"\"\n",
    "        if isinstance(diversity_params_val, dict):\n",
    "            domain_val = diversity_params_val.get(\"domain\", \"\")\n",
    "\n",
    "        ground_truth = {\n",
    "            \"original_variation_id\": abstract_id,\n",
    "            \"generating_model\": generating_model,\n",
    "            \"generating_provider\": generating_provider,\n",
    "            \"topic_mix\": row.get('topic_mix'), # Should be dict\n",
    "            \"selected_subtopics\": row.get('selected_subtopics'), # Should be dict\n",
    "            \"diversity_params\": diversity_params_val, # Should be dict or None\n",
    "            \"domain\": domain_val,\n",
    "            \"diversity_type\": row.get('diversity_type'),\n",
    "            \"abstract_length_setting\": row.get('abstract_length'),\n",
    "            \"allow_topic_mention\": row.get('allow_topic_mention'),\n",
    "            \"allow_subtopic_mention\": row.get('allow_subtopic_mention'),\n",
    "            # Add any other relevant ground truth columns from the input df\n",
    "            # e.g., 'topic_spec_id': row.get('topic_spec_id')\n",
    "        }\n",
    "\n",
    "        # --- Perform Coding with each configured CODING model ---\n",
    "        for coding_model_name in CODING_MODELS.keys():\n",
    "            # print(f\"  Coding Abstract ID {abstract_id} (Gen: {generating_model}) using Coder: {coding_model_name}\") # Verbose\n",
    "            \n",
    "                        \n",
    "            # Create the prompt for this abstract and codebook\n",
    "            prompt = create_deductive_coding_prompt(codebook, abstract_text)\n",
    "\n",
    "            try:\n",
    "                \n",
    "                # Add a small delay between API calls\n",
    "                time.sleep(random.uniform(2.0, 3.0))\n",
    "\n",
    "\n",
    "                if coding_model_name== \"llama-3.3-70b-versatile\":\n",
    "                    coding_model_name =  \"llama-3.3-70b-specdec\"\n",
    "\n",
    "\n",
    "                # Query the coding model\n",
    "                raw_response = query_model(prompt, coding_model_name)\n",
    "\n",
    "                # Parse the JSON response\n",
    "                coding_result_json = extract_json_from_response(raw_response)\n",
    "\n",
    "                # Store the results\n",
    "                result_entry = {\n",
    "                    \"original_variation_id\": abstract_id,\n",
    "                    \"generating_model\": generating_model,\n",
    "                    \"generating_provider\": generating_provider,\n",
    "                    \"coding_model\": coding_model_name, # Model used for coding\n",
    "                    \"abstract_text_coded\": abstract_text, # The actual text coded\n",
    "                    \"ground_truth\": ground_truth, # Original parameters & info\n",
    "                    # \"coding_prompt\": prompt, # Optional: store prompt if needed, can be large\n",
    "                    \"coding_result\": coding_result_json, # Parsed JSON result from coding model\n",
    "                    # \"coding_raw_response\": raw_response, # Optional: store raw response if needed\n",
    "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }\n",
    "                coding_results_list.append(result_entry)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                # Construct a detailed error message\n",
    "                error_message = f\"Coding Exception: {type(e).__name__} - {str(e)}\"\n",
    "\n",
    "                print(f\"\\nERROR processing: {error_message}\")\n",
    "\n",
    "                # Log error information (as before)\n",
    "                current_results_lookup[task_id] = {\n",
    "                    \"original_variation_id\": abstract_id, \"generating_model\": generating_model,\n",
    "                    \"generating_provider\": generating_provider, \"coding_model\": coding_model_name,\n",
    "                    \"abstract_text_coded\": abstract_text, \"ground_truth\": ground_truth,\n",
    "                    \"error\": error_message, # Store the detailed error\n",
    "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"status\": \"processed_error\"\n",
    "                }\n",
    "        # --- Periodic Save ---\n",
    "        if (idx + 1) % 20 == 0: # Save every 20 rows processed\n",
    "            try:\n",
    "                with open(OUTPUT_CODING_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(coding_results_list, f, indent=2)\n",
    "                # print(f\"\\n--- Periodically saved {len(coding_results_list)} results to {OUTPUT_CODING_RESULTS_FILE} ---\")\n",
    "            except Exception as save_e:\n",
    "                print(f\"\\nWarning: Periodic save failed: {save_e}\")\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a159c8-f910-4adc-a0bd-c8f0a6893995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Final Save ---\n",
    "print(f\"\\nFinished processing. Saving final {len(coding_results_list)} results...\")\n",
    "try:\n",
    "    with open(OUTPUT_CODING_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(coding_results_list, f, indent=2)\n",
    "    print(f\"Successfully saved final results to {OUTPUT_CODING_RESULTS_FILE}\")\n",
    "except Exception as save_e:\n",
    "    print(f\"ERROR: Final save failed: {save_e}\")\n",
    "\n",
    "print(\"Deductive Coding Process Complete.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
