{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b5d071-c8a3-4fff-91ee-3f308fb30fee",
   "metadata": {},
   "source": [
    "# Synthetic Academic Abstract Generation Lab Notebook\n",
    "## Introduction\n",
    "\n",
    "This lab notebook documents our exploration of synthetic academic abstract generation, a methodology that serves as a foundation for evaluating concept detection capabilities across different large language models (LLMs). By creating a corpus of synthetic academic abstracts with controlled topic mixtures and linguistic characteristics, we can systematically assess how well various LLMs extract and analyze conceptual information from text.\n",
    "\n",
    "The experiment follows a two-phase approach:\n",
    "\n",
    "- **Generation Phase**: Creating synthetic abstracts with precise topic distributions, vocabulary constraints, and stylistic parameters using two different LLM engines\n",
    "- **Evaluation Phase**: Testing how effectively other LLMs can detect, extract, and analyze the concepts deliberately embedded in these synthetic abstracts\n",
    "\n",
    "This approach establishes a ground truth dataset with known concept distributions and vocabulary constraints, creating an ideal testbed for evaluating concept detection capabilities. The key step is that abstracts must discuss specific topics without using explicit topic names or terms, requiring the models to employ alternative vocabulary and conceptual framing.\n",
    "\n",
    "**Important Caveat: Assumption of Generation Fidelity**\n",
    "\n",
    "A critical assumption underlying this methodology is that the LLMs faithfully follow our generation instructions, particularly regarding topic distribution, vocabulary constraints, and stylistic parameters. There is no guarantee that the generated abstracts actually contain the exact topic distribution we specified or completely avoid the prohibited vocabulary. To somewhat address this limitation, our implementation includes a verification step to check for the presence of explicitly prohibited topic and subtopic words in the generated abstracts.\n",
    "\n",
    "For a more comprehensive validation, future work could include:\n",
    "\n",
    "- Independent expert annotation of the generated abstracts to verify topic distributions\n",
    "- Statistical analysis of linguistic features to confirm adherence to stylistic parameters\n",
    "- Cross-model evaluation where different LLMs rate the adherence of generated abstracts to the original specifications\n",
    "\n",
    "## Model Specifications\n",
    "The generation phase employs two distinct LLM providers with the following specifications:\n",
    "\n",
    "| Provider | Model           | Architecture      | Parameters | Quantization    | API Access      | Key Features                                 |\n",
    "|----------|-----------------|-------------------|------------|-----------------|-----------------|----------------------------------------------|\n",
    "| Groq     | llama3-70b-8192 | Llama 3           | 70B        | -               | Remote API      | High performance, instruction-tuned          |\n",
    "| Ollama   | qwen2.5-7b      | Qwen 2.5          | 7.62B      | Q4_K_M            | Local API       | Multilingual, trained on 18T tokens, Apache 2.0 license |\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "The lab employs the following methodology for generating synthetic abstracts:\n",
    "\n",
    "- Topic Network Construction: Creating a toy weighted network of academic topics and subtopics\n",
    "- Conceptual Mixing: Sampling topic combinations based on network proximity with controlled distribution ratios\n",
    "- Parameter Sampling: Assigning diverse textual characteristics including methodology, formality, jargon density, and interdisciplinary orientation\n",
    "- Prompt Engineering: Developing detailed prompts with explicit vocabulary constraints that require models to discuss topics without using their explicit names\n",
    "- Parallel Generation: Generating abstracts using both Groq and Ollama services for comparative analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82f6b2-2ddb-4f13-99d0-86828d952193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Synthetic Abstract Generation for Concept Evaluation\n",
    "# ==================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re  # Import the regex module\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a56692-db93-41c6-8351-7c1b3188498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# !! SECURITY WARNING !!: Avoid hardcoding API keys in scripts.\n",
    "# Consider using environment variables or a secrets management tool.\n",
    "# groq_api_key = os.environ.get(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your Groq API Key: \")\n",
    "groq_api_key = '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'\n",
    "\n",
    "# --- Topic Space and Diversity Parameters ---\n",
    "TOPICS = {\n",
    "    \"T1\": {\n",
    "        \"name\": \"Machine Learning\",\n",
    "        \"subtopics\": [\"Neural Networks\", \"Reinforcement Learning\", \"Supervised Learning\", \"Unsupervised Learning\", \"Transfer Learning\"]\n",
    "    },\n",
    "    \"T7\": {\n",
    "        \"name\": \"Sustainable Development\",\n",
    "        \"subtopics\": [\"Renewable Energy\", \"Climate Change Mitigation\", \"Resource Management\", \"Environmental Monitoring\", \"Sustainable Cities\"]\n",
    "    },\n",
    "    \"T8\": {\n",
    "        \"name\": \"Behavioral Economics\",\n",
    "        \"subtopics\": [\"Decision Making\", \"Cognitive Biases\", \"Risk Assessment\", \"Social Preferences\", \"Intertemporal Choice\"]\n",
    "    },\n",
    "    \"T9\": {\n",
    "        \"name\": \"Digital Security\",\n",
    "        \"subtopics\": [\"Cybersecurity\", \"Privacy Enhancing Technologies\", \"Authentication Methods\", \"Threat Detection\", \"Security Policy\"]\n",
    "    },\n",
    "    \"T10\": {\n",
    "        \"name\": \"Public Health\",\n",
    "        \"subtopics\": [\"Epidemiology\", \"Health Promotion\", \"Disease Prevention\", \"Health Equity\", \"Health Systems\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "DOMAINS = [\"Sports\", \"Marriage\", \"Childcare\", \"Exercise\", \"School\", \"Social Media\", \"Advertisement\"]\n",
    "\n",
    "DIVERSITY_PARAMS = {\n",
    "    \"methodological_approaches\": [\n",
    "        \"Theory\", \"Qualitative\", \"Experimental\", \"Observational\",\n",
    "        \"Quasi-Experimental\", \"Laboratory Science\", \"Survey\",\n",
    "        \"Correlational\", \"Mixed-methods\"\n",
    "    ],\n",
    "    \"concept_granularity\": [\"General Principles\", \"Specific Applications\", \"Mixed\"],\n",
    "    \"interdisciplinary_orientation\": [\"Pure-discipline\", \"Multi-disciplinary\"],\n",
    "    \"rhetorical_structures\": [\n",
    "        \"Problem-solution\", \"Contribution-focused\",\n",
    "        \"Findings-centered\", \"Process-oriented\"\n",
    "    ],\n",
    "    \"formality_levels\": [\"Highly Formal\", \"Accessible\"],\n",
    "    \"terminology_density\": [\"Terminology-rich\", \"Balanced\", \"Minimal Jargon\"],\n",
    "    \"temporal_context\": [\"Contemporary\", \"Historical Context\", \"Future-oriented\"],\n",
    "    \"concept_blending\": [\"Juxtaposition\", \"Terminology-level\", \"Methodology-level\", \"Deep Integration\"]\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def create_topic_network():\n",
    "    \"\"\"Create a weighted network of topics based on similarity/co-occurrence\"\"\"\n",
    "    G = nx.Graph()\n",
    "    for topic_id, topic_data in TOPICS.items():\n",
    "        G.add_node(topic_id, name=topic_data[\"name\"], subtopics=topic_data[\"subtopics\"])\n",
    "\n",
    "    for topic1 in TOPICS:\n",
    "        for topic2 in TOPICS:\n",
    "            if topic1 != topic2:\n",
    "                subtopics1 = set(TOPICS[topic1][\"subtopics\"])\n",
    "                subtopics2 = set(TOPICS[topic2][\"subtopics\"])\n",
    "                jaccard = len(subtopics1.intersection(subtopics2)) / len(subtopics1.union(subtopics2))\n",
    "                similarity = jaccard + np.random.normal(0, 0.1)\n",
    "                similarity = max(0.05, min(0.95, similarity))\n",
    "                if not G.has_edge(topic1, topic2): # Add edge only once\n",
    "                    G.add_edge(topic1, topic2, weight=similarity)\n",
    "    return G\n",
    "\n",
    "\n",
    "def sample_concept_mix(topic_network, num_topics=None):\n",
    "    \"\"\"Sample a mix of topics from the topic network\"\"\"\n",
    "    if num_topics is None:\n",
    "        num_topics = np.random.choice([1, 2, 3], p=[0.2, 0.6, 0.2])\n",
    "\n",
    "    all_topics = list(topic_network.nodes())\n",
    "\n",
    "    if num_topics == 1:\n",
    "        topic = np.random.choice(all_topics)\n",
    "        return {topic: 1.0}\n",
    "    else:\n",
    "        selected_topics = [np.random.choice(all_topics)]\n",
    "        for _ in range(num_topics - 1):\n",
    "            all_neighbors = []\n",
    "            neighbor_weights = []\n",
    "            for t in selected_topics:\n",
    "                for n in topic_network.neighbors(t):\n",
    "                    # Ensure neighbor is not already selected and has weights\n",
    "                    if n not in selected_topics and n in topic_network[t]:\n",
    "                         # Check if neighbor exists and edge has weight data\n",
    "                        if n not in all_neighbors:\n",
    "                            all_neighbors.append(n)\n",
    "                            neighbor_weights.append(topic_network[t][n]['weight'])\n",
    "                        else:\n",
    "                            # If neighbor already listed (from another selected topic),\n",
    "                            # potentially average or sum weights? Let's just keep first found weight.\n",
    "                            pass\n",
    "\n",
    "\n",
    "            if not all_neighbors:\n",
    "                remaining = list(set(all_topics) - set(selected_topics))\n",
    "                if not remaining: break\n",
    "                selected_topics.append(np.random.choice(remaining))\n",
    "            else:\n",
    "                total_weight = sum(neighbor_weights)\n",
    "                if total_weight <= 0: # Handle cases where all weights are zero or negative\n",
    "                     normalized_weights = [1/len(neighbor_weights)] * len(neighbor_weights) # Equal probability\n",
    "                else:\n",
    "                    normalized_weights = [w / total_weight for w in neighbor_weights]\n",
    "\n",
    "                # Ensure lengths match before choice\n",
    "                if len(all_neighbors) != len(normalized_weights):\n",
    "                     print(f\"Warning: Mismatch in neighbors ({len(all_neighbors)}) and weights ({len(normalized_weights)}). Using uniform distribution.\")\n",
    "                     selected_topics.append(np.random.choice(all_neighbors))\n",
    "                else:\n",
    "                    selected_topics.append(np.random.choice(all_neighbors, p=normalized_weights))\n",
    "\n",
    "        if len(selected_topics) == 2:\n",
    "            weights = [0.7, 0.3]\n",
    "        elif len(selected_topics) == 3:\n",
    "            weights = [0.5, 0.3, 0.2]\n",
    "        else: # Handle cases where fewer than desired topics were found\n",
    "             weights = [1.0 / len(selected_topics)] * len(selected_topics)\n",
    "\n",
    "        # Ensure selected_topics and weights have the same length after sampling\n",
    "        weights = weights[:len(selected_topics)]\n",
    "        return {t: w for t, w in zip(selected_topics, weights)}\n",
    "\n",
    "\n",
    "def sample_diversity_params():\n",
    "    \"\"\"Sample diversity parameters for an abstract\"\"\"\n",
    "    params = {}\n",
    "    for param_name, options in DIVERSITY_PARAMS.items():\n",
    "        params[param_name] = np.random.choice(options)\n",
    "    params[\"domain\"] = np.random.choice(DOMAINS)\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Unified LLM Generation Function ---\n",
    "def parse_llm_response(response_text):\n",
    "    \"\"\"Attempts to parse the LLM response to extract the JSON object.\"\"\"\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    # Approach 1: Try direct JSON parsing\n",
    "    try:\n",
    "        # Find the first '{' and the last '}'\n",
    "        start_index = response_text.find('{')\n",
    "        end_index = response_text.rfind('}')\n",
    "        if start_index != -1 and end_index != -1 and end_index > start_index:\n",
    "            json_str = response_text[start_index : end_index + 1]\n",
    "            # Basic cleaning for common issues like trailing commas\n",
    "            json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "            json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "            result = json.loads(json_str)\n",
    "            # Validate expected keys\n",
    "            if \"title\" in result and \"abstract\" in result and \"keywords\" in result:\n",
    "                 # Ensure keywords is a list\n",
    "                 if not isinstance(result[\"keywords\"], list):\n",
    "                     result[\"keywords\"] = [str(k).strip() for k in str(result[\"keywords\"]).split(',')] # Attempt to convert if not list\n",
    "                 return result\n",
    "            else:\n",
    "                 print(\"Warning: Parsed JSON missing required keys (title, abstract, keywords).\")\n",
    "        else:\n",
    "            print(\"Warning: Could not find valid JSON structure '{...}'.\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Direct JSON parse failed: {e}. Trying regex.\")\n",
    "        # Fallback to regex if direct parsing fails - less reliable\n",
    "        try:\n",
    "            title_match = re.search(r'\"title\"\\s*:\\s*\"((?:\\\\\"|[^\"])*)\"', response_text)\n",
    "            # Regex for abstract, trying to handle escaped quotes within the abstract\n",
    "            abstract_match = re.search(r'\"abstract\"\\s*:\\s*\"((?:\\\\.|[^\"\\\\])*)\"', response_text, re.DOTALL)\n",
    "            keywords_match = re.search(r'\"keywords\"\\s*:\\s*\\[(.*?)\\]', response_text, re.DOTALL)\n",
    "\n",
    "            if title_match and abstract_match:\n",
    "                title = title_match.group(1).encode('utf-8').decode('unicode_escape') # Handle potential escapes\n",
    "                abstract = abstract_match.group(1).encode('utf-8').decode('unicode_escape') # Handle potential escapes\n",
    "\n",
    "                keywords = []\n",
    "                if keywords_match:\n",
    "                    keywords_text = keywords_match.group(1)\n",
    "                    # More robust keyword splitting\n",
    "                    keywords = [k.strip(' \"\\'') for k in re.findall(r'\"([^\"]*)\"', keywords_text)]\n",
    "                    if not keywords: # Fallback if keywords aren't quoted\n",
    "                        keywords = [k.strip(' \"\\'') for k in keywords_text.split(',') if k.strip()]\n",
    "\n",
    "\n",
    "                return {\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"keywords\": keywords\n",
    "                }\n",
    "            else:\n",
    "                 print(\"Regex extraction failed to find title/abstract.\")\n",
    "\n",
    "        except Exception as regex_e:\n",
    "            print(f\"Regex extraction attempt failed: {regex_e}\")\n",
    "\n",
    "\n",
    "    # If all parsing fails\n",
    "    print(f\"Could not parse LLM response. Raw response:\\n---\\n{response_text[:500]}...\\n---\")\n",
    "    return {\"error\": \"Failed to parse JSON response\", \"raw_response\": response_text}\n",
    "\n",
    "\n",
    "def generate_abstract(prompt, provider, config, retries=3, backoff_factor=2, session=None):\n",
    "    \"\"\"Calls the specified LLM provider API to generate an abstract.\"\"\"\n",
    "    # Use provided session or create a new one\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    \n",
    "    \"\"\"Calls the specified LLM provider API to generate an abstract.\"\"\"\n",
    "    provider_config = config.get(provider)\n",
    "    if not provider_config:\n",
    "        return {\"error\": f\"Configuration for provider '{provider}' not found.\"}\n",
    "\n",
    "    api_url = provider_config.get(\"api_url\")\n",
    "    model = provider_config.get(\"model\")\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": provider_config.get(\"temperature\", 0.7),\n",
    "         # Ensure max_tokens is included, default if not specified\n",
    "        \"max_tokens\": provider_config.get(\"max_tokens\", 1024),\n",
    "    }\n",
    "\n",
    "    if provider == 'ollama':\n",
    "        # Ollama uses a slightly different payload structure\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": provider_config.get(\"stream\", False),\n",
    "             # Ollama options might be nested\n",
    "            \"options\": {\n",
    "                 \"temperature\": provider_config.get(\"temperature\", 0.7),\n",
    "                 \"num_predict\": provider_config.get(\"max_tokens\", 4000), # Ollama uses num_predict,\n",
    "                \"num_thread\" : 30,\n",
    "                \"mirostat\" : 0,\n",
    "                \"repeat_penalty\" : 1\n",
    "             }\n",
    "        }\n",
    "        # No Authorization header needed for default local Ollama typically\n",
    "    elif provider == 'groq':\n",
    "        api_key = provider_config.get(\"api_key\")\n",
    "        if not api_key or api_key == 'gsk_...':\n",
    "             return {\"error\": \"Groq API key not configured or is placeholder.\"}\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        # Groq payload is already mostly correct from the base structure\n",
    "    else:\n",
    "        return {\"error\": f\"Unsupported provider: {provider}\"}\n",
    "\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"  Attempt {attempt + 1}/{retries} calling {provider.upper()} API ({model})...\")\n",
    "            response = session.post(api_url, headers=headers, json=payload, timeout=120) # Increased timeout\n",
    "\n",
    "            if response.status_code == 429: # Rate limit error\n",
    "                 wait_time = backoff_factor ** (attempt + 1) + random.uniform(0,1) # Add jitter\n",
    "                 print(f\"  Rate limit hit for {provider.upper()}. Retrying in {wait_time:.2f}s...\")\n",
    "                 time.sleep(wait_time)\n",
    "                 continue # Retry the loop\n",
    "\n",
    "            # Check for other HTTP errors\n",
    "            response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            response_json = response.json()\n",
    "\n",
    "            # --- Extract content based on provider ---\n",
    "            if provider == 'ollama':\n",
    "                if \"message\" in response_json and \"content\" in response_json[\"message\"]:\n",
    "                    response_text = response_json[\"message\"][\"content\"]\n",
    "                elif \"error\" in response_json:\n",
    "                     raise Exception(f\"Ollama API Error: {response_json['error']}\")\n",
    "                else:\n",
    "                    raise Exception(f\"Unexpected Ollama response format: {response_json}\")\n",
    "            elif provider == 'groq':\n",
    "                if \"choices\" in response_json and len(response_json[\"choices\"]) > 0:\n",
    "                    message = response_json[\"choices\"][0].get(\"message\", {})\n",
    "                    response_text = message.get(\"content\")\n",
    "                    if response_text is None:\n",
    "                         # Check finish reason\n",
    "                         finish_reason = response_json[\"choices\"][0].get(\"finish_reason\")\n",
    "                         if finish_reason == \"error\":\n",
    "                             raise Exception(f\"Groq API Error reported in choice: {response_json}\")\n",
    "                         elif finish_reason == \"length\":\n",
    "                              raise Exception(f\"Groq generation stopped due to max_tokens limit.\")\n",
    "                         else:\n",
    "                             raise Exception(\"Groq API response missing content in message.\")\n",
    "                elif \"error\" in response_json:\n",
    "                     error_info = response_json['error']\n",
    "                     raise Exception(f\"Groq API Error: {error_info.get('message', 'Unknown error')}\")\n",
    "                else:\n",
    "                    raise Exception(f\"Unexpected Groq response format: {response_json}\")\n",
    "            else:\n",
    "                 # Should not happen due to check at the start\n",
    "                 raise Exception(f\"Provider logic error for {provider}\")\n",
    "\n",
    "            # --- Parse the extracted text ---\n",
    "            parsed_data = parse_llm_response(response_text)\n",
    "            return parsed_data # Success\n",
    "\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Network Error calling {provider.upper()} API: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                 return {\"error\": f\"Network Error after {retries} attempts: {e}\"}\n",
    "            wait_time = backoff_factor ** attempt + random.uniform(0,1)\n",
    "            print(f\"  Retrying in {wait_time:.2f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {provider.upper()} response (Attempt {attempt + 1}): {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                return {\"error\": f\"Failed after {retries} attempts: {e}\"}\n",
    "            wait_time = backoff_factor ** attempt + random.uniform(0,1)\n",
    "            print(f\"  Retrying in {wait_time:.2f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    # Should not be reached if retries loop works correctly, but as a fallback\n",
    "    return {\"error\": f\"Failed to generate abstract from {provider.upper()} after {retries} attempts.\"}\n",
    "\n",
    "def create_prompt(topic_mix, diversity_params, selected_subtopics):\n",
    "    \"\"\"Create a prompt for the LLM to generate an abstract with consistent subtopics.\"\"\"\n",
    "    formatted_topics = []\n",
    "    topic_names = []\n",
    "    forbidden_words = []\n",
    "    allowed_subtopic_words = []\n",
    "    \n",
    "    for topic_id, weight in topic_mix.items():\n",
    "        topic_name = TOPICS[topic_id][\"name\"]\n",
    "        topic_names.append(topic_name)\n",
    "        forbidden_words.append(topic_name.lower())\n",
    "        forbidden_words.extend([word.lower() for word in topic_name.split()])\n",
    "        \n",
    "        # Use the pre-selected subtopic\n",
    "        subtopic = selected_subtopics[topic_id]\n",
    "        allowed_subtopic_words.append(subtopic)\n",
    "        \n",
    "        percentage = int(weight * 100)\n",
    "        formatted_topics.append(f\"{topic_name} (specifically {subtopic}): {percentage}% focus\")\n",
    "\n",
    "    # Remove duplicates and create strings\n",
    "    forbidden_words = list(set(forbidden_words))\n",
    "    forbidden_words_str = \", \".join([f'\"{w}\"' for w in forbidden_words])\n",
    "    allowed_subtopic_words_str = \", \".join([f'\"{w}\"' for w in allowed_subtopic_words])\n",
    "    \n",
    "    topics_text = \"\\n\".join([f\"- {t}\" for t in formatted_topics])\n",
    "    num_topics = len(topic_mix)\n",
    "\n",
    "    # Define core topic names\n",
    "    core_topic_names = [TOPICS[t]['name'] for t in topic_mix]\n",
    "    focus_description = ' AND '.join(core_topic_names) if num_topics <= 2 else 'these topics (' + ', '.join(core_topic_names) + ')'\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an academic expert simulating the creation of a research abstract. \n",
    "Your task is to generate ONE research abstract that fits a specific profile.\n",
    "\n",
    "**CRITICAL REQUIREMENT: The generated 'abstract' field's text MUST be a minimum of 250 words long.** Do not generate short summaries.\n",
    "\n",
    "Your paper synthesizes the following topics. Adhere strictly to this distribution:\n",
    "{topics_text}\n",
    "\n",
    "VOCABULARY RESTRICTIONS:\n",
    "- FORBIDDEN WORDS: You must NOT use the following topic words in your abstract or title: {forbidden_words_str}\n",
    "- FORBIDDEN WORDS: You must NOT use the following subtopic words in your abstract or title: {allowed_subtopic_words_str}\n",
    "- DOMAIN: Do not use the word \"{diversity_params['domain']}\" explicitly in the abstract or title\n",
    "\n",
    "REQUIRED ABSTRACT CONTENT GUIDELINES: \n",
    "- The study's focus: {focus_description}\n",
    "- Domain application: {diversity_params['domain']}\n",
    "- Methodology: {diversity_params['methodological_approaches']}\n",
    "- Findings\n",
    "- Conclude \n",
    "- Rhetorical style: {diversity_params['rhetorical_structures']}.\n",
    "\n",
    "Ensure the abstract is cohesive, detailed, and meets the 250-word minimum requirement.\n",
    "\n",
    "ADDITIONAL PAPER ATTRIBUTES TO REFLECT:\n",
    "- Concept granularity: {diversity_params['concept_granularity']} (Reflects in the level of detail in findings)\n",
    "- Interdisciplinary orientation: {diversity_params['interdisciplinary_orientation']} (Reflected if multiple topics are distinct)\n",
    "- Temporal context: {diversity_params['temporal_context']} (Use appropriate tense/phrasing)\n",
    "\n",
    "LINGUISTIC CHARACTERISTICS TO EMBODY:\n",
    "- Formality level: {diversity_params['formality_levels']} \n",
    "- Terminology density: {diversity_params['terminology_density']} \n",
    "- Concept blending approach: {diversity_params['concept_blending']} \n",
    "\n",
    "MANDATORY INSTRUCTIONS:\n",
    "1. **Generate ONE academic abstract where the 'abstract' text is MINIMUM 250 words.**\n",
    "2. DO NOT USE THE TOPIC WORDS listed in FORBIDDEN WORDS in the abstract or title.\n",
    "3. DO NOT USE THE SUBTOPIC words in your abstract or title. \n",
    "4. Strictly follow the content guidelines.\n",
    "5. Adhere to the Topic Distribution percentages.\n",
    "6. Include at least 3-5 specific, concrete findings, methods, or implications. AVOID VAGUENESS. Elaborate on points.\n",
    "7. Ensure the abstract is academically plausible and internally consistent.\n",
    "8. Do NOT mention the percentages, parameters, instructions, or section headers explicitly in the output abstract text.\n",
    "9. Do NOT write a short summary; generate a detailed, well-developed abstract fulfilling the minimum word count.\n",
    "10. **The final 'abstract' field content MUST meet the 250-word minimum.**\n",
    "\n",
    "OUTPUT FORMAT:  \n",
    "Return ONLY a single, valid JSON object containing the keys 'title', 'abstract', and 'keywords'.\n",
    "- The 'abstract' value must be a single string containing the full abstract text (minimum 250 words).\n",
    "- The 'keywords' value must be a list of 4-6 relevant strings.\n",
    "- Do NOT include ```json markdown wrappers, comments, explanations, or any text outside the JSON structure.\n",
    "\n",
    "Example JSON structure (fill with generated content):\n",
    "{{\n",
    "  \"title\": \"A Plausible and Specific Academic Title Reflecting the Content\",\n",
    "  \"abstract\": \"Abstract text here...\\\\n\\\\nMore text here... (Ensuring total abstract is >= 250 words)\",\n",
    "  \"keywords\": [\"Relevant Keyword 1\", \"Keyword 2\", \"Topic Keyword\", \"Method Keyword\", \"Domain Keyword\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "# --- Main Pipeline ---\n",
    "\n",
    "def generate_synthetic_dataset(config):\n",
    "    # Create session objects at the beginning\n",
    "    ollama_session = requests.Session()\n",
    "    groq_session = requests.Session()\n",
    "    \n",
    "    \"\"\"Generate a synthetic dataset of abstracts using configured LLM providers.\"\"\"\n",
    "    num_documents = config[\"num_documents\"]\n",
    "    print(\"Creating topic network...\")\n",
    "    topic_network = create_topic_network()\n",
    "\n",
    "    print(f\"\\nGenerating {num_documents} abstracts using Ollama and Groq...\")\n",
    "    dataset = []\n",
    "    providers = ['ollama', 'groq'] # Define providers to use\n",
    "\n",
    "    for i in tqdm(range(num_documents), desc=\"Generating Abstracts\"):\n",
    "        print(f\"\\n--- Generating Abstract Set {i+1}/{num_documents} ---\")\n",
    "        try:\n",
    "            # 1. Sample concept mix & diversity params (same for both LLMs)\n",
    "            topic_mix = sample_concept_mix(topic_network)\n",
    "            diversity_params = sample_diversity_params()\n",
    "\n",
    "            # Store forbidden topic words and selected subtopics for later checks\n",
    "            forbidden_words = []\n",
    "            selected_subtopics = {}\n",
    "            \n",
    "            for topic_id, weight in topic_mix.items():\n",
    "                topic_name = TOPICS[topic_id][\"name\"]\n",
    "                forbidden_words.append(topic_name.lower())\n",
    "                forbidden_words.extend([word.lower() for word in topic_name.split()])\n",
    "                \n",
    "                # Select a subtopic randomly\n",
    "                subtopic = np.random.choice(TOPICS[topic_id][\"subtopics\"])\n",
    "                selected_subtopics[topic_id] = subtopic\n",
    "\n",
    "            # Remove duplicates in forbidden words\n",
    "            forbidden_words = list(set(forbidden_words))\n",
    "\n",
    "            # 2. Create prompt - now passing selected_subtopics as an argument\n",
    "            prompt = create_prompt(topic_mix, diversity_params, selected_subtopics)\n",
    "\n",
    "            # 3. Generate abstract from each provider\n",
    "            results = {}\n",
    "            for provider in providers:\n",
    "                print(f\" Generating with {provider.upper()}...\")\n",
    "                start_time = time.time()\n",
    "                # Pass the main config, the function will extract provider-specific settings\n",
    "                result_data = generate_abstract(prompt, \n",
    "                                                provider, \n",
    "                                                config, \n",
    "                                                session=ollama_session if provider == 'ollama' else groq_session)\n",
    "                end_time = time.time()\n",
    "                print(f\"  {provider.upper()} generation took {end_time - start_time:.2f}s\")\n",
    "                results[provider] = result_data\n",
    "            \n",
    "            # 4. Check for forbidden words and subtopic presence\n",
    "            for provider in providers:\n",
    "                # Skip checks if there was an error or no abstract\n",
    "                if results[provider].get(\"error\") or not results[provider].get(\"abstract\"):\n",
    "                    continue\n",
    "\n",
    "                abstract_text = results[provider].get(\"abstract\", \"\").lower()\n",
    "                title_text = results[provider].get(\"title\", \"\").lower()\n",
    "                combined_text = abstract_text + \" \" + title_text\n",
    "\n",
    "                # Check for exact topic phrases (ignoring case)\n",
    "                found_forbidden_words = []\n",
    "                for topic_id, weight in topic_mix.items():\n",
    "                    topic_name = TOPICS[topic_id][\"name\"].lower()\n",
    "                    if topic_name in combined_text:\n",
    "                        found_forbidden_words.append(topic_name)\n",
    "\n",
    "                results[provider][\"contains_forbidden_words\"] = len(found_forbidden_words) > 0\n",
    "                results[provider][\"found_forbidden_words\"] = found_forbidden_words if found_forbidden_words else []\n",
    "\n",
    "                # Check for subtopic presence\n",
    "                subtopic_presence = {}\n",
    "                for topic_id, subtopic in selected_subtopics.items():\n",
    "                    subtopic_lower = subtopic.lower()\n",
    "                    subtopic_presence[subtopic] = subtopic_lower in abstract_text\n",
    "\n",
    "                results[provider][\"contains_subtopics\"] = subtopic_presence\n",
    "                results[provider][\"any_subtopic_present\"] = any(subtopic_presence.values())\n",
    "\n",
    "            # 5. Store results with metadata\n",
    "            entry = {\n",
    "                \"id\": i + 1,\n",
    "                \"topic_mix\": topic_mix,\n",
    "                \"diversity_params\": diversity_params,\n",
    "                \"forbidden_words\": forbidden_words,\n",
    "                \"selected_subtopics\": selected_subtopics,\n",
    "                \"prompt\": prompt,\n",
    "            }\n",
    "            \n",
    "            # Add provider-specific results\n",
    "            for provider in providers:\n",
    "                entry[f\"{provider}_title\"] = results[provider].get(\"title\")\n",
    "                entry[f\"{provider}_abstract\"] = results[provider].get(\"abstract\")\n",
    "                entry[f\"{provider}_keywords\"] = results[provider].get(\"keywords\")\n",
    "                entry[f\"{provider}_error\"] = results[provider].get(\"error\")\n",
    "                entry[f\"{provider}_raw_response\"] = results[provider].get(\"raw_response\")\n",
    "                \n",
    "                # Add new flags\n",
    "                if not results[provider].get(\"error\") and results[provider].get(\"abstract\"):\n",
    "                    entry[f\"{provider}_contains_forbidden_words\"] = results[provider].get(\"contains_forbidden_words\", False)\n",
    "                    entry[f\"{provider}_found_forbidden_words\"] = results[provider].get(\"found_forbidden_words\", [])\n",
    "                    entry[f\"{provider}_contains_subtopics\"] = results[provider].get(\"contains_subtopics\", {})\n",
    "                    entry[f\"{provider}_any_subtopic_present\"] = results[provider].get(\"any_subtopic_present\", False)\n",
    "            \n",
    "            dataset.append(entry)\n",
    "\n",
    "            # Optional delay between generating sets of abstracts\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!! Unexpected error during generation loop for abstract set {i+1}: {e}\")\n",
    "            # Add placeholder error entry\n",
    "            dataset.append({\n",
    "                \"id\": i + 1,\n",
    "                \"topic_mix\": topic_mix if 'topic_mix' in locals() else {},\n",
    "                \"diversity_params\": diversity_params if 'diversity_params' in locals() else {},\n",
    "                \"ollama_error\": f\"Outer loop error: {e}\",\n",
    "                \"groq_error\": f\"Outer loop error: {e}\",\n",
    "            })\n",
    "\n",
    "    # --- Post-processing and Saving ---\n",
    "    print(\"\\nProcessing results...\")\n",
    "\n",
    "    # Convert mix dictionaries to strings for easier CSV viewing\n",
    "    for item in dataset:\n",
    "        if isinstance(item.get(\"topic_mix\"), dict):\n",
    "             item[\"topic_mix_str\"] = \", \".join([f\"{TOPICS.get(k, {'name':k})['name']}: {v*100:.0f}%\" for k, v in item[\"topic_mix\"].items()])\n",
    "        else:\n",
    "             item[\"topic_mix_str\"] = \"Error processing mix\"\n",
    "\n",
    "        # Ensure keywords are stored as strings for CSV\n",
    "        for provider in providers:\n",
    "            kw_key = f\"{provider}_keywords\"\n",
    "            if isinstance(item.get(kw_key), list):\n",
    "                 item[kw_key] = json.dumps(item[kw_key]) # Store list as JSON string\n",
    "            elif item.get(kw_key) is None and item.get(f\"{provider}_error\") is None:\n",
    "                 item[kw_key] = json.dumps([]) # Empty list if generation was successful but no keywords found\n",
    "\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    # Reorder columns for better readability\n",
    "    cols_order = [\"id\", \"topic_mix_str\"] \n",
    "    cols_order.extend([f\"{p}_{field}\" for p in providers for field in [\"title\", \"abstract\", \"keywords\", \n",
    "                                                                     \"contains_forbidden_words\", \n",
    "                                                                     \"found_forbidden_words\", \n",
    "                                                                     \"contains_subtopics\", \n",
    "                                                                     \"any_subtopic_present\",\n",
    "                                                                     \"error\", \"raw_response\"]])\n",
    "    cols_order.extend([\"topic_mix\", \"diversity_params\", \"forbidden_words\", \"selected_subtopics\", \"prompt\"])    \n",
    "    cols_order = [col for col in cols_order if col in df.columns]\n",
    "    df = df[cols_order]\n",
    "\n",
    "\n",
    "    # Save dataset\n",
    "    output_file = config[\"output_file\"]\n",
    "    try:\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\nDataset saved successfully to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving dataset to CSV: {e}\")\n",
    "\n",
    "\n",
    "    # Create topics DataFrame (as before)\n",
    "    topic_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Ensure topic_mix is a dictionary before iterating\n",
    "         if isinstance(row.get(\"topic_mix\"), dict):\n",
    "             for topic_id, weight in row[\"topic_mix\"].items():\n",
    "                 # Safely get topic name, handle missing IDs\n",
    "                 topic_data = TOPICS.get(topic_id, {\"name\": f\"Unknown Topic ({topic_id})\"})\n",
    "                 topic_name = topic_data.get(\"name\", f\"Unnamed Topic ({topic_id})\")\n",
    "                 topic_rows.append({\n",
    "                     \"id\": row[\"id\"],\n",
    "                     \"topic_id\": topic_id,\n",
    "                     \"topic_name\": topic_name,\n",
    "                     \"weight\": weight\n",
    "                 })\n",
    "\n",
    "    topics_df = pd.DataFrame(topic_rows) if topic_rows else pd.DataFrame(columns=[\"id\", \"topic_id\", \"topic_name\", \"weight\"])\n",
    "\n",
    "    print(f\"\\nGenerated data for {len(df)} prompts.\")\n",
    "    # Count successes/failures per provider\n",
    "    for provider in providers:\n",
    "        success_count = df[f'{provider}_error'].isna().sum()\n",
    "        print(f\"  {provider.upper()}: {success_count} successful generations, {len(df) - success_count} failures/errors.\")\n",
    "\n",
    "\n",
    "    return df, topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9525aad-2773-4610-9e5f-18da32b5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"num_documents\": 100,  # Reduced for faster testing, increase as needed\n",
    "    \"output_file\": \"synthetic_abstracts_dual_llm_censored.csv\",\n",
    "    \"topic_network_file\": \"topic_network_censored.json\",\n",
    "\n",
    "    \"ollama\": {\n",
    "        \"api_url\": \"http://localhost:11434/api/chat\",\n",
    "        \"model\": \"llama3.1:latest\", #\"qwen2.5\",  # Or your preferred local model\n",
    "        \"max_tokens\": 8000, # Adjust token limit for Ollama model if needed\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": False,\n",
    "    },\n",
    "\n",
    "    \"groq\": {\n",
    "        \"api_url\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        \"api_key\": groq_api_key,\n",
    "        # Choose a Groq model: 'llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it'\n",
    "        \"model\": 'llama3-70b-8192',\n",
    "        \"max_tokens\": 8000, # Groq often uses token limits differently, adjust as needed\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "}\n",
    "\n",
    "# CONFIG['num_documents'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fcf7b-af7e-4d0a-aece-0b19cf12f581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure you have replaced 'gsk_...' in CONFIG with your actual Groq key\n",
    "\n",
    "    # Check if Groq key is still the placeholder\n",
    "    if CONFIG['groq']['api_key'] == 'gsk_...':\n",
    "        print(\"=\"*60)\n",
    "        print(\"ERROR: Please replace 'gsk_...' in the CONFIG dictionary\")\n",
    "        print(\"       with your actual Groq API key before running.\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        # Generate the dataset\n",
    "        generated_df, topics_info_df = generate_synthetic_dataset(CONFIG)\n",
    "\n",
    "        # Display first few rows of the generated data\n",
    "        print(\"\\n--- Sample Generated Data ---\")\n",
    "        print(generated_df[['id', 'topic_mix_str', 'ollama_title', 'groq_title']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119db5-8f96-4c71-8365-bd770b056b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df.to_feather('generated_df_100_censored_longer.feather')\n",
    "topics_info_df.to_feather('topics_info_df_100_censored_longer.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
