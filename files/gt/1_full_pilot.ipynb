{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550d4ff-d685-4712-9b05-495c55d0d019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Synthetic Abstract Generation for Concept Evaluation\n",
    "# ==================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re  # Import the regex module\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(4242)\n",
    "\n",
    "# --- Configuration ---\n",
    "# !! SECURITY WARNING !!: Avoid hardcoding API keys in scripts.\n",
    "# Consider using environment variables or a secrets management tool.\n",
    "# groq_api_key = os.environ.get(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your Groq API Key: \")\n",
    "groq_api_key = ''\n",
    "\n",
    "# --- Topic Space and Diversity Parameters ---\n",
    "TOPICS = {\n",
    "    \"T1\": {\n",
    "        \"name\": \"Machine Learning\",\n",
    "        \"subtopics\": [\"Neural Networks\", \"Reinforcement Learning\", \"Supervised Learning\", \"Unsupervised Learning\", \"Transfer Learning\"]\n",
    "    },\n",
    "    \"T7\": {\n",
    "        \"name\": \"Sustainable Development\",\n",
    "        \"subtopics\": [\"Renewable Energy\", \"Climate Change Mitigation\", \"Resource Management\", \"Environmental Monitoring\", \"Sustainable Cities\"]\n",
    "    },\n",
    "    \"T8\": {\n",
    "        \"name\": \"Behavioral Economics\",\n",
    "        \"subtopics\": [\"Decision Making\", \"Cognitive Biases\", \"Risk Assessment\", \"Social Preferences\", \"Intertemporal Choice\"]\n",
    "    },\n",
    "    \"T9\": {\n",
    "        \"name\": \"Digital Security\",\n",
    "        \"subtopics\": [\"Cybersecurity\", \"Privacy Enhancing Technologies\", \"Authentication Methods\", \"Threat Detection\", \"Security Policy\"]\n",
    "    },\n",
    "    \"T10\": {\n",
    "        \"name\": \"Public Health\",\n",
    "        \"subtopics\": [\"Epidemiology\", \"Health Promotion\", \"Disease Prevention\", \"Health Equity\", \"Health Systems\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "DOMAINS = [\"Sports\", \"Marriage\", \"Childcare\", \"Exercise\", \"School\", \"Social Media\", \"Advertisement\"]\n",
    "\n",
    "DIVERSITY_PARAMS = {\n",
    "    \"methodological_approaches\": [\n",
    "        \"Theory\", \"Qualitative\", \"Randomized Experiments\", \n",
    "        \"Quasi-Experimental\", \"Survey\", \"Correlational\",\"Mixed-methods\"\n",
    "    ],\n",
    "    \"concept_granularity\": [\n",
    "        \"General Principles\", \"Specific Applications\", \"Mixed\"\n",
    "    ],\n",
    "    \"interdisciplinary_orientation\": [\n",
    "        \"Pure-discipline\", \"Multi-disciplinary\"\n",
    "    ],\n",
    "    \"rhetorical_structures\": [\n",
    "        \"Problem-solution\", \"Contribution-focused\",\n",
    "        \"Findings-centered\", \"Process-oriented\"\n",
    "    ],\n",
    "    \"terminology_density\": [\n",
    "        \"Terminology-rich\", \"Balanced\", \"Minimal Jargon\"\n",
    "    ],\n",
    "    \"temporal_context\": [\n",
    "        \"Contemporary\", \"Historical Context\", \"Future-oriented\"\n",
    "    ],\n",
    "    \"concept_blending\": [\n",
    "        \"Separate\", \"Integrated\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def create_topic_network():\n",
    "    \"\"\"Create a weighted network of topics based on similarity/co-occurrence\"\"\"\n",
    "    G = nx.Graph()\n",
    "    for topic_id, topic_data in TOPICS.items():\n",
    "        G.add_node(topic_id, name=topic_data[\"name\"], subtopics=topic_data[\"subtopics\"])\n",
    "\n",
    "    for topic1 in TOPICS:\n",
    "        for topic2 in TOPICS:\n",
    "            if topic1 != topic2:\n",
    "                subtopics1 = set(TOPICS[topic1][\"subtopics\"])\n",
    "                subtopics2 = set(TOPICS[topic2][\"subtopics\"])\n",
    "                jaccard = len(subtopics1.intersection(subtopics2)) / len(subtopics1.union(subtopics2))\n",
    "                similarity = jaccard + np.random.normal(0, 0.1)\n",
    "                similarity = max(0.05, min(0.95, similarity))\n",
    "                if not G.has_edge(topic1, topic2): # Add edge only once\n",
    "                    G.add_edge(topic1, topic2, weight=similarity)\n",
    "    return G\n",
    "\n",
    "\n",
    "def sample_concept_mix(topic_network, num_topics=None):\n",
    "    \"\"\"Sample a mix of topics from the topic network\"\"\"\n",
    "    if num_topics is None:\n",
    "        num_topics = np.random.choice([1, 2, 3], p=[0.2, 0.6, 0.2])\n",
    "\n",
    "    all_topics = list(topic_network.nodes())\n",
    "\n",
    "    if num_topics == 1:\n",
    "        topic = np.random.choice(all_topics)\n",
    "        return {topic: 1.0}\n",
    "    else:\n",
    "        selected_topics = [np.random.choice(all_topics)]\n",
    "        for _ in range(num_topics - 1):\n",
    "            all_neighbors = []\n",
    "            neighbor_weights = []\n",
    "            for t in selected_topics:\n",
    "                for n in topic_network.neighbors(t):\n",
    "                    # Ensure neighbor is not already selected and has weights\n",
    "                    if n not in selected_topics and n in topic_network[t]:\n",
    "                         # Check if neighbor exists and edge has weight data\n",
    "                        if n not in all_neighbors:\n",
    "                            all_neighbors.append(n)\n",
    "                            neighbor_weights.append(topic_network[t][n]['weight'])\n",
    "                        else:\n",
    "                            # If neighbor already listed (from another selected topic),\n",
    "                            # potentially average or sum weights? Let's just keep first found weight.\n",
    "                            pass\n",
    "\n",
    "\n",
    "            if not all_neighbors:\n",
    "                remaining = list(set(all_topics) - set(selected_topics))\n",
    "                if not remaining: break\n",
    "                selected_topics.append(np.random.choice(remaining))\n",
    "            else:\n",
    "                total_weight = sum(neighbor_weights)\n",
    "                if total_weight <= 0: # Handle cases where all weights are zero or negative\n",
    "                     normalized_weights = [1/len(neighbor_weights)] * len(neighbor_weights) # Equal probability\n",
    "                else:\n",
    "                    normalized_weights = [w / total_weight for w in neighbor_weights]\n",
    "\n",
    "                # Ensure lengths match before choice\n",
    "                if len(all_neighbors) != len(normalized_weights):\n",
    "                     print(f\"Warning: Mismatch in neighbors ({len(all_neighbors)}) and weights ({len(normalized_weights)}). Using uniform distribution.\")\n",
    "                     selected_topics.append(np.random.choice(all_neighbors))\n",
    "                else:\n",
    "                    selected_topics.append(np.random.choice(all_neighbors, p=normalized_weights))\n",
    "\n",
    "        if len(selected_topics) == 2:\n",
    "            weights = [0.7, 0.3]\n",
    "        elif len(selected_topics) == 3:\n",
    "            weights = [0.5, 0.3, 0.2]\n",
    "        else: # Handle cases where fewer than desired topics were found\n",
    "             weights = [1.0 / len(selected_topics)] * len(selected_topics)\n",
    "\n",
    "        # Ensure selected_topics and weights have the same length after sampling\n",
    "        weights = weights[:len(selected_topics)]\n",
    "        return {t: w for t, w in zip(selected_topics, weights)}\n",
    "\n",
    "    \n",
    "# --- Unified LLM Generation Function ---\n",
    "def parse_llm_response(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Attempts to parse the LLM response to extract a JSON object containing\n",
    "    'title', 'abstract', and 'keywords'. Handles surrounding text,\n",
    "    removes control characters, attempts to fix trailing commas, and validates structure.\n",
    "    Uses print statements for messages instead of logging.\n",
    "\n",
    "    Args:\n",
    "        response_text: The raw string response from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the successfully parsed and validated data\n",
    "        (e.g., {'title': ..., 'abstract': ..., 'keywords': [...]})\n",
    "        or an error dictionary {'error': ..., 'raw_response': ...} if parsing\n",
    "        or validation fails.\n",
    "    \"\"\"\n",
    "    if not response_text:\n",
    "        print(\"ERROR: LLM response was empty.\")\n",
    "        return {\"error\": \"Empty response\", \"raw_response\": \"\"}\n",
    "\n",
    "    # 1. Initial Cleaning: Strip whitespace\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    # 2. Remove control characters (e.g., ANSI color codes, non-printable chars)\n",
    "    # This regex covers ASCII control characters and some common non-ASCII ones.\n",
    "    response_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', response_text)\n",
    "\n",
    "    # 3. Remove potential markdown wrappers (```json ... ``` or ``` ... ```)\n",
    "    # Handles optional 'json' language specifier and potential leading/trailing whitespace\n",
    "    response_text = re.sub(r'^```(?:json)?\\s*', '', response_text, flags=re.MULTILINE)\n",
    "    response_text = re.sub(r'\\s*```$', '', response_text, flags=re.MULTILINE)\n",
    "    response_text = response_text.strip() # Strip again after removing wrappers\n",
    "\n",
    "    # 4. Find the main JSON structure '{...}'\n",
    "    start_index = response_text.find('{')\n",
    "    end_index = response_text.rfind('}')\n",
    "\n",
    "    if start_index == -1 or end_index == -1 or end_index < start_index:\n",
    "        # If no clear '{...}' structure, print and return error\n",
    "        error_msg = \"Could not find valid JSON structure '{...}' in the response.\"\n",
    "        print(f\"ERROR: {error_msg} Raw Response Snippet: {response_text[:200]}\")\n",
    "        return {\n",
    "            \"error\": error_msg,\n",
    "            \"raw_response\": response_text[:500] # Return first 500 chars for context\n",
    "        }\n",
    "\n",
    "    # Extract the potential JSON string\n",
    "    json_str = response_text[start_index : end_index + 1]\n",
    "\n",
    "    # 5. Attempt to fix common JSON syntax issues (specifically trailing commas)\n",
    "    # This regex removes commas just before a closing brace '}' or bracket ']'\n",
    "    json_str_cleaned = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
    "\n",
    "    # 6. Attempt to parse the cleaned JSON string\n",
    "    try:\n",
    "        parsed_data = json.loads(json_str_cleaned)\n",
    "\n",
    "        # Ensure it's a dictionary (JSON standard allows other root types)\n",
    "        if not isinstance(parsed_data, dict):\n",
    "             error_msg = f\"Parsed JSON is not a dictionary (root type: {type(parsed_data).__name__}).\"\n",
    "             print(f\"ERROR: {error_msg} Parsed Data: {str(parsed_data)[:200]}\")\n",
    "             return {\n",
    "                \"error\": error_msg,\n",
    "                \"parsed_data\": parsed_data, # Return what was parsed\n",
    "                \"raw_response\": response_text[:500]\n",
    "             }\n",
    "\n",
    "        # --- 7. Validation ---\n",
    "        required_keys = [\"title\", \"abstract\", \"keywords\"]\n",
    "        missing_keys = [key for key in required_keys if key not in parsed_data]\n",
    "\n",
    "        if missing_keys:\n",
    "            error_msg = f\"Parsed JSON missing required keys: {', '.join(missing_keys)}.\"\n",
    "            print(f\"ERROR: {error_msg} Parsed Data Keys: {list(parsed_data.keys())}\")\n",
    "            return {\n",
    "                \"error\": error_msg,\n",
    "                \"parsed_data\": parsed_data, # Return partial data\n",
    "                \"raw_response\": response_text[:500]\n",
    "            }\n",
    "\n",
    "        # Validate data types\n",
    "        if not isinstance(parsed_data.get(\"title\"), str):\n",
    "             error_msg = f\"Field 'title' is not a string (type: {type(parsed_data.get('title')).__name__}).\"\n",
    "             print(f\"ERROR: {error_msg}\")\n",
    "             return {\n",
    "                \"error\": error_msg,\n",
    "                \"parsed_data\": parsed_data,\n",
    "                \"raw_response\": response_text[:500]\n",
    "             }\n",
    "        if not isinstance(parsed_data.get(\"abstract\"), str):\n",
    "             error_msg = f\"Field 'abstract' is not a string (type: {type(parsed_data.get('abstract')).__name__}).\"\n",
    "             print(f\"ERROR: {error_msg}\")\n",
    "             return {\n",
    "                \"error\": error_msg,\n",
    "                \"parsed_data\": parsed_data,\n",
    "                \"raw_response\": response_text[:500]\n",
    "             }\n",
    "        if not isinstance(parsed_data.get(\"keywords\"), list):\n",
    "             error_msg = f\"Field 'keywords' is not a list (type: {type(parsed_data.get('keywords')).__name__}).\"\n",
    "             print(f\"ERROR: {error_msg}\")\n",
    "             # Optionally, try to convert if it's a comma-separated string, but this is less robust\n",
    "             # keywords_val = parsed_data.get(\"keywords\")\n",
    "             # if isinstance(keywords_val, str):\n",
    "             #     print(\"WARNING: Attempting to split string keywords into list.\")\n",
    "             #     parsed_data[\"keywords\"] = [k.strip() for k in keywords_val.split(',') if k.strip()]\n",
    "             # else: # If not string or list, return error\n",
    "             return {\n",
    "                \"error\": error_msg,\n",
    "                \"parsed_data\": parsed_data,\n",
    "                \"raw_response\": response_text[:500]\n",
    "             }\n",
    "        # Optional: Check if all items in keywords list are strings\n",
    "        if not all(isinstance(item, str) for item in parsed_data.get(\"keywords\", [])):\n",
    "             error_msg = \"Not all items in 'keywords' list are strings.\"\n",
    "             print(f\"ERROR: {error_msg}\")\n",
    "             return {\n",
    "                \"error\": error_msg,\n",
    "                \"parsed_data\": parsed_data,\n",
    "                \"raw_response\": response_text[:500]\n",
    "             }\n",
    "\n",
    "\n",
    "        # If all checks pass, return the validated data\n",
    "        print(\"INFO: Successfully parsed and validated LLM response.\")\n",
    "        return parsed_data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        # If parsing fails even after cleaning, provide detailed error\n",
    "        error_msg = f\"JSONDecodeError: {e}. Issue likely near char {e.pos} in the cleaned JSON snippet.\"\n",
    "\n",
    "        # Try to show the problematic part of the string for context\n",
    "        context_chars = 40\n",
    "        start = max(0, e.pos - context_chars)\n",
    "        end = min(len(json_str_cleaned), e.pos + context_chars)\n",
    "        snippet = json_str_cleaned[start:end]\n",
    "        # Add ellipsis if snippet is truncated\n",
    "        if start > 0: snippet = \"...\" + snippet\n",
    "        if end < len(json_str_cleaned): snippet = snippet + \"...\"\n",
    "        # Create a pointer line\n",
    "        pointer = \" \" * (min(e.pos, context_chars) + (3 if start > 0 else 0)) + \"^\" # Adjust pointer for ellipsis\n",
    "        error_context = f\"\\nProblematic Snippet (approx. char {e.pos}):\\n{snippet}\\n{pointer}\"\n",
    "\n",
    "        print(f\"ERROR: {error_msg}{error_context}\")\n",
    "        return {\n",
    "            \"error\": error_msg + error_context,\n",
    "            \"raw_response\": response_text[:500] # Return original raw response snippet\n",
    "        }\n",
    "    except Exception as general_e:\n",
    "        # Catch any other unexpected errors during parsing/validation\n",
    "        error_msg = f\"Unexpected error during parsing/validation: {str(general_e)}\"\n",
    "        # Print exception info directly for unexpected errors\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print traceback for unexpected errors\n",
    "        return {\n",
    "            \"error\": error_msg,\n",
    "            \"raw_response\": response_text[:500]\n",
    "        }\n",
    "\n",
    "\n",
    "def generate_topic_subtopic_specifications(topic_network, Z=10):\n",
    "    \"\"\"\n",
    "    Generate Z different topic/subtopic specifications.\n",
    "    \n",
    "    Returns a list of dictionaries, each containing:\n",
    "    - topic_mix: Dictionary mapping topic IDs to weights\n",
    "    - selected_subtopics: Dictionary mapping topic IDs to selected subtopics\n",
    "    \"\"\"\n",
    "    specifications = []\n",
    "    \n",
    "    for i in range(Z):\n",
    "        # Sample the topic mix (existing function)\n",
    "        topic_mix = sample_concept_mix(topic_network)\n",
    "        \n",
    "        # Sample a subtopic for each topic\n",
    "        selected_subtopics = {}\n",
    "        for topic_id in topic_mix:\n",
    "            # Select a subtopic randomly\n",
    "            subtopic = np.random.choice(TOPICS[topic_id][\"subtopics\"])\n",
    "            selected_subtopics[topic_id] = subtopic\n",
    "        \n",
    "        # Store the specification\n",
    "        specification = {\n",
    "            \"id\": i + 1,\n",
    "            \"topic_mix\": topic_mix,\n",
    "            \"selected_subtopics\": selected_subtopics\n",
    "        }\n",
    "        \n",
    "        specifications.append(specification)\n",
    "    \n",
    "    return specifications\n",
    "\n",
    "\n",
    "def generate_diversity_parameter_variations(topic_subtopic_specs, N=30):\n",
    "    \"\"\"\n",
    "    For each topic/subtopic specification, generate N variations of diversity parameters.\n",
    "    \n",
    "    Returns a list of dictionaries, each containing:\n",
    "    - id: Sequential ID for this abstract\n",
    "    - topic_spec_id: ID of the topic/subtopic specification\n",
    "    - topic_mix: Dictionary mapping topic IDs to weights\n",
    "    - selected_subtopics: Dictionary mapping topic IDs to selected subtopics\n",
    "    - diversity_type: \"no_diversity\", \"partial_diversity\", or \"full_diversity\"\n",
    "    - abstract_length: \"short\" or \"long\"\n",
    "    - allow_topic_mention: Boolean\n",
    "    - allow_subtopic_mention: Boolean\n",
    "    - diversity_params: Dictionary of diversity parameters\n",
    "    \"\"\"\n",
    "    variations = []\n",
    "    id_counter = 1\n",
    "    \n",
    "    for spec in topic_subtopic_specs:\n",
    "        for n in range(N):\n",
    "            # 1. Sample diversity type\n",
    "            diversity_type = np.random.choice([\"no_diversity\", \"partial_diversity\", \"full_diversity\"])\n",
    "            \n",
    "            # 2. Sample abstract length\n",
    "            abstract_length = np.random.choice([\"short\", \"long\"])\n",
    "            \n",
    "            # 3. Sample whether topics and subtopics can be mentioned\n",
    "            allow_topic_mention = np.random.choice([True, False])\n",
    "            allow_subtopic_mention = np.random.choice([True, False])\n",
    "            \n",
    "            # 4. Sample diversity parameters based on diversity type\n",
    "            diversity_params = sample_diversity_params(diversity_type)\n",
    "            \n",
    "            # Create the variation\n",
    "            variation = {\n",
    "                \"id\": id_counter,\n",
    "                \"topic_spec_id\": spec[\"id\"],\n",
    "                \"topic_mix\": spec[\"topic_mix\"],\n",
    "                \"selected_subtopics\": spec[\"selected_subtopics\"],\n",
    "                \"diversity_type\": diversity_type,\n",
    "                \"abstract_length\": abstract_length,\n",
    "                \"allow_topic_mention\": allow_topic_mention,\n",
    "                \"allow_subtopic_mention\": allow_subtopic_mention,\n",
    "                \"diversity_params\": diversity_params\n",
    "            }\n",
    "            \n",
    "            variations.append(variation)\n",
    "            id_counter += 1\n",
    "    \n",
    "    return variations\n",
    "\n",
    "\n",
    "# Updated function to handle sampling of diversity parameters\n",
    "def sample_diversity_params(experiment_type):\n",
    "    \"\"\"Sample diversity parameters for an abstract based on experiment type.\"\"\"\n",
    "    if experiment_type == \"no_diversity\":\n",
    "        # Return empty dict or None to indicate no diversity parameters\n",
    "        return None\n",
    "    elif experiment_type == \"partial_diversity\":\n",
    "        # Sample a subset of diversity parameters (choose some randomly)\n",
    "        params = {}\n",
    "        all_params = list(DIVERSITY_PARAMS.keys())\n",
    "        # Randomly select ~half of the parameters\n",
    "        selected_params = np.random.choice(all_params, size=len(all_params)//2, replace=False)\n",
    "        for param_name in selected_params:\n",
    "            params[param_name] = np.random.choice(DIVERSITY_PARAMS[param_name])\n",
    "        # Always include domain for consistency\n",
    "        params[\"domain\"] = np.random.choice(DOMAINS)\n",
    "        return params\n",
    "    else:  # Full diversity (original behavior)\n",
    "        params = {}\n",
    "        for param_name, options in DIVERSITY_PARAMS.items():\n",
    "            params[param_name] = np.random.choice(options)\n",
    "        params[\"domain\"] = np.random.choice(DOMAINS)\n",
    "        return params\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def generate_individual_responses(variations, config):\n",
    "    \"\"\"\n",
    "    Process each prompt individually using API calls, storing results\n",
    "    with provider-prefixed keys only.\n",
    "\n",
    "    Args:\n",
    "        variations: List of variation dictionaries, each including a 'prompt'.\n",
    "        config: Dictionary containing API keys, URLs, and model configurations.\n",
    "\n",
    "    Returns:\n",
    "        List of variation dictionaries, updated with provider-prefixed results\n",
    "        and validation outcomes.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    total = len(variations)\n",
    "\n",
    "    print(f\"\\nProcessing {total} variations individually...\")\n",
    "\n",
    "    # Process each variation\n",
    "    for i, variation_original in enumerate(variations):\n",
    "        prompt = variation_original.get(\"prompt\", \"Missing prompt\")\n",
    "        if prompt == \"Missing prompt\":\n",
    "             print(f\"Warning: Variation {i+1} missing prompt. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Process this variation with each configured model\n",
    "        for model_config in config.get(\"models\", []):\n",
    "            model_name = model_config.get(\"name\", \"unknown_model\")\n",
    "            provider = model_config.get(\"provider\", \"unknown_provider\")\n",
    "\n",
    "            print(f\"Processing variation {i+1}/{total} with model {model_name} ({provider})...\")\n",
    "\n",
    "            # Create a copy to store results for this specific model/provider\n",
    "            model_variation = variation_original.copy()\n",
    "            model_variation[\"model\"] = model_name\n",
    "            model_variation[\"provider\"] = provider\n",
    "\n",
    "            try:\n",
    "                # Make the API call based on provider\n",
    "                if provider == \"groq\":\n",
    "                    api_key = config.get(\"groq\", {}).get(\"api_key\")\n",
    "                    api_url = config.get(\"groq\", {}).get(\"api_url\", \"https://api.groq.com/openai/v1/chat/completions\")\n",
    "                    if not api_key:\n",
    "                        raise ValueError(\"Groq API key not found in config\")\n",
    "\n",
    "                    headers = {\n",
    "                        \"Authorization\": f\"Bearer {api_key}\",\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    }\n",
    "                    data = {\n",
    "                        \"model\": model_name,\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        \"temperature\": model_config.get(\"temperature\", 0.7),\n",
    "                        \"max_tokens\": model_config.get(\"max_tokens\", 8000) # Use model-specific max_tokens\n",
    "                    }\n",
    "\n",
    "                    response = requests.post(api_url, headers=headers, json=data)\n",
    "\n",
    "                    if response.status_code != 200:\n",
    "                        error_text = response.text[:500] # Limit error text length\n",
    "                        model_variation[f\"{provider}_error\"] = f\"API Error: {response.status_code} - {error_text}\"\n",
    "                        model_variation[\"raw_response\"] = response.text # Store full raw response on error\n",
    "                    else:\n",
    "                        response_data = response.json()\n",
    "                        content = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "                        # Parse the response content\n",
    "                        parsed_data = parse_llm_response(content)\n",
    "\n",
    "                        # Store results with provider-prefixed keys ONLY\n",
    "                        model_variation[f\"{provider}_title\"] = parsed_data.get(\"title\")\n",
    "                        model_variation[f\"{provider}_abstract\"] = parsed_data.get(\"abstract\")\n",
    "                        model_variation[f\"{provider}_keywords\"] = parsed_data.get(\"keywords\")\n",
    "                        # Store parsing error (if any) under the provider prefix\n",
    "                        model_variation[f\"{provider}_error\"] = parsed_data.get(\"error\")\n",
    "                        # Store raw response details (consider storing less if large)\n",
    "                        model_variation[\"raw_response_metadata\"] = {\n",
    "                            \"usage\": response_data.get(\"usage\"),\n",
    "                            \"id\": response_data.get(\"id\"),\n",
    "                            \"model\": response_data.get(\"model\"),\n",
    "                            # Add other relevant metadata, avoid storing full 'content' again\n",
    "                        }\n",
    "                        # Optionally store the raw parsed data if needed for debugging,\n",
    "                        # but be mindful of data size\n",
    "                        # model_variation[f\"{provider}_raw_parsed\"] = parsed_data\n",
    "\n",
    "\n",
    "                        # Apply validation using the updated function\n",
    "                        # Pass the 'model_variation' dict which now contains the prefixed results\n",
    "                        if not model_variation[f\"{provider}_error\"]: # Only validate if parsing succeeded\n",
    "                             validation_results = validate_response(model_variation, provider)\n",
    "                             model_variation.update(validation_results)\n",
    "                        else:\n",
    "                             # Add flag indicating validation was skipped due to parsing error\n",
    "                             model_variation[f\"{provider}_validation_skipped\"] = True\n",
    "\n",
    "\n",
    "                # Add other providers here with elif statements as needed\n",
    "                # elif provider == \"openai\":\n",
    "                #     # OpenAI API call logic...\n",
    "                #     # parsed_data = parse_llm_response(openai_content)\n",
    "                #     # model_variation[f\"{provider}_title\"] = parsed_data.get(\"title\")\n",
    "                #     # ... etc ...\n",
    "                #     # validation_results = validate_response(model_variation, provider)\n",
    "                #     # model_variation.update(validation_results)\n",
    "                #     pass # Placeholder\n",
    "\n",
    "                else:\n",
    "                    model_variation[f\"{provider}_error\"] = f\"Unsupported provider: {provider}\"\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch potential errors during API call or processing\n",
    "                print(f\"ERROR: Exception during processing for variation {i+1}, model {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc() # Print traceback for debugging\n",
    "                model_variation[f\"{provider}_error\"] = f\"Processing error: {str(e)}\"\n",
    "\n",
    "            # Add the result (including potential errors) to the list\n",
    "            all_results.append(model_variation)\n",
    "\n",
    "            # Optional: add delay between requests to avoid rate limits\n",
    "            time.sleep(config.get(\"request_delay\", 0.5)) # Use configurable delay\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def prepare_batch_requests(prompts, config, model_config, batch_file_path):\n",
    "    \"\"\"\n",
    "    Prepare a JSONL batch file for Groq API for a specific model,\n",
    "    writing to the provided file path.\n",
    "\n",
    "    Args:\n",
    "        prompts (list): List of prompt strings.\n",
    "        config (dict): General configuration dictionary.\n",
    "        model_config (dict): Configuration for the specific model.\n",
    "        batch_file_path (str): The full path to write the batch request file to.\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    model_name = model_config[\"name\"]\n",
    "\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # Create a request entry for each prompt\n",
    "        request = {\n",
    "            \"custom_id\": f\"{model_name}-abstract-{idx+1}\", # Used to map results back\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\", # Standard endpoint for chat\n",
    "            \"body\": {\n",
    "                \"model\": model_name,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": model_config.get(\"temperature\", 0.7),\n",
    "                \"max_tokens\": model_config.get(\"max_tokens\", 8000)\n",
    "                # Add other parameters like 'top_p', 'stop' if needed in model_config\n",
    "            }\n",
    "        }\n",
    "        # Convert the dictionary to a JSON string for the JSONL file\n",
    "        batch_requests.append(json.dumps(request))\n",
    "\n",
    "    # Write requests to the specified JSONL file path\n",
    "    # The 'batch_file_path' is now passed directly from process_groq_batch\n",
    "    try:\n",
    "        with open(batch_file_path, 'w', encoding='utf-8') as f:\n",
    "            for request_json_line in batch_requests:\n",
    "                f.write(request_json_line + '\\n')\n",
    "        print(f\"Batch request file prepared: {batch_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"ERROR: Failed to write batch request file {batch_file_path}: {e}\")\n",
    "        raise # Re-raise the exception so the calling function knows writing failed\n",
    "\n",
    "    return batch_file_path # Return the path (useful for confirmation)\n",
    "\n",
    "    \n",
    "def process_groq_batch(variations, config, model_config):\n",
    "    \"\"\"\n",
    "    Process a batch of prompts using Groq's batch API for a specific model,\n",
    "    storing results with provider-prefixed keys only.\n",
    "\n",
    "    Args:\n",
    "        variations: List of variation dictionaries for this batch.\n",
    "        config: Dictionary containing API keys, URLs.\n",
    "        model_config: Dictionary for the specific model being processed.\n",
    "        batch_file_path\n",
    "\n",
    "    Returns:\n",
    "        List of variation dictionaries from the batch, updated with provider-prefixed\n",
    "        results and validation outcomes.\n",
    "    \"\"\"\n",
    "    groq_api_key = config.get(\"groq\", {}).get(\"api_key\")\n",
    "    if not groq_api_key:\n",
    "        print(\"ERROR: Groq API key not found in config for batch processing.\")\n",
    "        # Return variations marked with error\n",
    "        results = []\n",
    "        for var in variations:\n",
    "             var_copy = var.copy()\n",
    "             var_copy[\"groq_error\"] = \"Configuration error: Missing API Key\"\n",
    "             results.append(var_copy)\n",
    "        return results\n",
    "\n",
    "    model_name = model_config.get(\"name\", \"unknown_model\")\n",
    "    provider = \"groq\" # Hardcoded for this function\n",
    "\n",
    "    # Prepare prompts list for batch file creation\n",
    "    prompts = [variation.get(\"prompt\", \"Missing prompt\") for variation in variations]\n",
    "\n",
    "    # Prepare batch file for this model\n",
    "    # Use a unique filename to avoid conflicts if run in parallel\n",
    "    batch_file_path = f\"batch_requests_{model_name}_{time.time_ns()}.jsonl\"\n",
    "    try:\n",
    "        batch_file_path = prepare_batch_requests(prompts, config, model_config, batch_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to prepare batch request file: {e}\")\n",
    "        results = []\n",
    "        for var in variations:\n",
    "             var_copy = var.copy()\n",
    "             var_copy[\"groq_error\"] = f\"Batch prep error: {e}\"\n",
    "             results.append(var_copy)\n",
    "        return results\n",
    "\n",
    "\n",
    "    print(f\"\\nProcessing batch for model {model_name}...\")\n",
    "    batch_results_list = []\n",
    "    batch_error = None\n",
    "\n",
    "    try:\n",
    "        # Upload batch file\n",
    "        print(f\"Uploading batch file {batch_file_path}...\")\n",
    "        file_upload_response = upload_file_to_groq(groq_api_key, batch_file_path)\n",
    "        input_file_id = file_upload_response[\"id\"]\n",
    "        print(f\"Uploaded file ID: {input_file_id}\")\n",
    "\n",
    "        # Create batch job\n",
    "        print(f\"Creating batch job...\")\n",
    "        batch_job_response = create_batch_job(groq_api_key, input_file_id, completion_window=\"72h\")\n",
    "        batch_id = batch_job_response[\"id\"]\n",
    "        print(f\"Batch job ID: {batch_id}\")\n",
    "\n",
    "        # Poll for completion\n",
    "        print(f\"Polling for batch completion (ID: {batch_id})...\")\n",
    "        batch_complete = False\n",
    "        poll_interval = config.get(\"batch_poll_interval\", 60) # seconds, configurable\n",
    "        max_poll_attempts = config.get(\"batch_max_poll_attempts\", 60) # e.g., 60 attempts * 60s = 1 hour\n",
    "        attempt = 0\n",
    "\n",
    "        output_file_id = None\n",
    "        while not batch_complete and attempt < max_poll_attempts:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                batch_status_response = check_batch_status(groq_api_key, batch_id)\n",
    "                status = batch_status_response[\"status\"]\n",
    "                print(f\"Batch status: {status} (Attempt {attempt}/{max_poll_attempts})\")\n",
    "\n",
    "                if status == \"completed\":\n",
    "                    batch_complete = True\n",
    "                    output_file_id = batch_status_response.get(\"output_file_id\")\n",
    "                    error_file_id = batch_status_response.get(\"error_file_id\") # Check for error file\n",
    "                    request_counts = batch_status_response.get(\"request_counts\", {})\n",
    "                    completed_count = request_counts.get('completed', 0)\n",
    "                    failed_count = request_counts.get('failed', 0)\n",
    "                    print(f\"Batch completed! Output File ID: {output_file_id}, Error File ID: {error_file_id}\")\n",
    "                    print(f\"Request Counts: {completed_count} succeeded, {failed_count} failed.\")\n",
    "                    if error_file_id:\n",
    "                         print(\"WARNING: Batch job reported an error file.\")\n",
    "                         # Potentially download and inspect error file here\n",
    "                    break # Exit polling loop\n",
    "\n",
    "                elif status in [\"failed\", \"expired\", \"cancelling\", \"cancelled\"]:\n",
    "                    batch_error = f\"Batch job {status}. Errors: {batch_status_response.get('errors')}\"\n",
    "                    print(f\"ERROR: {batch_error}\")\n",
    "                    break # Exit polling loop\n",
    "\n",
    "                else: # validating, in_progress, finalizing\n",
    "                    print(f\"Waiting {poll_interval} seconds before checking again...\")\n",
    "                    time.sleep(poll_interval)\n",
    "\n",
    "            except Exception as poll_e:\n",
    "                print(f\"ERROR: Exception during batch status polling: {poll_e}\")\n",
    "                # Decide whether to break or continue polling\n",
    "                time.sleep(poll_interval * 2) # Longer wait after error\n",
    "\n",
    "        if not batch_complete and attempt >= max_poll_attempts:\n",
    "             batch_error = f\"Batch job did not complete after {max_poll_attempts} attempts.\"\n",
    "             print(f\"ERROR: {batch_error}\")\n",
    "\n",
    "        # --- Process results if batch completed and output file exists ---\n",
    "        if batch_complete and output_file_id:\n",
    "            results_local_file = f\"batch_results_{model_name}_{batch_id}.jsonl\"\n",
    "            try:\n",
    "                print(f\"Downloading results file {output_file_id} to {results_local_file}...\")\n",
    "                results_local_file = download_batch_results(groq_api_key, output_file_id, results_local_file)\n",
    "\n",
    "                # Read and process batch results line by line\n",
    "                processed_indices = set()\n",
    "                with open(results_local_file, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            result_line = json.loads(line)\n",
    "                            custom_id = result_line.get(\"custom_id\") # e.g., \"llama-3.1-8b-instant-abstract-1\"\n",
    "                            response_body = result_line.get(\"response\", {}).get(\"body\", {})\n",
    "                            error_body = result_line.get(\"error\") # Check for line-specific errors\n",
    "\n",
    "                            if not custom_id:\n",
    "                                print(f\"Warning: Skipping result line with missing custom_id: {line[:100]}...\")\n",
    "                                continue\n",
    "\n",
    "                            # Extract original index from custom_id\n",
    "                            # Assumes format \"{model_name}-abstract-{original_index+1}\"\n",
    "                            try:\n",
    "                                import re\n",
    "                                match = re.search(r'-(\\d+)$', custom_id)\n",
    "                                if match:\n",
    "                                    original_idx = int(match.group(1)) - 1\n",
    "                                    processed_indices.add(original_idx)\n",
    "                                else:\n",
    "                                    # Handle error: couldn't parse index\n",
    "                                    print(f\"Warning: Could not parse index from custom_id '{custom_id}'. Skipping line.\")\n",
    "                                    continue\n",
    "                            except (IndexError, ValueError) as e:\n",
    "                                print(f\"Warning: Could not parse index from custom_id '{custom_id}': {e}. Skipping line.\")\n",
    "                                continue\n",
    "\n",
    "                            # Match with the original variation using the index\n",
    "                            if 0 <= original_idx < len(variations):\n",
    "                                variation_copy = variations[original_idx].copy()\n",
    "                                variation_copy[\"model\"] = model_name\n",
    "                                variation_copy[\"provider\"] = provider\n",
    "                                variation_copy[\"original_index\"] = original_idx # Keep track\n",
    "\n",
    "                                # Store raw response line for reference\n",
    "                                variation_copy[\"raw_response\"] = result_line\n",
    "\n",
    "                                if error_body:\n",
    "                                    # Handle error reported for this specific request in the batch\n",
    "                                    print(f\"Warning: Error reported for custom_id {custom_id}: {error_body}\")\n",
    "                                    variation_copy[f\"{provider}_error\"] = f\"Batch API Error: {error_body.get('message', str(error_body))}\"\n",
    "                                else:\n",
    "                                    # Extract content if no error\n",
    "                                    content = response_body.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                                    parsed_data = parse_llm_response(content)\n",
    "\n",
    "                                    # Store results with provider-prefixed keys ONLY\n",
    "                                    variation_copy[f\"{provider}_title\"] = parsed_data.get(\"title\")\n",
    "                                    variation_copy[f\"{provider}_abstract\"] = parsed_data.get(\"abstract\")\n",
    "                                    variation_copy[f\"{provider}_keywords\"] = parsed_data.get(\"keywords\")\n",
    "                                    variation_copy[f\"{provider}_error\"] = parsed_data.get(\"error\") # Parsing error\n",
    "                                    # Store raw response metadata (usage, etc.)\n",
    "                                    variation_copy[\"raw_response_metadata\"] = {\n",
    "                                        \"usage\": response_body.get(\"usage\"),\n",
    "                                        \"id\": response_body.get(\"id\"),\n",
    "                                        \"model\": response_body.get(\"model\"),\n",
    "                                        \"status_code\": result_line.get(\"response\", {}).get(\"status_code\")\n",
    "                                    }\n",
    "\n",
    "                                    # Apply validation if parsing succeeded\n",
    "                                    if not variation_copy[f\"{provider}_error\"]:\n",
    "                                        validation_results = validate_response(variation_copy, provider)\n",
    "                                        variation_copy.update(validation_results)\n",
    "                                    else:\n",
    "                                         variation_copy[f\"{provider}_validation_skipped\"] = True\n",
    "\n",
    "                                batch_results_list.append(variation_copy)\n",
    "                            else:\n",
    "                                print(f\"Warning: Index {original_idx} from custom_id '{custom_id}' is out of range for variations list (size {len(variations)}).\")\n",
    "\n",
    "                        except json.JSONDecodeError as json_e:\n",
    "                            print(f\"Warning: Failed to decode JSON from result line: {json_e}. Line: {line[:100]}...\")\n",
    "                        except Exception as line_proc_e:\n",
    "                             print(f\"ERROR: Unexpected error processing result line for custom_id {custom_id}: {line_proc_e}\")\n",
    "                             # Optionally add an error entry for this variation index if possible\n",
    "\n",
    "                # Add error entries for variations that were not in the results file\n",
    "                for i, original_variation in enumerate(variations):\n",
    "                    if i not in processed_indices:\n",
    "                        print(f\"Warning: Variation index {i} was not found in the batch results file.\")\n",
    "                        error_variation = original_variation.copy()\n",
    "                        error_variation[\"model\"] = model_name\n",
    "                        error_variation[\"provider\"] = provider\n",
    "                        error_variation[f\"{provider}_error\"] = \"No response found in batch results file\"\n",
    "                        batch_results_list.append(error_variation)\n",
    "\n",
    "            except Exception as download_e:\n",
    "                 print(f\"ERROR: Failed to download or process batch results file: {download_e}\")\n",
    "                 batch_error = f\"Results processing error: {download_e}\" # Set batch error\n",
    "\n",
    "        elif not output_file_id:\n",
    "             # Handle cases where batch completed but had no output file ID (e.g., all failed)\n",
    "             if not batch_error: # If no specific batch error was already recorded\n",
    "                 batch_error = \"Batch completed but no output file ID was generated (likely all requests failed).\"\n",
    "             print(f\"ERROR: {batch_error}\")\n",
    "\n",
    "\n",
    "    except Exception as batch_proc_e:\n",
    "        print(f\"ERROR: Major exception during batch processing pipeline for model {model_name}: {batch_proc_e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        batch_error = f\"Batch pipeline error: {batch_proc_e}\"\n",
    "\n",
    "    # If the whole batch process failed, mark all variations with the batch error\n",
    "    if batch_error and not batch_results_list: # Avoid overwriting if some results were processed\n",
    "        print(f\"Marking all variations in this batch with error: {batch_error}\")\n",
    "        for variation in variations:\n",
    "            error_variation = variation.copy()\n",
    "            error_variation[\"model\"] = model_name\n",
    "            error_variation[\"provider\"] = provider\n",
    "            error_variation[f\"{provider}_error\"] = batch_error\n",
    "            batch_results_list.append(error_variation)\n",
    "\n",
    "    # Clean up batch file? (Optional)\n",
    "    # import os\n",
    "    # try:\n",
    "    #     if os.path.exists(batch_file_path):\n",
    "    #         os.remove(batch_file_path)\n",
    "    #         print(f\"Cleaned up batch file: {batch_file_path}\")\n",
    "    # except OSError as e:\n",
    "    #     print(f\"Warning: Could not remove batch file {batch_file_path}: {e}\")\n",
    "\n",
    "    return batch_results_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_forbidden_words(variation, combined_text, provider_prefix):\n",
    "    \"\"\"Helper function to validate forbidden words in the text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of validation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check for forbidden topics (if they were forbidden)\n",
    "    if not variation[\"allow_topic_mention\"]:\n",
    "        found_forbidden_topics = []\n",
    "        for topic_id in variation[\"topic_mix\"]:\n",
    "            topic_name = TOPICS[topic_id][\"name\"].lower()\n",
    "            if topic_name in combined_text:\n",
    "                found_forbidden_topics.append(topic_name)\n",
    "        \n",
    "        results[f\"{provider_prefix}_contains_forbidden_topics\"] = len(found_forbidden_topics) > 0\n",
    "        results[f\"{provider_prefix}_found_forbidden_topics\"] = found_forbidden_topics\n",
    "    \n",
    "    # Check for forbidden subtopics (if they were forbidden)\n",
    "    if not variation[\"allow_subtopic_mention\"]:\n",
    "        found_forbidden_subtopics = []\n",
    "        for topic_id, subtopic in variation[\"selected_subtopics\"].items():\n",
    "            subtopic_lower = subtopic.lower()\n",
    "            if subtopic_lower in combined_text:\n",
    "                found_forbidden_subtopics.append(subtopic_lower)\n",
    "        \n",
    "        results[f\"{provider_prefix}_contains_forbidden_subtopics\"] = len(found_forbidden_subtopics) > 0\n",
    "        results[f\"{provider_prefix}_found_forbidden_subtopics\"] = found_forbidden_subtopics\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def upload_file_to_groq(api_key, file_path):\n",
    "    \"\"\"Upload a file to Groq API.\"\"\"\n",
    "    url = \"https://api.groq.com/openai/v1/files\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    # Prepare the file and form data\n",
    "    files = {\n",
    "        \"file\": (\"batch_file.jsonl\", open(file_path, \"rb\"))\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"purpose\": \"batch\"\n",
    "    }\n",
    "    \n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"File upload failed: {response.text}\")\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def create_batch_job(api_key, input_file_id, completion_window=\"72h\"):\n",
    "    \"\"\"Create a batch job with Groq API.\"\"\"\n",
    "    url = \"https://api.groq.com/openai/v1/batches\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": \"/v1/chat/completions\",\n",
    "        \"completion_window\": completion_window\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Batch job creation failed: {response.text}\")\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def check_batch_status(api_key, batch_id):\n",
    "    \"\"\"Check the status of a batch job.\"\"\"\n",
    "    url = f\"https://api.groq.com/openai/v1/batches/{batch_id}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Batch status check failed: {response.text}\")\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def download_batch_results(api_key, file_id, output_file):\n",
    "    \"\"\"Download batch results from Groq API.\"\"\"\n",
    "    url = f\"https://api.groq.com/openai/v1/files/{file_id}/content\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Download failed: {response.text}\")\n",
    "    \n",
    "    # Write the content to a file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(f\"File downloaded successfully to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def create_prompt(variation):\n",
    "    \"\"\"Create a prompt based on a complete parameter variation\"\"\"\n",
    "    # Extract parameters from the variation\n",
    "    topic_mix = variation[\"topic_mix\"]\n",
    "    selected_subtopics = variation[\"selected_subtopics\"]\n",
    "    diversity_type = variation[\"diversity_type\"]\n",
    "    abstract_length = variation[\"abstract_length\"]\n",
    "    allow_topic_mention = variation[\"allow_topic_mention\"]\n",
    "    allow_subtopic_mention = variation[\"allow_subtopic_mention\"]\n",
    "    diversity_params = variation[\"diversity_params\"]\n",
    "    \n",
    "    # Prepare topic information\n",
    "    formatted_topics = []\n",
    "    topic_names = []\n",
    "    forbidden_words = []\n",
    "    forbidden_subtopic_words = []\n",
    "    \n",
    "    for topic_id, weight in topic_mix.items():\n",
    "        topic_name = TOPICS[topic_id][\"name\"]\n",
    "        topic_names.append(topic_name)\n",
    "        \n",
    "        # Add topic words to forbidden list only if not allowed\n",
    "        if not allow_topic_mention:\n",
    "            forbidden_words.append(topic_name.lower())\n",
    "            forbidden_words.extend([word.lower() for word in topic_name.split()])\n",
    "        \n",
    "        # Get the subtopic\n",
    "        subtopic = selected_subtopics[topic_id]\n",
    "        \n",
    "        # Add subtopic to forbidden list only if not allowed\n",
    "        if not allow_subtopic_mention:\n",
    "            forbidden_subtopic_words.append(subtopic.lower())\n",
    "            forbidden_subtopic_words.extend([word.lower() for word in subtopic.split()])\n",
    "        \n",
    "        # Format the topic with percentage\n",
    "        percentage = int(weight * 100)\n",
    "        formatted_topics.append(f\"{topic_name} (specifically {subtopic}): {percentage}% focus\")\n",
    "    \n",
    "    # Remove duplicates from forbidden words\n",
    "    forbidden_words = list(set(forbidden_words))\n",
    "    forbidden_subtopic_words = list(set(forbidden_subtopic_words))\n",
    "    \n",
    "    # Format for prompt\n",
    "    topics_text = \"\\n\".join([f\"- {t}\" for t in formatted_topics])\n",
    "    forbidden_words_str = \", \".join([f'\"{w}\"' for w in forbidden_words]) if forbidden_words else \"None - you may use topic terms\"\n",
    "    forbidden_subtopic_words_str = \", \".join([f'\"{w}\"' for w in forbidden_subtopic_words]) if forbidden_subtopic_words else \"None - you may use subtopic terms\"\n",
    "    \n",
    "    # Determine core topic description\n",
    "    num_topics = len(topic_mix)\n",
    "    core_topic_names = [TOPICS[t]['name'] for t in topic_mix]\n",
    "    focus_description = ' AND '.join(core_topic_names) if num_topics <= 2 else 'these topics (' + ', '.join(core_topic_names) + ')'\n",
    "    \n",
    "    # Set abstract length requirement\n",
    "    min_words = 250 if abstract_length == \"short\" else 450\n",
    "    max_words = 350 if abstract_length == \"short\" else 550\n",
    "    length_requirement = f\"MINIMUM {min_words} words and MAXIMUM {max_words} words\"\n",
    "    \n",
    "    # Build diversity sections based on diversity type and parameters\n",
    "    diversity_content = \"\"\n",
    "    additional_attributes = \"\"\n",
    "    linguistics_char = \"\"\n",
    "    forbidden_domain = \"\"\n",
    "    \n",
    "    if diversity_type != \"no_diversity\" and diversity_params:\n",
    "        if \"domain\" in diversity_params:\n",
    "            diversity_content += f\"- Domain application: {diversity_params['domain']}\\n\"\n",
    "            forbidden_domain = f\"- DOMAIN: Do not use the word \\\"{diversity_params['domain']}\\\" explicitly in the abstract or title\\n\"\n",
    "            \n",
    "        if \"methodological_approaches\" in diversity_params:\n",
    "            diversity_content += f\"- Methodology: {diversity_params['methodological_approaches']}\\n\"\n",
    "            \n",
    "        if \"rhetorical_structures\" in diversity_params:\n",
    "            diversity_content += f\"- Rhetorical style: {diversity_params['rhetorical_structures']}.\\n\"\n",
    "        \n",
    "        # Build additional attributes section if those params are present\n",
    "        additional_attrs = []\n",
    "        if \"concept_granularity\" in diversity_params:\n",
    "            additional_attrs.append(f\"- Concept granularity: {diversity_params['concept_granularity']} (Reflects in the level of detail in findings)\")\n",
    "        if \"interdisciplinary_orientation\" in diversity_params:\n",
    "            additional_attrs.append(f\"- Interdisciplinary orientation: {diversity_params['interdisciplinary_orientation']} (Reflected if multiple topics are distinct)\")\n",
    "        if \"temporal_context\" in diversity_params:\n",
    "            additional_attrs.append(f\"- Temporal context: {diversity_params['temporal_context']} (Use appropriate tense/phrasing)\")\n",
    "        \n",
    "        if additional_attrs:\n",
    "            additional_attributes = \"ADDITIONAL PAPER ATTRIBUTES TO REFLECT:\\n\" + \"\\n\".join(additional_attrs) + \"\\n\\n\"\n",
    "        \n",
    "        # Build linguistic characteristics section if those params are present\n",
    "        linguistic_attrs = []\n",
    "        if \"terminology_density\" in diversity_params:\n",
    "            linguistic_attrs.append(f\"- Terminology density: {diversity_params['terminology_density']}\")\n",
    "        if \"concept_blending\" in diversity_params:\n",
    "            linguistic_attrs.append(f\"- Concept blending approach: {diversity_params['concept_blending']}\")\n",
    "        \n",
    "        if linguistic_attrs:\n",
    "            linguistics_char = \"LINGUISTIC CHARACTERISTICS TO EMBODY:\\n\" + \"\\n\".join(linguistic_attrs) + \"\\n\\n\"\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an academic expert simulating the creation of a research abstract. \n",
    "Your task is to generate ONE research abstract that fits a specific profile.\n",
    "\n",
    "**CRITICAL REQUIREMENT: The generated 'abstract' field's text MUST be {length_requirement} long.** Do not generate summaries outside this range.\n",
    "\n",
    "Your paper synthesizes the following topics. Adhere strictly to this distribution:\n",
    "{topics_text}\n",
    "\n",
    "VOCABULARY RESTRICTIONS:\n",
    "- FORBIDDEN TOPIC WORDS: {forbidden_words_str}\n",
    "- FORBIDDEN SUBTOPIC WORDS: {forbidden_subtopic_words_str}\n",
    "{forbidden_domain}\n",
    "\n",
    "REQUIRED ABSTRACT CONTENT GUIDELINES: \n",
    "- The study's focus: {focus_description}\n",
    "{diversity_content}\n",
    "\n",
    "\n",
    "{additional_attributes}{linguistics_char}MANDATORY INSTRUCTIONS:\n",
    "1. **Generate ONE academic abstract where the 'abstract' text is {length_requirement}.**\n",
    "2. Do NOT use any FORBIDDEN TOPIC WORDS in your abstract or title.\n",
    "3. Do NOT use any FORBIDDEN SUBTOPIC WORDS in your abstract or title. \n",
    "4. Strictly follow the content guidelines.\n",
    "5. Adhere to the Topic Distribution percentages.\n",
    "6. Include at least 3-5 specific, concrete findings, methods, or implications. AVOID VAGUENESS. Elaborate on points.\n",
    "7. Ensure the abstract is academically plausible and internally consistent.\n",
    "8. Do NOT mention the percentages, parameters, instructions, or section headers explicitly in the output abstract text.\n",
    "9. Do NOT write a short summary; generate a detailed, well-developed abstract fulfilling the word count requirements.\n",
    "10. **The final 'abstract' field content MUST meet the {length_requirement} requirement.**\n",
    "\n",
    "OUTPUT FORMAT:  \n",
    "Return ONLY a single, valid JSON object containing the keys 'title', 'abstract', and 'keywords'.\n",
    "- The 'title' value must be a descriptive academic title of 10-20 words\n",
    "- The 'abstract' value must be a single string containing the full abstract text ({length_requirement}).\n",
    "- The 'keywords' value must be a list of 4-6 relevant strings.\n",
    "- Do NOT include ```json markdown wrappers, comments, explanations, or any text outside the JSON structure.\n",
    "\n",
    "Example JSON structure (fill with generated content):\n",
    "{{\n",
    "  \"title\": \"A Plausible and Specific Academic Title Reflecting the Content\",\n",
    "  \"abstract\": \"Abstract text here... More text here... (Ensuring total abstract is within {length_requirement})\",\n",
    "  \"keywords\": [\"Relevant Keyword 1\", \"Keyword 2\", \"Topic Keyword\", \"Method Keyword\", \"Domain Keyword\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# --- Updated Validation Function ---\n",
    "def validate_response(variation: dict, provider_prefix: str) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive validation function that checks constraints on generated content,\n",
    "    reading input fields using the provider prefix.\n",
    "\n",
    "    Args:\n",
    "        variation: The dictionary containing original constraints AND the\n",
    "                   provider-prefixed results (e.g., 'groq_title', 'groq_abstract').\n",
    "        provider_prefix: The provider name (e.g., 'groq', 'openai') for field access.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of validation results with provider-prefixed keys.\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "\n",
    "    # Read title and abstract using the provider prefix\n",
    "    abstract_text = variation.get(f\"{provider_prefix}_abstract\", \"\").lower()\n",
    "    title_text = variation.get(f\"{provider_prefix}_title\", \"\").lower()\n",
    "\n",
    "    # Skip validation if no abstract was generated for this provider\n",
    "    if not abstract_text:\n",
    "        # Return empty results or specific flags if needed\n",
    "        return {f\"{provider_prefix}_validation_skipped\": True}\n",
    "\n",
    "    combined_text = abstract_text + \" \" + title_text\n",
    "\n",
    "    # --- Check word count ---\n",
    "    word_count = len(abstract_text.split())\n",
    "    min_words = 250 if variation.get(\"abstract_length\") == \"short\" else 450\n",
    "    max_words = 350 if variation.get(\"abstract_length\") == \"short\" else 550\n",
    "    validation_results[f\"{provider_prefix}_meets_length_requirements\"] = min_words <= word_count <= max_words\n",
    "    validation_results[f\"{provider_prefix}_word_count\"] = word_count\n",
    "\n",
    "    # --- Check for forbidden words ---\n",
    "    # Pass the combined text derived from prefixed fields\n",
    "    forbidden_words_results = validate_forbidden_words(variation, combined_text, provider_prefix)\n",
    "    validation_results.update(forbidden_words_results)\n",
    "\n",
    "    # --- Add other validations as needed ---\n",
    "    # e.g., keyword count, title length, checking specific content based on diversity params\n",
    "    keywords = variation.get(f\"{provider_prefix}_keywords\", [])\n",
    "    if isinstance(keywords, list):\n",
    "         validation_results[f\"{provider_prefix}_keyword_count\"] = len(keywords)\n",
    "         validation_results[f\"{provider_prefix}_keywords_valid_format\"] = True\n",
    "    else:\n",
    "         validation_results[f\"{provider_prefix}_keyword_count\"] = 0\n",
    "         validation_results[f\"{provider_prefix}_keywords_valid_format\"] = False\n",
    "\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "\n",
    "\n",
    "def generate_synthetic_dataset(config: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset using systematic parameter variation,\n",
    "    handling results with provider-prefixed keys and calculating relevant statistics.\n",
    "\n",
    "    Args:\n",
    "        config: Dictionary containing generation parameters, model configs, API keys, etc.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the generated data and metadata.\n",
    "    \"\"\"\n",
    "    # Initial setup - create topic network and variations\n",
    "    print(f\"Creating topic network...\")\n",
    "    topic_network = create_topic_network()\n",
    "\n",
    "    num_topic_specs = config.get('num_topic_specs', 10) # Default value\n",
    "    print(f\"\\nGenerating {num_topic_specs} topic/subtopic specifications...\")\n",
    "    topic_subtopic_specs = generate_topic_subtopic_specifications(\n",
    "        topic_network,\n",
    "        num_topic_specs\n",
    "    )\n",
    "\n",
    "    variations_per_spec = config.get('variations_per_spec', 20) # Default value\n",
    "    print(f\"\\nGenerating {variations_per_spec} diversity parameter variations for each specification...\")\n",
    "    variations = generate_diversity_parameter_variations(\n",
    "        topic_subtopic_specs,\n",
    "        variations_per_spec\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPreparing {len(variations)} prompts for processing...\")\n",
    "    # Create prompts for each variation\n",
    "    for variation in tqdm(variations, desc=\"Preparing prompts\"):\n",
    "        try:\n",
    "             variation[\"prompt\"] = create_prompt(variation)\n",
    "        except Exception as e:\n",
    "             print(f\"Error creating prompt for variation ID {variation.get('id')}: {e}\")\n",
    "             variation[\"prompt\"] = \"Error generating prompt\" # Mark problematic variations\n",
    "\n",
    "    # Choose processing mode based on configuration\n",
    "    all_results = []\n",
    "    batch_mode = config.get(\"batch_mode\", False) # Default to non-batch\n",
    "\n",
    "    if batch_mode:\n",
    "        print(\"\\nUsing BATCH mode for generation...\")\n",
    "        # Process each model configured for batch mode\n",
    "        for model_config in config.get(\"models\", []):\n",
    "            model_name = model_config.get(\"name\", \"unknown_model\")\n",
    "            provider = model_config.get(\"provider\", \"unknown_provider\")\n",
    "\n",
    "            print(f\"\\nProcessing model: {model_name} via {provider} (Batch Mode)...\")\n",
    "\n",
    "            if provider == \"groq\":\n",
    "                # Pass only the variations relevant to this batch call\n",
    "                # (assuming process_groq_batch handles the full list internally if needed,\n",
    "                # or expects just the variations list)\n",
    "                batch_results = process_groq_batch(variations, config, model_config)\n",
    "                all_results.extend(batch_results)\n",
    "\n",
    "            # Add other providers supporting batch mode here\n",
    "            # elif provider == \"openai\" and config.get(\"openai\",{}).get(\"supports_batch\"):\n",
    "            #     batch_results = process_openai_batch(variations, config, model_config)\n",
    "            #     all_results.extend(batch_results)\n",
    "            else:\n",
    "                 print(f\"Warning: Batch mode not implemented or configured for provider '{provider}'. Skipping.\")\n",
    "\n",
    "    else: # Non-batch mode\n",
    "        print(\"\\nUsing NON-BATCH mode for generation...\")\n",
    "        # Process with individual API calls\n",
    "        # generate_individual_responses handles looping through models internally\n",
    "        individual_results = generate_individual_responses(variations, config)\n",
    "        all_results.extend(individual_results)\n",
    "\n",
    "    # --- Create and save the dataset ---\n",
    "    if not all_results:\n",
    "         print(\"\\nERROR: No results were generated. Cannot create DataFrame.\")\n",
    "         return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    print(\"\\nCreating DataFrame from results...\")\n",
    "    try:\n",
    "        df = pd.DataFrame(all_results)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Failed to create DataFrame from results: {e}\")\n",
    "        print(\"Dumping raw results for inspection (first 5):\")\n",
    "        print(all_results[:5])\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    # --- Post-processing and Analysis ---\n",
    "    print(\"Post-processing DataFrame...\")\n",
    "\n",
    "    # Add topic_mix_str for easier reading if 'topic_mix' exists and is dict\n",
    "    if 'topic_mix' in df.columns:\n",
    "        df['topic_mix_str'] = df['topic_mix'].apply(\n",
    "            lambda mix: \", \".join([f\"{TOPICS.get(k, {'name':k})['name']}: {v*100:.0f}%\" for k, v in mix.items()])\n",
    "            if isinstance(mix, dict) else \"Invalid/Missing Topic Mix\"\n",
    "        )\n",
    "    else:\n",
    "        df['topic_mix_str'] = \"N/A\"\n",
    "\n",
    "\n",
    "    # Save the dataset\n",
    "    output_file = config.get(\"output_file\", \"synthetic_abstracts_output.csv\")\n",
    "    feather_output_file = output_file.replace('.csv', '.feather')\n",
    "    print(f\"\\nSaving dataset...\")\n",
    "    try:\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"Dataset saved to {output_file}\")\n",
    "        try:\n",
    "            df.to_feather(feather_output_file)\n",
    "            print(f\"Dataset saved to {feather_output_file}\")\n",
    "        except Exception as fe:\n",
    "            print(f\"Warning: Could not save to Feather format: {fe}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving dataset: {e}\")\n",
    "\n",
    "    # --- Print Summary Statistics ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  Generation Summary\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty, cannot generate summary.\")\n",
    "        return df\n",
    "\n",
    "    total_rows = len(df)\n",
    "    print(f\"Total rows (Variations * Models): {total_rows}\")\n",
    "    if 'provider' not in df.columns:\n",
    "         print(\"ERROR: 'provider' column missing, cannot generate provider-specific stats.\")\n",
    "         return df\n",
    "\n",
    "    # Count success by provider\n",
    "    print(\"\\n--- Success Rate by Provider ---\")\n",
    "    providers = df['provider'].unique()\n",
    "    for provider in providers:\n",
    "        if pd.isna(provider): continue # Skip if provider is NaN\n",
    "        provider_df = df[df['provider'] == provider].copy() # Use copy to avoid SettingWithCopyWarning\n",
    "        provider_error_col = f'{provider}_error'\n",
    "\n",
    "        if provider_error_col not in provider_df.columns:\n",
    "             print(f\"Warning: Error column '{provider_error_col}' not found for provider '{provider}'. Cannot calculate success rate.\")\n",
    "             continue\n",
    "\n",
    "        # Ensure the error column is treated correctly (NaN is success)\n",
    "        # Fill NaNs in the specific error column with a placeholder indicating success for calculation\n",
    "        provider_df[f'{provider}_error_present'] = ~provider_df[provider_error_col].isna()\n",
    "        success_count = provider_df[f'{provider}_error_present'].eq(False).sum()\n",
    "\n",
    "        total_provider = len(provider_df)\n",
    "        if total_provider > 0:\n",
    "            print(f\"{str(provider).capitalize()} Provider:\")\n",
    "            print(f\"  Total Results: {total_provider}\")\n",
    "            print(f\"  Successful Generations (No Error): {success_count} ({success_count/total_provider*100:.1f}%)\")\n",
    "            print(f\"  Failed Generations (Error Present): {total_provider - success_count} ({(total_provider - success_count)/total_provider*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{str(provider).capitalize()} Provider: No results found.\")\n",
    "\n",
    "    # Statistics by parameters (using provider-specific errors)\n",
    "    print(\"\\n--- Success Rate by Diversity Type ---\")\n",
    "    if 'diversity_type' in df.columns:\n",
    "        for dtype in df['diversity_type'].unique():\n",
    "            if pd.isna(dtype): continue\n",
    "            type_df = df[df['diversity_type'] == dtype]\n",
    "            count = len(type_df)\n",
    "            if count > 0:\n",
    "                 # Calculate success by checking the provider-specific error for each row\n",
    "                 success_count = type_df.apply(\n",
    "                     lambda row: pd.isna(row.get(f\"{row.get('provider', 'unknown')}_error\", \"Error Present\")),\n",
    "                     axis=1\n",
    "                 ).sum()\n",
    "                 print(f\"  {dtype}: {count} total, {success_count} successful ({success_count/count*100:.1f}%)\")\n",
    "            else:\n",
    "                 print(f\"  {dtype}: 0 total\")\n",
    "    else:\n",
    "        print(\"  'diversity_type' column not found.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Success Rate by Abstract Length ---\")\n",
    "    if 'abstract_length' in df.columns:\n",
    "        for length in df['abstract_length'].unique():\n",
    "             if pd.isna(length): continue\n",
    "             length_df = df[df['abstract_length'] == length]\n",
    "             count = len(length_df)\n",
    "             if count > 0:\n",
    "                 # Calculate success by checking the provider-specific error for each row\n",
    "                 success_count = length_df.apply(\n",
    "                     lambda row: pd.isna(row.get(f\"{row.get('provider', 'unknown')}_error\", \"Error Present\")),\n",
    "                     axis=1\n",
    "                 ).sum()\n",
    "                 print(f\"  {length}: {count} total, {success_count} successful ({success_count/count*100:.1f}%)\")\n",
    "             else:\n",
    "                  print(f\"  {length}: 0 total\")\n",
    "    else:\n",
    "        print(\"  'abstract_length' column not found.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Input Parameter Counts ---\")\n",
    "    if 'allow_topic_mention' in df.columns:\n",
    "        print(f\"  Topic allowed: {(df['allow_topic_mention']==True).sum()} variations\")\n",
    "        print(f\"  Topic forbidden: {(df['allow_topic_mention']==False).sum()} variations\")\n",
    "    if 'allow_subtopic_mention' in df.columns:\n",
    "        print(f\"  Subtopic allowed: {(df['allow_subtopic_mention']==True).sum()} variations\")\n",
    "        print(f\"  Subtopic forbidden: {(df['allow_subtopic_mention']==False).sum()} variations\")\n",
    "\n",
    "    # Constraint violations by provider\n",
    "    print(\"\\n--- Constraint Violations by Provider (Among Successful Generations) ---\")\n",
    "    for provider in providers:\n",
    "        if pd.isna(provider): continue\n",
    "        provider_prefix = str(provider)\n",
    "        # Filter for successful generations for this provider\n",
    "        success_df = df[\n",
    "            (df['provider'] == provider) &\n",
    "            (df[f'{provider_prefix}_error'].isna())\n",
    "        ].copy() # Use copy\n",
    "\n",
    "        total_successful = len(success_df)\n",
    "        print(f\"\\n{provider_prefix.capitalize()} Provider (Successful: {total_successful}):\")\n",
    "\n",
    "        if total_successful == 0:\n",
    "            print(\"  No successful generations to check for constraint violations.\")\n",
    "            continue\n",
    "\n",
    "        # Check forbidden topic violations\n",
    "        col_forbidden_topic = f'{provider_prefix}_contains_forbidden_topics'\n",
    "        if col_forbidden_topic in success_df.columns:\n",
    "            forbidden_topic_violations = success_df[col_forbidden_topic].eq(True).sum()\n",
    "            print(f\"  Mention forbidden topics: {forbidden_topic_violations} ({forbidden_topic_violations/total_successful*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  Column '{col_forbidden_topic}' not found.\")\n",
    "\n",
    "        # Check forbidden subtopic violations\n",
    "        col_forbidden_subtopic = f'{provider_prefix}_contains_forbidden_subtopics'\n",
    "        if col_forbidden_subtopic in success_df.columns:\n",
    "            forbidden_subtopic_violations = success_df[col_forbidden_subtopic].eq(True).sum()\n",
    "            print(f\"  Mention forbidden subtopics: {forbidden_subtopic_violations} ({forbidden_subtopic_violations/total_successful*100:.1f}%)\")\n",
    "        else:\n",
    "             print(f\"  Column '{col_forbidden_subtopic}' not found.\")\n",
    "\n",
    "        # Check length requirement violations\n",
    "        col_length_met = f'{provider_prefix}_meets_length_requirements'\n",
    "        if col_length_met in success_df.columns:\n",
    "            # Count where requirement is False\n",
    "            length_violations = success_df[col_length_met].eq(False).sum()\n",
    "            print(f\"  Don't meet length reqs: {length_violations} ({length_violations/total_successful*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  Column '{col_length_met}' not found.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  Summary End\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Updated Configuration ---\n",
    "\n",
    "CONFIG = {\n",
    "    \"num_topic_specs\": 15,      \n",
    "    \"variations_per_spec\": 30,  \n",
    "    \"output_file\": \"synthetic_abstracts_multi_model.csv\",\n",
    "    \"batch_mode\": False,  # Flag to determine batch or non-batch operation\n",
    "    \n",
    "    \"models\": [\n",
    "        {\n",
    "            \"name\": \"llama-3.1-8b-instant\",\n",
    "            \"provider\": \"groq\",\n",
    "            \"max_tokens\": 8000,\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"name\": \"llama-3.3-70b-versatile\",\n",
    "            \"provider\": \"groq\",\n",
    "            \"max_tokens\": 8000,\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"groq\": {\n",
    "        \"api_url\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        \"api_key\": groq_api_key,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "results_df = generate_synthetic_dataset(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75befffd-460b-4cab-893e-30e15e157ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
