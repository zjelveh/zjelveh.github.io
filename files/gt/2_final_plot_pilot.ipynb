{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846922f-8a72-4510-886a-83497e38d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "from plotnine import *\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set visual style and figure size defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "analyzer = pd.read_feather(\"synthetic_abstracts_multi_model.feather\")\n",
    "\n",
    "analyzer.columns = analyzer.columns.str.replace('groq_', '')\n",
    "\n",
    "data = analyzer.copy()\n",
    "\n",
    "data.shape\n",
    "data = data.dropna(subset=['abstract'])\n",
    "data.shape\n",
    "\n",
    "model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "batch_size=8\n",
    "embeddings = model.encode(\n",
    "            data.abstract.values, \n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "data['uid'] = data.id.astype(str) + '_' +  data.model\n",
    "data.uid.duplicated().sum()\n",
    "\n",
    "data.loc[data.diversity_params.isna(), 'diversity_params'] = {'concept_blending': '',\n",
    " 'concept_granularity': '',\n",
    " 'domain': '',\n",
    " 'interdisciplinary_orientation': '',\n",
    " 'methodological_approaches': '',\n",
    " 'rhetorical_structures': '',\n",
    " 'temporal_context': '',\n",
    " 'terminology_density': ''}\n",
    "\n",
    "temp = pd.json_normalize(data.diversity_params)\n",
    "\n",
    "temp.shape\n",
    "data.shape\n",
    "temp['uid'] = [i for i in range(0, temp.shape[0])]\n",
    "data['uid'] = [i for i in range(0, temp.shape[0])]\n",
    "\n",
    "b = temp.merge(data, on=['uid'])\n",
    "b.shape\n",
    "\n",
    "if hasattr(embeddings, 'cpu'):\n",
    "    embeddings_np = embeddings.cpu().numpy()\n",
    "    \n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings_np)\n",
    "\n",
    "similarity_matrix\n",
    "\n",
    "df = pd.DataFrame(similarity_matrix)\n",
    "\n",
    "df.columns = data.uid\n",
    "\n",
    "df.index = data.uid\n",
    "\n",
    "# Get the indices for the upper triangle, excluding the diagonal (k=1)\n",
    "indices = np.triu_indices_from(df, k=1)\n",
    "\n",
    "\n",
    "\n",
    "# Use these indices to extract the pairs and values\n",
    "item1 = df.index[indices[0]]\n",
    "item2 = df.columns[indices[1]]\n",
    "similarity_values = df.values[indices]\n",
    "\n",
    "# Create the result DataFrame\n",
    "df_collapsed = pd.DataFrame({'left_id': item1,\n",
    "                           'right_id': item2,\n",
    "                           'similarity': similarity_values})\n",
    "\n",
    "temp.columns\n",
    "\n",
    "comb = df_collapsed.merge(b, left_on=['left_id'], right_on=['uid'], how='left')\n",
    "comb.shape\n",
    "\n",
    "comb = comb.merge(b, left_on=['right_id'], right_on=['uid'], how='left', suffixes=['_left', '_right'])\n",
    "\n",
    "comb['same_model'] = comb.model_left==comb.model_right\n",
    "comb['same_id'] = comb.id_left==comb.id_right\n",
    "comb['same_topic_spec'] = comb.topic_spec_id_left==comb.topic_spec_id_right\n",
    "comb['contains_subtopics'] = comb.found_forbidden_subtopics_left==comb.found_forbidden_subtopics_right\n",
    "# comb['contains_topics'] = comb.found_forbidden_topics_left==comb.found_forbidden_topics_right\n",
    "comb['same_rhetoric'] = comb.rhetorical_structures_left==comb.rhetorical_structures_right\n",
    "comb['same_diversity_type'] = comb.diversity_type_left==comb.diversity_type_right\n",
    "comb['same_concept_blending'] = comb.concept_blending_left==comb.concept_blending_right\n",
    "comb['same_concept_granularity'] = comb.concept_granularity_left==comb.concept_granularity_right\n",
    "comb['same_interdisc'] = comb.interdisciplinary_orientation_left==comb.interdisciplinary_orientation_right\n",
    "comb['same_methods'] = comb.methodological_approaches_left==comb.methodological_approaches_right\n",
    "comb['same_temporal'] = comb.temporal_context_left==comb.temporal_context_right\n",
    "comb['same_term_density'] = comb.terminology_density_left==comb.terminology_density_right\n",
    "comb['same_domain'] = comb.domain_left==comb.domain_right\n",
    "\n",
    "sample_data = comb.sample(10000)\n",
    "\n",
    "n_samples=10000\n",
    "sample_data = comb.sample(n_samples, random_state=42) # Added random_state for reproducibility\n",
    "\n",
    "# --- Regression Analysis ---\n",
    "print(\"Performing regression analysis...\")\n",
    "\n",
    "# Identify predictor columns (starting with 'same_')\n",
    "predictor_cols = [col for col in sample_data.columns if col.startswith('same_')]\n",
    "\n",
    "# Define dependent variable (y) and independent variables (X)\n",
    "y = sample_data['similarity']\n",
    "X = sample_data[predictor_cols]\n",
    "\n",
    "# Convert boolean columns to integers (0 or 1) for regression\n",
    "X = X.astype(int)\n",
    "\n",
    "# Add a constant (intercept) to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the Ordinary Least Squares (OLS) model\n",
    "model_ols = sm.OLS(y, X)\n",
    "results = model_ols.fit()\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- OLS Regression Results ---\")\n",
    "print(results.summary())\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "# --- Visualize Coefficients ---\n",
    "print(\"Generating coefficient plot...\")\n",
    "# Get coefficients (excluding the constant term)\n",
    "coeffs = results.params.drop('const')\n",
    "conf_int = results.conf_int().drop('const') # Get confidence intervals\n",
    "conf_int.columns = ['lower', 'upper']\n",
    "errors = (conf_int['upper'] - conf_int['lower']) / 2 # Calculate error bar lengths\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Coefficient': coeffs,\n",
    "    'Error': errors\n",
    "}).reset_index().rename(columns={'index': 'Variable'})\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x='Coefficient', y='Variable', data=coeff_df, palette='viridis', xerr=coeff_df['Error'])\n",
    "plt.title('Regression Coefficients for Similarity Predictors (with 95% CI)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Predictor Variable')\n",
    "plt.axvline(0, color='grey', linestyle='--') # Add line at zero\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Import necessary libraries\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer # Still useful for comparison or if needed later\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel # For accessing hidden layers\n",
    "import torch # Required by transformers\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 16 # Adjust based on GPU memory\n",
    "N_SAMPLES_REGRESSION = 10000 # Number of pairs to sample for regression\n",
    "RANDOM_STATE = 42 # For reproducibility\n",
    "\n",
    "# --- Set visual style ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def mean_pooling(model_output_hidden_state, attention_mask):\n",
    "    \"\"\"\n",
    "    Performs mean pooling on token embeddings using the attention mask.\n",
    "\n",
    "    Args:\n",
    "        model_output_hidden_state: Tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "                                   from a specific layer.\n",
    "        attention_mask: Tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, hidden_size) representing pooled embeddings.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def get_all_layer_embeddings(texts, tokenizer, model, batch_size=8, device='cpu'):\n",
    "    \"\"\"\n",
    "    Encodes texts and extracts pooled embeddings from ALL layers.\n",
    "\n",
    "    Args:\n",
    "        texts (list): A list of strings to encode.\n",
    "        tokenizer: A Hugging Face tokenizer.\n",
    "        model: A Hugging Face model enabled for hidden state output.\n",
    "        batch_size (int): Processing batch size.\n",
    "        device (str): 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of numpy arrays. Each array corresponds to a layer\n",
    "              (0=input embeddings, 1=layer1, ...) and contains the\n",
    "              pooled embeddings for all input texts for that layer.\n",
    "              Shape of each array: (num_texts, hidden_size).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    num_texts = len(texts)\n",
    "    # Initialize list to store embeddings for each layer\n",
    "    all_layer_pooled_embeddings = []\n",
    "\n",
    "    print(f\"Encoding {num_texts} texts in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, num_texts, batch_size)):\n",
    "        batch_texts = texts[i:min(i + batch_size, num_texts)]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # Compute token embeddings with hidden states\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input, output_hidden_states=True)\n",
    "\n",
    "        # Get hidden states (tuple of tensors, one for each layer + input)\n",
    "        # Shape: (num_layers + 1) x (batch_size, seq_len, hidden_dim)\n",
    "        hidden_states = model_output.hidden_states\n",
    "\n",
    "        # Apply mean pooling to each layer's hidden state for this batch\n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "        if not all_layer_pooled_embeddings:\n",
    "            # Initialize the list with empty lists for each layer on first batch\n",
    "            all_layer_pooled_embeddings = [[] for _ in range(len(hidden_states))]\n",
    "\n",
    "        for layer_idx, layer_hidden_state in enumerate(hidden_states):\n",
    "            pooled = mean_pooling(layer_hidden_state, attention_mask)\n",
    "            all_layer_pooled_embeddings[layer_idx].append(pooled.cpu().numpy())\n",
    "\n",
    "    # Concatenate results from all batches for each layer\n",
    "    final_layer_embeddings = [np.vstack(layer_batches) for layer_batches in all_layer_pooled_embeddings]\n",
    "\n",
    "    print(\"Encoding complete.\")\n",
    "    return final_layer_embeddings\n",
    "\n",
    "\n",
    "def run_regression_analysis(df_collapsed_layer, data_attributes, layer_index, n_samples, random_state):\n",
    "    \"\"\"\n",
    "    Runs OLS regression for similarity based on 'same_*' attributes for a specific layer.\n",
    "\n",
    "    Args:\n",
    "        df_collapsed_layer (pd.DataFrame): DataFrame with 'left_id', 'right_id', 'similarity'.\n",
    "        data_attributes (pd.DataFrame): DataFrame 'b' containing attributes linked by 'uid'.\n",
    "        layer_index (int): The index of the layer being analyzed.\n",
    "        n_samples (int): Number of pairs to sample for regression.\n",
    "        random_state (int): Random state for sampling reproducibility.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Layer {layer_index} ---\")\n",
    "\n",
    "    # --- Merge data back for comparison ---\n",
    "    print(f\"Layer {layer_index}: Merging data for comparison...\")\n",
    "    if 'uid' not in data_attributes.columns:\n",
    "        print(\"Error: 'uid' column missing in data_attributes DataFrame 'b'. Cannot merge.\")\n",
    "        return\n",
    "\n",
    "    comb = df_collapsed_layer.merge(data_attributes, left_on=['left_id'], right_on=['uid'], how='left')\n",
    "    comb = comb.merge(data_attributes, left_on=['right_id'], right_on=['uid'], how='left', suffixes=['_left', '_right'])\n",
    "\n",
    "    # --- Create 'same_*' comparison columns ---\n",
    "    print(f\"Layer {layer_index}: Creating 'same_*' columns...\")\n",
    "    \n",
    "    def safe_compare(df, col_left, col_right):\n",
    "        if col_left in df.columns and col_right in df.columns:\n",
    "            # Handle potential NaN values before comparison if necessary\n",
    "            # For simplicity here, assume columns are comparable directly\n",
    "            # Fillna might be needed depending on data:\n",
    "            # return df[col_left].fillna('NA') == df[col_right].fillna('NA')\n",
    "            return df[col_left] == df[col_right]\n",
    "        else:\n",
    "            print(f\"Warning: Columns {col_left} or {col_right} not found for comparison. Skipping.\")\n",
    "            return pd.Series([False] * len(df), index=df.index) # Ensure index alignment\n",
    "\n",
    "    # Dynamically find all comparable columns based on suffixes\n",
    "    cols_left = [col for col in comb.columns if col.endswith('_left')]\n",
    "    base_cols = [col.replace('_left', '') for col in cols_left]\n",
    "\n",
    "    for base_col in base_cols:\n",
    "        col_left = base_col + '_left'\n",
    "        col_right = base_col + '_right'\n",
    "        # Exclude uid columns from comparison\n",
    "        if base_col not in ['uid', 'diversity_params', 'uid_base', 'raw_response', 'validation_skipped', \n",
    " 'keyword_count', 'keywords_valid_format', 'found_forbidden_subtopics', \n",
    " 'abstract_length', 'abstract', 'index', 'keywords',  'found_forbidden_topics', \n",
    " 'allow_subtopic_mention' , 'prompt', 'title', 'error', 'word_count', 'provider',\n",
    "'raw_response_metadata', 'topic_mix']: # Add other non-comparable base names\n",
    "             # Check if both columns exist before creating the 'same_' column\n",
    "             if col_left in comb.columns and col_right in comb.columns:\n",
    "                  same_col_name = f'same_{base_col}'\n",
    "                  comb[same_col_name] = safe_compare(comb, col_left, col_right)\n",
    "\n",
    "\n",
    "    # --- Sample data ---\n",
    "    actual_n_samples = min(n_samples, len(comb))\n",
    "    if actual_n_samples <= 0:\n",
    "        print(f\"Error: Layer {layer_index}: Not enough data in 'comb' DataFrame after merging to perform sampling and regression.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Layer {layer_index}: Sampling {actual_n_samples} data points...\")\n",
    "    # Check for NaN in similarity before sampling/regression\n",
    "    comb_clean = comb.dropna(subset=['similarity'])\n",
    "    if len(comb_clean) < actual_n_samples:\n",
    "        print(f\"Warning: Layer {layer_index}: Dropped {len(comb) - len(comb_clean)} rows with NaN similarity. Sampling from {len(comb_clean)} rows.\")\n",
    "        actual_n_samples = len(comb_clean)\n",
    "        if actual_n_samples == 0:\n",
    "             print(f\"Error: Layer {layer_index}: No valid similarity values left after dropping NaNs.\")\n",
    "             return\n",
    "\n",
    "    sample_data = comb_clean.sample(actual_n_samples, random_state=random_state)\n",
    "\n",
    "    # --- Regression Analysis ---\n",
    "    print(f\"Layer {layer_index}: Performing regression analysis...\")\n",
    "\n",
    "    # Identify predictor columns (starting with 'same_')\n",
    "    predictor_cols = [col for col in sample_data.columns if col.startswith('same_')]\n",
    "\n",
    "    if not predictor_cols:\n",
    "        print(f\"Error: Layer {layer_index}: No 'same_*' predictor columns found.\")\n",
    "        return\n",
    "\n",
    "    # Define dependent variable (y) and independent variables (X)\n",
    "    y = sample_data['similarity']\n",
    "    X = sample_data[predictor_cols]\n",
    "\n",
    "    # Convert boolean columns to integers (0 or 1) for regression\n",
    "    X = X.astype(int)\n",
    "\n",
    "    # Add a constant (intercept) to the model\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Check for NaN/inf values introduced (e.g., by add_constant or type conversion)\n",
    "    if X.isnull().values.any() or np.isinf(X.values).any():\n",
    "        print(f\"Error: Layer {layer_index}: NaN or Inf values found in predictor matrix X. Check data cleaning.\")\n",
    "        # Optional: print X.isnull().sum() to find problematic columns\n",
    "        return\n",
    "    if y.isnull().values.any() or np.isinf(y.values).any():\n",
    "        print(f\"Error: Layer {layer_index}: NaN or Inf values found in dependent variable y.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Fit the Ordinary Least Squares (OLS) model\n",
    "    try:\n",
    "        model_ols = sm.OLS(y, X)\n",
    "        results = model_ols.fit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Layer {layer_index}: OLS regression failed. Error: {e}\")\n",
    "        # Consider printing shapes: print(y.shape, X.shape)\n",
    "        return\n",
    "\n",
    "    # --- Display Results ---\n",
    "    print(f\"\\n--- Layer {layer_index} OLS Regression Results ---\")\n",
    "    print(results.summary())\n",
    "    print(\"------------------------------\\n\")\n",
    "\n",
    "    # --- Visualize Coefficients ---\n",
    "    print(f\"Layer {layer_index}: Generating coefficient plot...\")\n",
    "    try:\n",
    "        # Get coefficients (excluding the constant term)\n",
    "        coeffs = results.params.drop('const')\n",
    "        conf_int = results.conf_int().drop('const') # Get confidence intervals\n",
    "        conf_int.columns = ['lower', 'upper']\n",
    "        # Calculate error bar lengths (handle potential NaN in CI)\n",
    "        errors = (conf_int['upper'] - conf_int['lower']) / 2\n",
    "        errors = errors.fillna(0) # Replace NaN errors with 0 for plotting\n",
    "\n",
    "        # Create DataFrame for plotting\n",
    "        coeff_df = pd.DataFrame({\n",
    "            'Coefficient': coeffs,\n",
    "            'Error': errors\n",
    "        }).reset_index().rename(columns={'index': 'Variable'})\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, max(7, len(coeff_df) * 0.5))) # Adjust height based on number of variables\n",
    "        sns.barplot(x='Coefficient', y='Variable', data=coeff_df, palette='viridis', xerr=coeff_df['Error'])\n",
    "        plt.title(f'Layer {layer_index}: Regression Coefficients for Similarity Predictors (with 95% CI)')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.ylabel('Predictor Variable')\n",
    "        plt.axvline(0, color='grey', linestyle='--') # Add line at zero\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Layer {layer_index}: Failed to generate coefficient plot. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2e1f4-0651-4609-bbb0-aba43ed0266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# Load data (assuming the file exists in the environment)\n",
    "analyzer = pd.read_feather(\"synthetic_abstracts_multi_model.feather\")\n",
    "\n",
    "\n",
    "# Clean column names\n",
    "analyzer.columns = analyzer.columns.str.replace('groq_', '')\n",
    "\n",
    "# Copy data and handle missing abstracts\n",
    "data = analyzer.copy()\n",
    "data = data.dropna(subset=['abstract'])\n",
    "print(f\"Loaded {len(data)} rows after dropping NA abstracts.\")\n",
    "\n",
    "if len(data) < 2:\n",
    "    print(\"Error: Not enough data rows (< 2) to proceed with analysis.\")\n",
    "    # Exit or handle appropriately\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Attribute Data ('b') ---\n",
    "print(\"Preparing attribute data...\")\n",
    "# Create unique IDs\n",
    "data['uid_base'] = data.id.astype(str) + '_' + data.model.astype(str)\n",
    "\n",
    "# Handle potential duplicate UIDs more robustly - use index if needed\n",
    "if data.uid_base.duplicated().any():\n",
    "    print(\"Warning: Duplicate base UIDs (id + model) detected. Using DataFrame index as unique ID ('uid').\")\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['uid'] = data.index.astype(str)\n",
    "else:\n",
    "    data['uid'] = data['uid_base']\n",
    "\n",
    "# Handle missing diversity_params\n",
    "default_params = {\n",
    "    'concept_blending': '', 'concept_granularity': '', 'domain': '',\n",
    "    'interdisciplinary_orientation': '', 'methodological_approaches': '',\n",
    "    'rhetorical_structures': '', 'temporal_context': '', 'terminology_density': ''\n",
    "}\n",
    "data['diversity_params'] = data['diversity_params'].apply(lambda x: x if isinstance(x, dict) else default_params)\n",
    "\n",
    "# Normalize diversity_params\n",
    "temp = pd.json_normalize(data['diversity_params'])\n",
    "\n",
    "# Ensure temp and data align using index reset (important!)\n",
    "data = data.reset_index(drop=True)\n",
    "temp = temp.reset_index(drop=True)\n",
    "\n",
    "# Merge normalized params back with main data\n",
    "b = pd.concat([data, temp], axis=1)\n",
    "print(f\"Attribute data 'b' prepared with shape: {b.shape}\")\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "print(f\"Loading tokenizer and model: {MODEL_NAME}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model_hf = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    # Determine device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    exit() # Cannot proceed without model\n",
    "\n",
    "# --- Get Embeddings for All Layers ---\n",
    "all_embeddings_list = get_all_layer_embeddings(\n",
    "    data.abstract.tolist(), # Use the abstract list from the final 'data' df\n",
    "    tokenizer,\n",
    "    model_hf,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "# all_embeddings_list contains numpy arrays: [layer0_embeds, layer1_embeds, ...]\n",
    "# Number of layers = len(all_embeddings_list) - 1 (excluding input embedding layer 0)\n",
    "num_layers_output = len(all_embeddings_list)\n",
    "print(f\"Extracted embeddings for {num_layers_output} layers (including input).\")\n",
    "\n",
    "# --- Loop Through Layers for Analysis ---\n",
    "# Ensure the number of embeddings matches the attribute data 'b'\n",
    "# This check should ideally be inside the embedding function or handled robustly\n",
    "if all_embeddings_list and all_embeddings_list[0].shape[0] != b.shape[0]:\n",
    "     print(f\"FATAL ERROR: Mismatch between number of embeddings ({all_embeddings_list[0].shape[0]}) and attribute data rows ({b.shape[0]}). Aborting.\")\n",
    "     # This indicates a critical issue in data preparation or embedding steps.\n",
    "     exit()\n",
    "\n",
    "for i, layer_embeddings_np in enumerate(all_embeddings_list):\n",
    "    print(f\"\\n=======================================\")\n",
    "    print(f\"Processing Layer {i} Embeddings...\")\n",
    "    print(f\"=======================================\")\n",
    "\n",
    "    # --- Calculate Similarity Matrix ---\n",
    "    print(f\"Layer {i}: Calculating similarity matrix...\")\n",
    "    similarity_matrix = cosine_similarity(layer_embeddings_np)\n",
    "    # Ensure index/columns align with the 'uid' in 'b'\n",
    "    df_sim = pd.DataFrame(similarity_matrix, index=b['uid'], columns=b['uid'])\n",
    "\n",
    "    # --- Collapse Similarity Matrix ---\n",
    "    print(f\"Layer {i}: Collapsing similarity matrix...\")\n",
    "    indices = np.triu_indices_from(df_sim, k=1) # Exclude diagonal\n",
    "\n",
    "    # Check if indices are valid (matrix might be too small)\n",
    "    if len(indices[0]) == 0:\n",
    "        print(f\"Warning: Layer {i}: No pairs found in the upper triangle (matrix size likely <= 1). Skipping analysis for this layer.\")\n",
    "        continue\n",
    "\n",
    "    item1 = df_sim.index[indices[0]]\n",
    "    item2 = df_sim.columns[indices[1]]\n",
    "    similarity_values = df_sim.values[indices]\n",
    "\n",
    "    df_collapsed_layer = pd.DataFrame({'left_id': item1,\n",
    "                                       'right_id': item2,\n",
    "                                       'similarity': similarity_values})\n",
    "\n",
    "    # --- Run Regression for this Layer ---\n",
    "    run_regression_analysis(df_collapsed_layer, b, i, N_SAMPLES_REGRESSION, RANDOM_STATE)\n",
    "\n",
    "\n",
    "print(\"\\n--- Layer-wise Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
