{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf710ea-f34a-4ed1-a044-2009370ba13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "\n",
    "# Define models and their configurations\n",
    "MODELS = {\n",
    "    \"llama3.2\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"model_name\": \"llama3.2\",\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    "    \"llama3.1\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_name\": \"llama3-70b-8192\",\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    "    \"qwen2.5\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"model_name\": \"qwen2.5\",\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    "    \"deepseek-r1\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_name\": \"deepseek-r1-distill-qwen-32b\",\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "}\n",
    "\n",
    "# API configuration\n",
    "API_CONFIG = {\n",
    "    \"groq\": {\n",
    "        \"api_key\": 'API KEY HERE'\n",
    ",  # Set your Groq API key as environment variable\n",
    "        \"api_url\": \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    },\n",
    "    \"ollama\": {\n",
    "        \"api_url\": \"http://localhost:11434/api/chat\"  # Assumes Ollama is running locally\n",
    "    }\n",
    "}\n",
    "\n",
    "# Topic and domain definitions from your synthetic data generation\n",
    "TOPICS = {\n",
    "    \"T1\": {\n",
    "        \"name\": \"Machine Learning\",\n",
    "        \"subtopics\": [\"Neural Networks\", \"Reinforcement Learning\", \"Supervised Learning\", \n",
    "                      \"Unsupervised Learning\", \"Transfer Learning\"],\n",
    "        \"description\": \"Machine Learning involves developing algorithms and models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed.\"\n",
    "    },\n",
    "    \"T7\": {\n",
    "        \"name\": \"Sustainable Development\",\n",
    "        \"subtopics\": [\"Renewable Energy\", \"Climate Change Mitigation\", \"Resource Management\", \n",
    "                      \"Environmental Monitoring\", \"Sustainable Cities\"],\n",
    "        \"description\": \"Sustainable Development focuses on meeting present needs without compromising future generations, balancing economic growth, environmental protection, and social equity.\"\n",
    "    },\n",
    "    \"T8\": {\n",
    "        \"name\": \"Behavioral Economics\",\n",
    "        \"subtopics\": [\"Decision Making\", \"Cognitive Biases\", \"Risk Assessment\", \n",
    "                      \"Social Preferences\", \"Intertemporal Choice\"],\n",
    "        \"description\": \"Behavioral Economics studies how psychological, social, cognitive, and emotional factors influence economic decisions, challenging the assumption of perfect rationality.\"\n",
    "    },\n",
    "    \"T9\": {\n",
    "        \"name\": \"Digital Security\",\n",
    "        \"subtopics\": [\"Cybersecurity\", \"Privacy Enhancing Technologies\", \"Authentication Methods\", \n",
    "                     \"Threat Detection\", \"Security Policy\"],\n",
    "        \"description\": \"Digital Security encompasses technologies, protocols, and practices designed to protect computers, networks, programs, and data from attacks, damage, or unauthorized access.\"\n",
    "    },\n",
    "    \"T10\": {\n",
    "        \"name\": \"Public Health\",\n",
    "        \"subtopics\": [\"Epidemiology\", \"Health Promotion\", \"Disease Prevention\", \n",
    "                      \"Health Equity\", \"Health Systems\"],\n",
    "        \"description\": \"Public Health focuses on protecting and improving health at the population level through organized efforts, education, policies, and research.\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bc1f55d-519b-4247-97ee-3343935bfd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the codebook for deductive coding\n",
    "def create_codebook():\n",
    "    \"\"\"Create a structured codebook for deductive coding\"\"\"\n",
    "    \n",
    "    codebook = {\n",
    "        \"topics\": {k: {\n",
    "            \"name\": v[\"name\"],\n",
    "            \"id\": k,\n",
    "            \"subtopics\": v[\"subtopics\"],\n",
    "            \"description\": v[\"description\"]\n",
    "        } for k, v in TOPICS.items()},\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    # Add disambiguation guidelines\n",
    "    codebook[\"disambiguation_guidelines\"] = \"\"\"\n",
    "        When coding abstracts, carefully distinguish between topics and subtopics:\n",
    "    \n",
    "    TOPICS refer to the academic fields, subjects, or methodologies that form the theoretical or methodological foundation of the research. They answer \"what knowledge area is being studied or applied?\"\n",
    "    \n",
    "    SUBTOPICS refer a finer-grained version of the relevant TOPIC in the abstract\n",
    "    \n",
    "    For example, an abstract might describe using Machine Learning (TOPIC) to performed supervised learning (SUBTOPIC). Here, Machine Learning is the broad academic subject being discussed and supervised learning is the more fine-graied topic being discussed.\n",
    "    \n",
    "    Evidence for topics typically includes:\n",
    "    - Specific methodologies, theories, or frameworks from that academic field\n",
    "    - Technical terminology associated with the discipline\n",
    "    - Citations or references to literature in that field\n",
    "    \n",
    "    \n",
    "    Evidence for subtopics typically includes:\n",
    "    - Evidence of less granular topic \n",
    "    - Specific methodologies, theories, or frameworks from that academic field\n",
    "    - Technical terminology associated with the discipline\n",
    "    - Citations or references to literature in that field\n",
    "    \n",
    "    Be aware that sometimes terminology can overlap. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Add examples of topic vs domain distinction\n",
    "    codebook[\"examples\"] = [\n",
    "        {\n",
    "            \"excerpt\": \"This study employed neural networks to predict player performance based on biometric data collected during professional basketball games.\",\n",
    "            \"topic\": \"Machine Learning (specifically Neural Networks)\",\n",
    "            \"subtopic\": \"Neural networks\",\n",
    "            \"explanation\": \"Neural networks (a Machine Learning technique) is the TOPIC, while professional basketball represents the DOMAIN of Sports.\"\n",
    "        },\n",
    "        {\n",
    "            \"excerpt\": \"We explore how gamification elements on educational social media platforms affect student engagement and learning outcomes.\",\n",
    "            \"topic\": \"Behavioral Economics\",\n",
    "            \"domain\": \"Decision Making\",\n",
    "            \"explanation\": \"The study focuses on decision-making and engagement behaviors (Behavioral Economics) in the context of educational social media platforms (domains).\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cba81e76-0adf-4bbf-9af6-739e07d9b4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the deductive coding prompt\n",
    "def create_deductive_coding_prompt(codebook, abstract):\n",
    "    \"\"\"Create a prompt for deductive coding with escaped curly braces for the example JSON format\"\"\"\n",
    "    topics_json = json.dumps([{\"id\": t_id, \"name\": t_info[\"name\"], \"description\": t_info[\"description\"], \"subtopics\": t_info[\"subtopics\"]} \n",
    "                             for t_id, t_info in codebook[\"topics\"].items()], indent=2)\n",
    "    \n",
    "    # domains_json = json.dumps([{\"name\": d_info[\"name\"], \"description\": d_info[\"description\"]} \n",
    "    #                            for d_name, d_info in codebook[\"domains\"][\"subtopics\"].items()], indent=2)\n",
    "    \n",
    "    # Using triple quotes and explicit curly braces to avoid f-string formatting issues\n",
    "    prompt = f\"\"\"\n",
    "You are a highly skilled research methodologist performing deductive coding on academic abstracts. Your task is to analyze the following abstract and systematically identify both TOPICS and SUBTOPICS based on a predefined codebook.\n",
    "\n",
    "# CODEBOOK\n",
    "\n",
    "{topics_json}\n",
    "\n",
    "## DISAMBIGUATION GUIDELINES\n",
    "{codebook[\"disambiguation_guidelines\"]}\n",
    "\n",
    "# CODING TASK\n",
    "\n",
    "Analyze the following abstract:\n",
    "\n",
    "---\n",
    "{abstract}\n",
    "---\n",
    "\n",
    "Perform the following analysis:\n",
    "\n",
    "1. TOPIC IDENTIFICATION:\n",
    "   - Identify which topics (from the codebook) are present in the abstract.\n",
    "   - For each identified topic, estimate the proportion (percentage) of the abstract devoted to it. Proportions should sum to 100%.\n",
    "   - Rate your confidence for each topic identification on a scale of 1-5 (5 being highest).\n",
    "\n",
    "2. SUBTOPIC IDENTIFICATION:\n",
    "   - Identify which subtopics(s) from the codebook are represented in the abstract.\n",
    "   - Rate your confidence for domain identification on a scale of 1-5.\n",
    "\n",
    "\n",
    "Return your analysis in the following JSON format without additional text:\n",
    "\n",
    "{{\n",
    "  \"topics\": [\n",
    "    {{\n",
    "      \"topic_id\": \"topic_identifier_from_codebook\",\n",
    "      \"proportion\": percentage_as_number,\n",
    "      \"confidence\": rating_from_1_to_5\n",
    "    }}\n",
    "  ],\n",
    "  \"domains\": [\n",
    "    {{\n",
    "      \"subtopic_name\": \"subtopic_name_from_codebook\",\n",
    "      \"confidence\": rating_from_1_to_5\n",
    "    }}\n",
    "  ],\n",
    "  \"disambiguation_explanation\": \"explanation_of_how_topics_and_subtopics_were_distinguished\"\n",
    "}}\n",
    "\n",
    "Important:\n",
    "- Be precise in your identification and thorough in your evidence.\n",
    "- Ensure your analysis is based ONLY on content explicitly present in the abstract.\n",
    "- Make sure to return strictly valid JSON without any markdown formatting, additional text, or explanations.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# API request handlers with retry logic\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), \n",
    "       retry=retry_if_exception_type((requests.RequestException, json.JSONDecodeError)))\n",
    "def query_groq_api(prompt, model_config):\n",
    "    \"\"\"Send a request to Groq API with retry logic\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_CONFIG['groq']['api_key']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_config[\"model_name\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": model_config[\"max_tokens\"],\n",
    "        \"temperature\": model_config[\"temperature\"]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        API_CONFIG[\"groq\"][\"api_url\"],\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        timeout=60  # 60 second timeout\n",
    "    )\n",
    "    \n",
    "    response.raise_for_status()  # Raise an exception for 4XX/5XX responses\n",
    "    \n",
    "    result = response.json()\n",
    "    if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format from Groq: {result}\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10), \n",
    "       retry=retry_if_exception_type((requests.RequestException, json.JSONDecodeError)))\n",
    "def query_ollama_api(prompt, model_config):\n",
    "    \"\"\"Send a request to Ollama API with retry logic\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_config[\"model_name\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": model_config[\"temperature\"],\n",
    "            \"num_predict\": model_config[\"max_tokens\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        API_CONFIG[\"ollama\"][\"api_url\"],\n",
    "        json=payload,\n",
    "        timeout=60  # 60 second timeout\n",
    "    )\n",
    "    \n",
    "    response.raise_for_status()  # Raise an exception for 4XX/5XX responses\n",
    "    \n",
    "    result = response.json()\n",
    "    if \"message\" in result and \"content\" in result[\"message\"]:\n",
    "        return result[\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format from Ollama: {result}\")\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"Query the appropriate model API based on model name\"\"\"\n",
    "    model_config = MODELS[model_name]\n",
    "    provider = model_config[\"provider\"]\n",
    "    \n",
    "    try:\n",
    "        if provider == \"groq\":\n",
    "            return query_groq_api(prompt, model_config)\n",
    "        elif provider == \"ollama\":\n",
    "            return query_ollama_api(prompt, model_config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {model_name}: {str(e)}\")\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    \"\"\"Extract JSON from potentially non-JSON response text\"\"\"\n",
    "    # Try to find JSON block in the response\n",
    "    json_pattern = r'({[\\s\\S]*})'\n",
    "    json_match = re.search(json_pattern, response_text)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group(1)\n",
    "        try:\n",
    "            # Try to parse the extracted JSON\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, try to clean the JSON string\n",
    "            # Remove trailing commas\n",
    "            json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "            json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "            \n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError:\n",
    "                # If still fails, return error\n",
    "                return {\"error\": \"Could not parse JSON from response\", \"raw_response\": response_text}\n",
    "    \n",
    "    # If no JSON pattern is found\n",
    "    return {\"error\": \"No JSON found in response\", \"raw_response\": response_text}\n",
    "\n",
    "def run_deductive_coding(abstracts_df, \n",
    "                         model_names=None, \n",
    "                         max_samples=None, output_file=\"deductive_coding_results.json\"):\n",
    "    \"\"\"Run deductive coding on abstracts using multiple models\"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = list(MODELS.keys())\n",
    "    \n",
    "    # Create the codebook\n",
    "    codebook = create_codebook()\n",
    "    \n",
    "    # Select samples (all or a subset)\n",
    "    if max_samples is not None and max_samples < len(abstracts_df):\n",
    "        selected_df = abstracts_df.sample(max_samples, random_state=42)\n",
    "    else:\n",
    "        selected_df = abstracts_df\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each abstract\n",
    "    for idx, row in tqdm(selected_df.iterrows(), total=len(selected_df), desc=\"Processing abstracts\"):\n",
    "        abstract_id = row.get('id', idx)\n",
    "        \n",
    "        # Get both groq and ollama abstracts if available\n",
    "        for provider in ['groq', 'ollama']:\n",
    "            abstract_key = f\"{provider}_abstract\"\n",
    "            if abstract_key in row and pd.notna(row[abstract_key]):\n",
    "                abstract = row[abstract_key]\n",
    "                \n",
    "                # Get ground truth data for later evaluation\n",
    "                ground_truth = {\n",
    "                    \"id\": abstract_id,\n",
    "                    \"provider\": provider,\n",
    "                    \"topic_mix\": row.get('topic_mix', {}),\n",
    "                    \"topic_mix_str\": row.get('topic_mix_str', \"\"),\n",
    "                    \"diversity_params\": row.get('diversity_params', {}),\n",
    "                }\n",
    "                \n",
    "                # Extract domain from diversity params if available\n",
    "                if isinstance(ground_truth[\"diversity_params\"], dict):\n",
    "                    ground_truth[\"domain\"] = ground_truth[\"diversity_params\"].get(\"domain\", \"\")\n",
    "                elif isinstance(ground_truth[\"diversity_params\"], str):\n",
    "                    try:\n",
    "                        diversity_dict = json.loads(ground_truth[\"diversity_params\"].replace(\"'\", \"\\\"\"))\n",
    "                        ground_truth[\"domain\"] = diversity_dict.get(\"domain\", \"\")\n",
    "                    except:\n",
    "                        ground_truth[\"domain\"] = \"\"\n",
    "                \n",
    "                # Extract domain from diversity params if available\n",
    "                ground_truth[\"subtopics\"] = row['selected_subtopics']\n",
    "\n",
    "                # Process with each model\n",
    "                for model_name in model_names:\n",
    "                    print(f\"Processing {provider} abstract {abstract_id} with {model_name}\")\n",
    "                    \n",
    "                    # Create the coding prompt\n",
    "                    prompt = create_deductive_coding_prompt(codebook, abstract)\n",
    "                    \n",
    "                    # Query the model\n",
    "                    try:\n",
    "                        # Add some delay to avoid rate limiting\n",
    "                        time.sleep(random.uniform(0.5, 1.5))\n",
    "                        response = query_model(prompt, model_name)\n",
    "                        \n",
    "                        # Try to parse JSON from response\n",
    "                        coding_result = extract_json_from_response(response)\n",
    "                        \n",
    "                        # Store result with metadata\n",
    "                        result_entry = {\n",
    "                            \"abstract_id\": abstract_id,\n",
    "                            \"provider\": provider,\n",
    "                            \"model\": model_name,\n",
    "                            \"ground_truth\": ground_truth,\n",
    "                            \"coding_result\": coding_result,\n",
    "                            \"raw_response\": response,\n",
    "                            \"timestamp\": time.time()\n",
    "                        }\n",
    "                        \n",
    "                        results.append(result_entry)\n",
    "                        \n",
    "                        # Periodically save results to avoid losing data\n",
    "                        if len(results) % 10 == 0:\n",
    "                            with open(output_file, 'w') as f:\n",
    "                                json.dump(results, f, indent=2)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing abstract {abstract_id} with {model_name}: {str(e)}\")\n",
    "                        results.append({\n",
    "                            \"abstract_id\": abstract_id,\n",
    "                            \"provider\": provider,\n",
    "                            \"model\": model_name,\n",
    "                            \"ground_truth\": ground_truth,\n",
    "                            \"error\": str(e),\n",
    "                            \"timestamp\": time.time()\n",
    "                        })\n",
    "    \n",
    "    # Save final results\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Deductive coding complete. Results saved to {output_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1881007-1212-40be-a50b-0a123b7e0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the abstract data\n",
    "abstracts_file = \"https://zjelveh.github.io/files/gt/generated_df_200_censored.feather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835827f7-9dbd-4dce-aa66-55f0c217752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 abstract entries from generated_df_200_censored.feather\n"
     ]
    }
   ],
   "source": [
    "abstracts_df = pd.read_feather(abstracts_file)\n",
    "print(f\"Loaded {len(abstracts_df)} abstract entries from {abstracts_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53081ddd-2787-4ec4-aa3c-371c5db24e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run deductive coding\n",
    "# You can limit the number of samples for initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921dcb7-ce11-406a-afbf-8f7b1452ec61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50eb02c791be449fbc3d02016b0f8396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing abstracts:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing groq abstract 1 with llama3.2\n",
      "Processing groq abstract 1 with llama3.1\n",
      "Processing groq abstract 1 with qwen2.5\n",
      "Processing groq abstract 1 with deepseek-r1\n",
      "Processing ollama abstract 1 with llama3.2\n",
      "Processing ollama abstract 1 with llama3.1\n",
      "Processing ollama abstract 1 with qwen2.5\n",
      "Processing ollama abstract 1 with deepseek-r1\n",
      "Processing groq abstract 2 with llama3.2\n",
      "Processing groq abstract 2 with llama3.1\n",
      "Processing groq abstract 2 with qwen2.5\n",
      "Processing groq abstract 2 with deepseek-r1\n",
      "Processing ollama abstract 2 with llama3.2\n",
      "Processing ollama abstract 2 with llama3.1\n",
      "Processing ollama abstract 2 with qwen2.5\n",
      "Processing ollama abstract 2 with deepseek-r1\n",
      "Processing groq abstract 3 with llama3.2\n",
      "Processing groq abstract 3 with llama3.1\n",
      "Processing groq abstract 3 with qwen2.5\n",
      "Processing groq abstract 3 with deepseek-r1\n",
      "Processing ollama abstract 3 with llama3.2\n",
      "Processing ollama abstract 3 with llama3.1\n",
      "Processing ollama abstract 3 with qwen2.5\n",
      "Processing ollama abstract 3 with deepseek-r1\n",
      "Processing groq abstract 4 with llama3.2\n",
      "Processing groq abstract 4 with llama3.1\n",
      "Processing groq abstract 4 with qwen2.5\n",
      "Processing groq abstract 4 with deepseek-r1\n",
      "Processing ollama abstract 4 with llama3.2\n",
      "Processing ollama abstract 4 with llama3.1\n",
      "Processing ollama abstract 4 with qwen2.5\n",
      "Processing ollama abstract 4 with deepseek-r1\n",
      "Processing groq abstract 5 with llama3.2\n",
      "Processing groq abstract 5 with llama3.1\n",
      "Processing groq abstract 5 with qwen2.5\n",
      "Processing groq abstract 5 with deepseek-r1\n",
      "Processing ollama abstract 5 with llama3.2\n",
      "Processing ollama abstract 5 with llama3.1\n",
      "Processing ollama abstract 5 with qwen2.5\n",
      "Processing ollama abstract 5 with deepseek-r1\n",
      "Processing groq abstract 6 with llama3.2\n",
      "Processing groq abstract 6 with llama3.1\n",
      "Processing groq abstract 6 with qwen2.5\n",
      "Processing groq abstract 6 with deepseek-r1\n",
      "Processing ollama abstract 6 with llama3.2\n",
      "Processing ollama abstract 6 with llama3.1\n",
      "Processing ollama abstract 6 with qwen2.5\n",
      "Processing ollama abstract 6 with deepseek-r1\n",
      "Processing groq abstract 7 with llama3.2\n",
      "Processing groq abstract 7 with llama3.1\n",
      "Processing groq abstract 7 with qwen2.5\n",
      "Processing groq abstract 7 with deepseek-r1\n",
      "Processing ollama abstract 7 with llama3.2\n",
      "Processing ollama abstract 7 with llama3.1\n",
      "Processing ollama abstract 7 with qwen2.5\n",
      "Processing ollama abstract 7 with deepseek-r1\n",
      "Processing groq abstract 8 with llama3.2\n",
      "Processing groq abstract 8 with llama3.1\n",
      "Processing groq abstract 8 with qwen2.5\n",
      "Processing groq abstract 8 with deepseek-r1\n",
      "Processing ollama abstract 8 with llama3.2\n",
      "Processing ollama abstract 8 with llama3.1\n",
      "Processing ollama abstract 8 with qwen2.5\n",
      "Processing ollama abstract 8 with deepseek-r1\n",
      "Processing groq abstract 9 with llama3.2\n",
      "Processing groq abstract 9 with llama3.1\n",
      "Processing groq abstract 9 with qwen2.5\n",
      "Processing groq abstract 9 with deepseek-r1\n",
      "Processing ollama abstract 9 with llama3.2\n",
      "Processing ollama abstract 9 with llama3.1\n",
      "Processing ollama abstract 9 with qwen2.5\n",
      "Processing ollama abstract 9 with deepseek-r1\n",
      "Processing groq abstract 10 with llama3.2\n",
      "Processing groq abstract 10 with llama3.1\n",
      "Processing groq abstract 10 with qwen2.5\n",
      "Processing groq abstract 10 with deepseek-r1\n",
      "Processing ollama abstract 10 with llama3.2\n",
      "Processing ollama abstract 10 with llama3.1\n",
      "Processing ollama abstract 10 with qwen2.5\n",
      "Processing ollama abstract 10 with deepseek-r1\n",
      "Processing groq abstract 11 with llama3.2\n",
      "Processing groq abstract 11 with llama3.1\n",
      "Processing groq abstract 11 with qwen2.5\n",
      "Processing groq abstract 11 with deepseek-r1\n",
      "Processing ollama abstract 11 with llama3.2\n",
      "Processing ollama abstract 11 with llama3.1\n",
      "Processing ollama abstract 11 with qwen2.5\n",
      "Processing ollama abstract 11 with deepseek-r1\n",
      "Processing groq abstract 12 with llama3.2\n",
      "Processing groq abstract 12 with llama3.1\n",
      "Processing groq abstract 12 with qwen2.5\n",
      "Processing groq abstract 12 with deepseek-r1\n",
      "Processing ollama abstract 12 with llama3.2\n",
      "Processing ollama abstract 12 with llama3.1\n",
      "Processing ollama abstract 12 with qwen2.5\n",
      "Processing ollama abstract 12 with deepseek-r1\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"llama3.2\", \"llama3.1\", \"qwen2.5\", \"deepseek-r1\"]\n",
    "# max_samples = 20  # Set to None to process all abstracts\n",
    "# model_names = [\"llama3.2\", \"qwen2.5\"]\n",
    "\n",
    "results = run_deductive_coding(\n",
    "    abstracts_df, \n",
    "    model_names=model_names,\n",
    "    max_samples=None,\n",
    "    output_file=\"deductive_coding_results_censored.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
