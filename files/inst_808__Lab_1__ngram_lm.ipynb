{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983e00a2-fb30-418b-802b-b1ee57bb65cd",
   "metadata": {},
   "source": [
    "# N-gram Language Models: Understanding Context in Language\n",
    "\n",
    "This lab introduces n-gram language models, a fundamental approach to understanding and predicting text. We'll explore how these models work, their limitations, and why they led to the development of more sophisticated approaches like neural networks and transformers.\n",
    "\n",
    "## Understanding Language Models: From N-grams to Transformers\n",
    "\n",
    "Modern Large Language Models like GPT-4 and Claude appear almost magical in their ability to understand and generate human language. However, at their core, these models are attempting to solve the same fundamental challenge that motivated the earliest language models: predicting what word comes next given some context. By starting with N-gram models, we can build an intuitive understanding of this core task while encountering key concepts that remain relevant in today's most advanced models.\n",
    "\n",
    "### Why Start with N-grams?\n",
    "N-gram models represent one of the simplest approaches to modeling language: they predict the next word based on a fixed window of previous words. This straightforward approach helps us understand several fundamental concepts:\n",
    "\n",
    "1. **Context Windows**: Like N-gram models, modern transformers also process text through windows of context, though they do so in a much more sophisticated way. Understanding the limitations of fixed-context windows in N-grams helps explain why mechanisms like attention were revolutionary.\n",
    "\n",
    "2. **The Sparsity Problem**: As we increase N to capture more context, we quickly run into the problem of data sparsity - many perfectly valid word sequences never appear in our training data. This same challenge motivated the development of neural language models that can generalize beyond exact matches.\n",
    "\n",
    "3. **Probability Distributions**: N-gram models output probability distributions over possible next words. While modern LLMs use much more complex mechanisms to generate these distributions, the basic idea of modeling language as probability distributions over tokens remains the same.\n",
    "\n",
    "4. **Evaluation Metrics**: Concepts like perplexity, which we use to evaluate N-gram models, remain relevant for evaluating modern language models. Understanding these metrics helps us grasp what it means for a language model to be \"good.\"\n",
    "\n",
    "By working through this lab, you'll develop an intuition for these core concepts that will help you better understand how modern language models work. As we progress from simple counting-based N-grams to neural approaches, you'll see how each innovation helps address the limitations of previous approaches while building on the same fundamental principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413e29f-a446-4e56-a792-dd724a182db2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import our required libraries and load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b24b2-55b4-47e5-b7a8-2bd45a5995f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist, FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download required NLTK data for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a translation table to remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33a0b3-d18a-4258-b432-8ed53205571f",
   "metadata": {},
   "source": [
    "## Load our datasets\n",
    "`randomized_train` is a set of clinical trial descriptions that contain the word `randomized` in them. We will train ngram language models with this data.\n",
    "\n",
    "We will test the performance of the ngram language models on heldout data:\n",
    "`randomized_test` is a different set of clinical trial descriptions that contain the word `randomized` in them. \n",
    "`physics_test` is a set of clinical trial descriptions that contain the word `physics` in them.\n",
    "\n",
    "We should see that when evaluated on heldout data, the ngram language model should be more representative of `randomized_test` than `physics_test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270d757-9c1c-4b0b-a522-42cd4f86814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_train = pd.read_csv('https://www.dropbox.com/scl/fi/rp66akszfxugpavlh8ewy/randomized_train.csv?rlkey=ayx18lr6hdkoawksvynuydcd1&st=cq9a6rx0&dl=1')\n",
    "randomized_test = pd.read_csv('https://www.dropbox.com/scl/fi/0d0kfhafi3aju5yzejek9/randomized_test.csv?rlkey=vcfvl2vmz85qcenfpx46us9c8&st=iwd2lzws&dl=1')\n",
    "physics_test = pd.read_csv('https://www.dropbox.com/scl/fi/nat7w9dfkv7om2vsqbh38/physics_test.csv?rlkey=hoe1fzu2a9hr183lbu7a58zqr&st=0npcpsfv&dl=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f484f2-e8f1-4eb7-bb1a-637610d02284",
   "metadata": {},
   "source": [
    "## Understanding N-grams\n",
    "\n",
    "N-grams are contiguous sequences of n items from a text. In our case, these items will be words. Let's start by preparing our text data and examining unigrams (individual words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee94ab6-a473-4f29-8087-9e532ef6d867",
   "metadata": {},
   "source": [
    "### Preprocess training data.\n",
    "**Note**: This is fairly basic preprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445363f3-e7e8-4e6a-9a98-fa339e93a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and remove short words\n",
    "    text = text.lower()\n",
    "    # Only keep words 3 or more characters long\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "    # Remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ca882-42b4-41ba-abb3-62fb4871caf1",
   "metadata": {},
   "source": [
    "First, combine all descriptions into a single text using pandas `str.cat()`\n",
    "\n",
    "The `sep=' '` parameter adds a space between each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93a7dc-7e2d-462c-a058-28c9dd3ef39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_text_train = preprocess_text(randomized_train.description.str.cat(sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c813a1b-b6ad-43e8-ab16-097703b329a7",
   "metadata": {},
   "source": [
    "Now do the same for other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f241d0-b443-4bb9-9ad3-f22952df446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_text_test = preprocess_text(randomized_test.description.str.cat(sep=' '))\n",
    "physics_text_test = preprocess_text(physics_test.description.str.cat(sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119b4e3-3081-4950-bd48-616c07faf4c5",
   "metadata": {},
   "source": [
    "Convert our text into a list of tokens (words) using NLTK's tokenizer\n",
    "\n",
    "This is more sophisticated than just splitting on spaces - it handles contractions, punctuation, and other special cases properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71705ae9-d5db-4500-94e7-94bc32c04bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tokens_train = nltk.word_tokenize(randomized_text_train)\n",
    "rand_tokens_test = nltk.word_tokenize(randomized_text_test)\n",
    "physics_tokens_test = nltk.word_tokenize(physics_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678278b-c53b-421e-aa93-a4cdc7ac1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a small sample of our tokens\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "print(rand_tokens_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37132d62-e510-4712-8c93-e811686abb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic corpus statistics\n",
    "print(\"Training corpus size:\", len(rand_tokens_train), \"tokens\")\n",
    "print(\"Vocabulary size:\", len(set(rand_tokens_train)), \"unique words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53c2e6-5168-40ca-b53c-1c3e29563f29",
   "metadata": {},
   "source": [
    "### Now, let's create our language model with specified n-gram size\n",
    "Step 1: Create a list of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd12efa-04b3-4c37-9814-e493f5d34040",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list = list(ngrams(rand_tokens_train, 2))\n",
    "ngrams_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128202e6-7a06-4a67-8a4f-ac9d5544c3b2",
   "metadata": {},
   "source": [
    "Step 2: Create conditional frequency distribution\n",
    "\n",
    "For each n-gram, we use all but the last word as the condition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8bf20-068a-4cad-adea-138be2b0121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = ConditionalFreqDist((' '.join(gram[:-1]), gram[-1]) for gram in ngrams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046512e6-7c07-49a4-ae7e-531bd01d1fc0",
   "metadata": {},
   "source": [
    "`ConditionalFreqDist` is an object that says for each context (in this case a word), how many times other words follow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82db118-d811-4781-95bc-ff13599533d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the words that follow 'randomzed' the number of times it happens\n",
    "cfd['randomized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88eb9-49a0-4904-9a93-da157c230b53",
   "metadata": {},
   "source": [
    "Step 3: Convert to probabilities\n",
    "\n",
    "Note MLEProbDist is the function that finds the ngram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68228a0b-8ae3-4be1-b7bb-1d6d24e7e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd = ConditionalProbDist(cfd, MLEProbDist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd135b87-928f-4521-b27e-4107dbaf1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: P(control|randomized)\n",
    "cpd['randomized'].prob('control')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33ca6e-40d8-461a-8eb9-86b073f9faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at word frequencies/counts\n",
    "word_counts = FreqDist(rand_tokens_train)\n",
    "\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41281f60-a89d-4901-b4bb-751d72a56447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting most common words using seaborn\n",
    "most_common = pd.DataFrame(word_counts.most_common(10), columns=['word', 'count'])\n",
    "\n",
    "ax = sns.barplot(data=most_common, x='word', y='count', color='blue')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c942d-ba43-418c-8aa9-c66c83ae6acc",
   "metadata": {},
   "source": [
    "### Understanding Word Distributions\n",
    "\n",
    "Let's examine how word frequencies are distributed in our corpus. This will help us understand the challenge of data sparsity in language modeling.\n",
    "\n",
    "The big spike at 1 shows that most words in our corpus show up very few times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a6992-d1b2-49c5-b5aa-1ef13e6ee4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(list(word_counts.values()), columns=['counts'])\n",
    "counts['log_counts'] = np.log10(counts.counts)\n",
    "sns.histplot(data=counts, x='counts', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d012b15-5ae3-4ec9-968c-7f4a5c03bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plot with counts logged for visualization purposes\n",
    "sns.histplot(data=counts, x='log_counts', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef01d56-f558-48e5-b461-73ae2a6e19e7",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring Context\n",
    "Try answering these questions using the code we've written (feel free to ask an LLM for help):\n",
    "1. What are the most common words that follow \"control\" in our dataset?\n",
    "2. What percentage of bigrams appear only once?\n",
    "3. How many unique words follow \"the\" vs. a less common word like \"experiment\"?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45495bf-b639-4883-a8dd-a807d9718cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that we've already created these objects in our lab \n",
    "# that you can use to answer these questions:\n",
    "# cfd: Conditional frequency distribution\n",
    "# word_counts: Frequency distribution of individual words\n",
    "# ngrams_list: List of bigrams from our corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2599b5-8739-49df-8a9d-7183b01efae3",
   "metadata": {},
   "source": [
    "# Measuring Language Model Quality with Perplexity\n",
    "\n",
    "Perplexity is a way to evaluate how well our language model predicts text. Lower perplexity means better predictions. Intuitively, if a model is \"perplexed\" by text, it means it didn't expect those word combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfce9-a47a-408e-9864-feccd391a754",
   "metadata": {},
   "source": [
    "## Evaluating Language Models with Perplexity\n",
    "\n",
    "Now that we've built our n-gram models, we need a way to evaluate how well they work. The standard metric for evaluating language models is **perplexity**. \n",
    "\n",
    "Perplexity tells us how \"surprised\" or \"perplexed\" our model is by new text. The lower the perplexity, the better our model is at predicting the text. Here's how it works:\n",
    "\n",
    "1. For each word in our text, we:\n",
    "   - Look at the previous n-1 words (the context)\n",
    "   - Calculate how likely our model thinks the current word is\n",
    "   - Add this to our running calculation\n",
    "2. At the end, we take the geometric mean of these probabilities\n",
    "\n",
    "A few important things to note:\n",
    "- If our model thinks a word sequence is very likely (high probability), it contributes to lower perplexity\n",
    "- If our model thinks a word sequence is unlikely (low probability), it contributes to higher perplexity\n",
    "- If our model has never seen a particular sequence before, it will be very perplexed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7a382-e7dd-4237-89b9-5bfae1680ef1",
   "metadata": {},
   "source": [
    "Since we are going to experiment with different context lengths, we'll first create a function that generates a language model given tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1168b-9a0c-417b-a84a-4fc020df01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_model(tokens, n):\n",
    "    \"\"\"\n",
    "    Creates an n-gram language model from a sequence of tokens.\n",
    "    \n",
    "    The model estimates P(word_n | word_1, ..., word_{n-1}) using maximum likelihood estimation:\n",
    "    P(word_n | context) = count(context, word_n) / count(context)\n",
    "    \n",
    "    Parameters:\n",
    "        tokens: List of tokens from which to build the model\n",
    "        n: Length of n-gram sequences to consider\n",
    "    \n",
    "    Returns:\n",
    "        ConditionalProbDist object containing the probability distributions\n",
    "    \"\"\"\n",
    "    # Create n-grams from training data\n",
    "    # Each n-gram is a sequence of n consecutive tokens\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    \n",
    "    # Create conditional frequency distribution\n",
    "    # For each n-gram (w1, w2, ..., wn), we use (w1, ..., w{n-1}) as the condition\n",
    "    # and wn as the word we're trying to predict\n",
    "    # The resulting cfd maps: context -> {word: count}\n",
    "    cfd = ConditionalFreqDist(\n",
    "        (' '.join(gram[:-1]), gram[-1]) \n",
    "        for gram in ngrams_list\n",
    "    )\n",
    "    \n",
    "    # Convert raw counts to probabilities using maximum likelihood estimation\n",
    "    # For each context, P(word|context) = count(context,word) / count(context)\n",
    "    # MLEProbDist handles this normalization for us\n",
    "    cpd = ConditionalProbDist(cfd, MLEProbDist)\n",
    "    \n",
    "    return cpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e75a9-986b-4106-9b50-875abf3188f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_2 = create_ngram_model(rand_tokens_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e95846-4e40-48e8-8236-e49371c2cd2c",
   "metadata": {},
   "source": [
    "Then we will create a function that computes perplexity which takes as input:\n",
    " - a set of test tokens,\n",
    " - the ngram language model created from `create_ngram_model`\n",
    " - the value of n in the ngram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144df49f-5596-4a28-bec8-8c518aa6dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test_tokens, cpd, n):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a language model on test data.\n",
    "    \n",
    "    Perplexity is defined as: exp(-1/N * sum(log P(w_i|context_i)))\n",
    "    where N is the number of words and P(w_i|context_i) is the model's\n",
    "    predicted probability of word w_i given its context.\n",
    "    \n",
    "    Lower perplexity means the model is less \"surprised\" by the test data,\n",
    "    indicating better predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        test_tokens: List of tokens to evaluate on\n",
    "        cpd: Conditional probability distribution from training\n",
    "        n: Size of n-grams used in the model\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity score (lower is better)\n",
    "    \"\"\"\n",
    "    # Create n-grams from test data\n",
    "    test_ngrams = list(ngrams(test_tokens, n))\n",
    "    N = len(test_ngrams)\n",
    "    log_prob_sum = 0\n",
    "    \n",
    "    # Keep track of problematic sequences for analysis\n",
    "    worst_sequences = []\n",
    "    \n",
    "    for ngram in test_ngrams:\n",
    "        # Split into context and word to predict\n",
    "        context = ' '.join(ngram[:-1])  # Previous n-1 words\n",
    "        word = ngram[-1]                # Word to predict\n",
    "        \n",
    "        try:\n",
    "            # Get model's predicted probability P(word|context)\n",
    "            prob = cpd[context].prob(word)\n",
    "            \n",
    "            if prob == 0:\n",
    "                # Handle zero probabilities by assigning a small value\n",
    "                # This prevents log(0) which would be undefined\n",
    "                # 1e-7 is a form of smoothing to handle unseen sequences\n",
    "                prob = 1e-7\n",
    "                worst_sequences.append((ngram, prob))\n",
    "        except:\n",
    "            # Handle cases where we haven't seen this context before\n",
    "            prob = 1e-7\n",
    "            worst_sequences.append((ngram, prob))\n",
    "            \n",
    "        # Add log probability to our running sum\n",
    "        # We use log probabilities to prevent numerical underflow\n",
    "        # when multiplying many small probabilities\n",
    "        log_prob_sum += np.log(prob)\n",
    "    \n",
    "    # Calculate final perplexity:\n",
    "    # exp(-1/N * sum(log P(w_i|context_i)))\n",
    "    return np.exp(-log_prob_sum / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51789059-3606-45e4-b407-8bb731760ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate perplexity on our different datasets\n",
    "print(\"Perplexity on training data:\", perplexity(rand_tokens_train, cpd_2, 2))\n",
    "print(\"Perplexity on test data (same domain):\", perplexity(rand_tokens_test, cpd_2, 2))\n",
    "print(\"Perplexity on physics data (different domain):\", perplexity(physics_tokens_test, cpd_2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3e0b4-99d2-410c-9f07-9efe1df1c99a",
   "metadata": {},
   "source": [
    "Now let's see what happens when we use a trigram language model (n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4ac79-da18-4e1f-bea3-5c65ad11d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the model on training data\n",
    "cpd_3 = create_ngram_model(rand_tokens_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b1401-302e-40de-aa34-164f761da18d",
   "metadata": {},
   "source": [
    "Then evaluate on our three sets (train, test from same distribution, test from different distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5503106-0999-4992-922f-87facaa7e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate perplexity on our different datasets\n",
    "print(\"Perplexity on training data:\", perplexity(rand_tokens_train, cpd_3, 3))\n",
    "print(\"Perplexity on test data (same domain):\", perplexity(rand_tokens_test, cpd_3, 3))\n",
    "print(\"Perplexity on physics data (different domain):\", perplexity(physics_tokens_test, cpd_3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0e673-53b3-4348-9495-e33e460456f7",
   "metadata": {},
   "source": [
    "### Exercise 2: Perplexity and sparsity\n",
    "Now create a language model with n=4 and evaluate on our various datasets. \n",
    "\n",
    "- Explain why perplexity on the training set goes down as we increase n\n",
    "- Why does perplexity go up on BOTH of the test sets, even the one from the same distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6a212-e107-4b22-87e2-e60480f87d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2792197-4622-40cd-9145-648b312dc4ae",
   "metadata": {},
   "source": [
    "# From Counting to Learning: Moving Beyond N-grams\n",
    "\n",
    "Our exploration of N-gram models has revealed some fundamental challenges in modeling language. Let's examine why these limitations lead us naturally toward machine learning approaches.\n",
    "\n",
    "## The Limitations of Counting\n",
    "\n",
    "Through our experiments with N-grams, we've encountered three key challenges:\n",
    "\n",
    "First, we saw how increasing N to capture more context leads to severe data sparsity. When we moved from bigrams to trigrams, our perplexity on test data increased dramatically. This happened because many legitimate word sequences in our test data never appeared in our training set --- we simply can't observe every possible combination of words.\n",
    "\n",
    "Second, N-gram models have no way to recognize similar contexts. Consider these phrases from clinical trials:\n",
    "- \"patients were randomly assigned to\"\n",
    "- \"subjects were randomly allocated to\"\n",
    "These convey the same meaning, but our N-gram model treats them as completely different sequences. It can't recognize that \"patients\" and \"subjects\" play similar roles, or that \"assigned\" and \"allocated\" are nearly synonymous in this context.\n",
    "\n",
    "Third, N-gram models are extremely rigid in their use of context. A bigram model looking at \"The patient was\" can only use \"was\" to predict the next word, even though \"patient\" might be more informative in this case. We need a more flexible way to use context.\n",
    "\n",
    "## Why Machine Learning Helps\n",
    "\n",
    "Machine learning approaches, starting with the simple logistic regression model we'll explore next, help address these limitations in several ways:\n",
    "\n",
    "1. Parameter Sharing: Instead of treating each word sequence as unique, machine learning models can learn patterns that generalize across similar contexts. They might learn, for example, that words following \"randomly\" tend to be similar regardless of what came before.\n",
    "\n",
    "2. Feature-Based Representation: By representing words as features rather than atomic units, we can capture similarities between different contexts. This helps our model generalize to sequences it hasn't seen exactly during training.\n",
    "\n",
    "3. Learnable Context Usage: Rather than using a fixed context window, machine learning models can learn which parts of the context are most important for prediction. This flexibility helps them make better use of available information.\n",
    "\n",
    "## A Simple First Step: Logistic Regression\n",
    "\n",
    "We'll begin our exploration of machine learning approaches with logistic regression - one of the simplest machine learning models. While it won't solve most of the problems we've identified, it will help us understand how moving from counting to learning can improve our language models.\n",
    "\n",
    "Let's start by focusing on a specific prediction task: given a word, predict whether it will be followed by \"controlled\" (a key word in clinical trials). This simplified task will let us clearly see how machine learning differs from our N-gram approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04f3c5-abe1-4267-8aaa-962d2fc263bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfa469-1b28-482d-bdda-9c93e7cc4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's look at our baseline from n-grams\n",
    "print(\"N-gram probability of 'controlled' following 'randomized':\")\n",
    "print(cpd['randomized'].prob('controlled'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7156c-ca3b-45eb-af43-510882f11731",
   "metadata": {},
   "source": [
    "Now let's frame this as a binary classification problem\n",
    "For each word in our corpus, we'll create a feature indicating if it predicts 'controlled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9f2ad-dcd0-48e9-854c-4fff0cee99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "bigrams_for_logistic = [bigram[0] for bigram in ngrams_list]  # First word of each bigram\n",
    "y = [1 if bigram[1]=='controlled' else 0 for bigram in ngrams_list]  # Is second word 'controlled'?\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c014d2-365d-4354-b9d7-843b20876383",
   "metadata": {},
   "source": [
    "In the code below, the vectorizer creates a sparse matrix X where:\n",
    "- Each row represents a word (the context)\n",
    "- Each column represents a feature (presence/absence of words)\n",
    "- X[i,j] = 1 if word j appears in context i, 0 otherwise\n",
    "\n",
    "This converts our text prediction problem into a standard classification task:\n",
    "- Input: Feature vector representing the current word\n",
    "- Output: Probability distribution over possible next words (in this case, `controlled` or not `controlled`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65661ce4-c28f-4bb2-8679-707be6de6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to features using bag-of-words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(bigrams_for_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb096e-7256-421b-b808-08d7ee2c2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature matrix shape:\", X.shape)\n",
    "print(\"Number of unique words (features):\", len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c3220-dadb-46eb-a25a-171a939ef566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "clf = lr(n_jobs=10)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a71b8-898d-4871-9ebb-637229267065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine predictions for specific words\n",
    "test_words = [\"randomized\", \"random\", \"the\", \"study\", \"patients\"]\n",
    "X_test = vectorizer.transform(test_words)\n",
    "predictions = clf.predict_proba(X_test)[:, 1]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5dd86f-0529-4b23-b07d-d3943401df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProbability of 'controlled' following:\")\n",
    "for word, prob in zip(test_words, predictions):\n",
    "    \n",
    "    print(f\"From logistic regression model: {word}: {prob:.3f}\")\n",
    "    print(f\"From bigram language model: {word}: {cpd[word].prob('controlled'):.3f}\")\n",
    "    print('===========')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd38539-72ec-4343-935a-e1cf76fe87d0",
   "metadata": {},
   "source": [
    "### Multinomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958c77f-e780-4f4c-be2c-7e0c8f05e797",
   "metadata": {},
   "source": [
    "The logistic regression model generates probabilities that are very similar to the ones from the bigram language model --- which is what we want. However, it does this without resorting to simple counting. It does this via optimizing an objective function -- we'll talk more about this in the next class.  \n",
    "\n",
    "Predicting just 'controlled' is limiting. Let's extend this to predict\n",
    "the full distribution over possible next words using multinomial logistic regression.\n",
    "\n",
    "In the process we'll see that even this approach is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20df56a-5a8b-4a57-b1a7-b2c7b96abfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to more common words to make computation manageable\n",
    "min_freq = 500\n",
    "filtered_tokens = [t for t in rand_tokens_train if word_counts[t] >= min_freq]\n",
    "print(f\"\\nVocabulary size after filtering (freq >= {min_freq}):\", \n",
    "      len(set(filtered_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f36986-4d62-4235-be61-b7154b7c6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data for multinomial model\n",
    "bigrams_filtered = list(ngrams(filtered_tokens, 2))\n",
    "X_words = [bigram[0] for bigram in bigrams_filtered]\n",
    "y_next = [bigram[1] for bigram in bigrams_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae96285-9af0-447a-94c1-0c560bb40f4f",
   "metadata": {},
   "source": [
    "**Note**: `y_next` is no longer a vector of just 0s and 1s, but is now a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5206590-a031-4cae-960e-e058352d5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:20] # outcome for logistic regression where 1 means controlled follows randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85844f2-84d9-4ba3-b861-496bf5f30d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_next[:20] # outcome for multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30eb552-0813-4cb9-98ae-9fc6c3007588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to features\n",
    "vectorizer_multi = CountVectorizer()\n",
    "X_filtered = vectorizer_multi.fit_transform(X_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd50cd-183c-4cc7-9c5d-ba890d3cddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multinomial model\n",
    "clf_multi = lr(multi_class='multinomial', n_jobs=10, max_iter=500)\n",
    "clf_multi.fit(X_filtered, y_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e23e25-e40e-49a0-950f-28f3f2492910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(word, cpd, multi_model, vectorizer, top_k=10):\n",
    "    \"\"\"\n",
    "    Compares how N-gram and multinomial logistic regression models predict\n",
    "    the next word distribution.\n",
    "    \n",
    "    This function visualizes the differences between:\n",
    "    1. Simple counting-based N-gram probabilities\n",
    "    2. Learned probabilities from the multinomial model\n",
    "    \n",
    "    The comparison helps us understand how moving beyond simple counting\n",
    "    affects our probability estimates.\n",
    "    \n",
    "    Parameters:\n",
    "        word: String - The context word whose following distribution we want to examine\n",
    "        cpd: ConditionalProbDist - N-gram model containing count-based probabilities\n",
    "        multi_model: LogisticRegression - Trained multinomial model\n",
    "        vectorizer: CountVectorizer - Fitted vectorizer for processing input words\n",
    "        top_k: int - Number of top predictions to compare (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        None - Displays a plot comparing the distributions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, get the N-gram based probability distribution\n",
    "        # cpd[word] gives us the probability distribution for words following our target word\n",
    "        # .samples() returns all words that ever followed our target word in training\n",
    "        vocab = cpd[word].samples()\n",
    "        \n",
    "        # Calculate probability for each possible next word according to N-gram model\n",
    "        # This is based on simple counting: P(next|word) = count(word,next)/count(word)\n",
    "        ngram_probs = [cpd[word].prob(w) for w in vocab]\n",
    "        \n",
    "        # Now get predictions from our multinomial model\n",
    "        # First transform our word into the feature representation expected by the model\n",
    "        # This creates a sparse vector where each position represents a word in our vocabulary\n",
    "        X_pred = vectorizer.transform([word])\n",
    "        \n",
    "        # Get probability distribution over all possible next words\n",
    "        # The model uses learned weights to convert word features into probabilities\n",
    "        # predict_proba returns a matrix; we want the first (only) row\n",
    "        model_probs = multi_model.predict_proba(X_pred)[0]\n",
    "        \n",
    "        # Sort both distributions by probability and take top k\n",
    "        # zip combines words with their probabilities\n",
    "        # sorted with reverse=True gives highest probabilities first\n",
    "        ngram_top = sorted(zip(vocab, ngram_probs), \n",
    "                          key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        model_top = sorted(zip(multi_model.classes_, model_probs), \n",
    "                          key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        # Convert to DataFrames for plotting\n",
    "        # We create separate DataFrames for each model and mark their source\n",
    "        # This lets us plot them side by side for comparison\n",
    "        ngram_df = pd.DataFrame({\n",
    "            'word': [w for w, _ in ngram_top],\n",
    "            'probability': [p for _, p in ngram_top],\n",
    "            'model': ['N-gram'] * top_k  # Label these points as coming from N-gram model\n",
    "        })\n",
    "        \n",
    "        model_df = pd.DataFrame({\n",
    "            'word': [w for w, _ in model_top],\n",
    "            'probability': [p for _, p in model_top],\n",
    "            'model': ['Multinomial'] * top_k  # Label these points as coming from multinomial model\n",
    "        })\n",
    "        \n",
    "        # Combine into single DataFrame for plotting\n",
    "        # concat vertically stacks the DataFrames, keeping all columns\n",
    "        plot_df = pd.concat([ngram_df, model_df])\n",
    "        \n",
    "        # Create a grouped bar plot comparing the distributions\n",
    "        # figsize=(15, 6) makes the plot wide enough to read all labels\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        g = sns.barplot(data=plot_df, x='word', y='probability', hue='model')\n",
    "        \n",
    "        # Rotate labels for readability\n",
    "        # ha='right' aligns the rotated labels with their bars\n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.title(f'Top {top_k} Words Following \"{word}\"')\n",
    "        plt.tight_layout()  # Adjust spacing to prevent label cutoff\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If anything goes wrong (e.g., word not in vocabulary),\n",
    "        # print informative error message\n",
    "        print(f\"'{word}' not found or other error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722bf32d-9af3-4c0a-b13c-a678bc07d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for interesting words\n",
    "for word in ['randomized', 'study', 'patients']:\n",
    "    compare_distributions(word, cpd, clf_multi, vectorizer_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077fa10-9d27-4e17-93d0-77e2ec152640",
   "metadata": {},
   "source": [
    "We see bigger differences between the multionomial model and the ngram language model here. This is  driven by the fact that we dropped a bunch of words for the multinomial model but are using the ngram model that was based on all words. \n",
    "\n",
    "### Exercise 3: Standardizing the vocabulary\n",
    "To test this, create a new ngram language model that uses the same filtered vocabulary as the multinomial logistic model. And then call `compare_distributions` in the same way as above (just make sure to replace the old langauge model w/ the updated one that you created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb6ff1-bd0e-4338-9de1-9c36a5ae4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint, you want something like this to create the ngram model\n",
    "# cpd_filtered_2 = create_ngram_model(xxx, xxx)\n",
    "# just fillin the xxxs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fade4-9c1f-4740-b789-fc9e38919fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
