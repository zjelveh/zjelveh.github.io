{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3fc659-b7ac-42e8-8175-dd32a252d0d8",
   "metadata": {},
   "source": [
    "# LLM-Based Qualitative Coding Example\n",
    "\n",
    "This notebook demonstrates how to use an API to interact with language models in order to perform qualitative coding. \n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b71a29-ca8f-4dca-8f3e-89b60d80ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1781f-71e4-43dd-a48c-f5d5c0231365",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59024d13-1a74-408d-aa61-be2709f6d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mh = pd.read_csv('https://zjelveh.github.io/files/reddit_mh.csv')\n",
    "mh.columns\n",
    "mh['uid'] = [i for i in range(mh.shape[0])]\n",
    "\n",
    "# Clean text data\n",
    "mh['clean_text'] = mh['selftext'].fillna('').apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "# Print basic info about the dataset\n",
    "print(f\"Dataset shape: {mh.shape}\")\n",
    "print(\"\\nSubreddit distribution:\")\n",
    "print(mh.subreddit.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512e489-1672-4040-8a2f-234f89676b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_config = yaml.safe_load(open('https://zjelveh.github.io/files/prompt_config.yaml'))\n",
    "themes_prompt_template = prompt_config['prompts']['themes_prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcef02-154a-4089-a23f-0adb100de608",
   "metadata": {},
   "source": [
    "## Setup API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420fd84-953b-4ab0-9565-1c9bb03857d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "groq_key = 'ENTER API KEY here'\n",
    "API_KEY = groq_key\n",
    "# model = 'deepseek-r1-distill-llama-70b'\n",
    "# model = 'llama-3.2-1b-preview'\n",
    "model = 'llama-3.1-8b-instant'\n",
    "\n",
    "def query_llm(prompt, \n",
    "              model=model, \n",
    "              temperature=0.2,\n",
    "              API_KEY=groq_key,\n",
    "              max_tokens=8000):\n",
    "    # Call the Groq API\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.2,  # Low temperature for consistent coding\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    import requests\n",
    "        \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4e409-b78b-40fc-ae9f-5b155c4ab5d3",
   "metadata": {},
   "source": [
    "## Part 1: LLM-Based Inductive Coding\n",
    "\n",
    "### Step 1: Sample Posts for Theme Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6adf6-ff2d-486c-b89f-d1c558478767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of posts for initial theme identification\n",
    "theme_sample_size = 20\n",
    "theme_sample = mh.sample(n=theme_sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e570a-cdaa-4839-9fda-afdff203addf",
   "metadata": {},
   "source": [
    "### Step 2: Identify Themes in the Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97671d0-872e-4c2b-b8db-d92f068d4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches of posts for theme identification\n",
    "batch_size = 5\n",
    "batches = [theme_sample[i:i+batch_size] for i in range(0, len(theme_sample), batch_size)]\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4bbe3-c72e-44d6-a7ef-774a8342ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a batch and extract themes\n",
    "def process_batch(batch, batch_num):\n",
    "    # Prepare posts text\n",
    "    posts_text = \"\"\n",
    "    for j, (_, row) in enumerate(batch.iterrows()):\n",
    "        posts_text += f\"POST {j+1} (from r/{row['subreddit']}):\\n\"\n",
    "        posts_text += f\"Title: {row['title']}\\n\"\n",
    "        posts_text += f\"Content: {row['clean_text']}...\\n\\n\"  # Truncate long posts\n",
    "\n",
    "    # Create prompt\n",
    "    \n",
    "    # This updates themes_prompt_template variable and replaces the placeholders\n",
    "    # for {batch_size} and {posts} with actual data\n",
    "    prompt = themes_prompt_template.format(\n",
    "        batch_size=len(batch),\n",
    "        posts=posts_text\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Get response from LLM\n",
    "        print(f\"Processing batch {batch_num + 1}/{len(batches)}...\")\n",
    "        result = query_llm(prompt)\n",
    "        result = result.json()\n",
    "        response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON from response: {e}\")\n",
    "        print(\"Response was:\", response)\n",
    "        return {\"themes\": []}    \n",
    "    # Extract JSON from response\n",
    "    \n",
    "    try:\n",
    "        # Find JSON block in response\n",
    "        json_text = response.split(\"```json\")[1].split(\"```\")[0] if \"```json\" in response else response\n",
    "        batch_themes = json.loads(json_text)\n",
    "        return batch_themes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON from response: {e}\")\n",
    "        print(\"Response was:\", response)\n",
    "        return {\"themes\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9d9b1-bbc1-4c4e-8894-7ba120b78aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_batch_themes = []\n",
    "for i, b in enumerate(batches):\n",
    "    all_batch_themes.append(process_batch(b, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5352a5-f4e5-4fc3-b279-a052c26d1a37",
   "metadata": {},
   "source": [
    "### Step 3: Consolidate Themes into a Codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f5445-5ec8-479e-8df3-0905d61dfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_themes_prompt = prompt_config['prompts']['consolidate_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2774b3d-a7b5-4961-8da8-c85ec148de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch themes text for insertion into prompt\n",
    "batch_themes_text = \"\"\n",
    "\n",
    "for i, batch_result in enumerate(all_batch_themes):\n",
    "    batch_themes_text += f\"BATCH {i+1}:\\n\"\n",
    "\n",
    "    for theme in batch_result.get(\"themes\", []):\n",
    "        batch_themes_text += f\"- Theme: {theme.get('name', 'Unnamed')}\\n\"\n",
    "        batch_themes_text += f\"  Description: {theme.get('description', 'No description')}\\n\"\n",
    "        batch_themes_text += f\"  Indicators: {', '.join(theme.get('indicators', []))}\\n\"\n",
    "    batch_themes_text += \"\\n\"\n",
    "    \n",
    "\n",
    "# Function to process a batch and extract themes\n",
    "print(f\"Consolidating\")\n",
    "result = query_llm(consolidated_themes_prompt)\n",
    "\n",
    "result = result.json()\n",
    "response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Extract JSON from response\n",
    "try:\n",
    "    # Find JSON block in response\n",
    "    json_text = response.split(\"```json\")[1].split(\"```\")[0] if \"```json\" in response else response\n",
    "    codebook = json.loads(json_text)\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing JSON from response: {e}\")\n",
    "    print(\"Response was:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184eadf1-d185-4ac8-b7e6-e733d93cf675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Successfully created consolidated codebook with the following codes:\")\n",
    "for code_entry in codebook[\"codebook\"]:\n",
    "    print(f\"- {code_entry['code']}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ba8f6-4319-4140-b80a-d14dea076004",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a779b-f163-4142-b7a1-44821e27dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save codebook for later use\n",
    "with open(f'llm_codebook_{model}', 'w') as f:\n",
    "    json.dump(codebook, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127ceab-918b-49ab-bb03-b079877c1723",
   "metadata": {},
   "source": [
    "### Step 4: Apply Codes to Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024403c-412e-4f37-afcd-0ba2241f2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "deduction_prompt_template = prompt_config['prompts']['deduction_prompt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8463e3-9ead-48b2-815d-58abe227746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a formatted list of codes from the codebook\n",
    "codes_list = \"\\n\".join([\n",
    "    f\"{i+1}. {code['code']}: {code['description']}\\n   Indicators: {', '.join(code['indicators'])}\"\n",
    "    for i, code in enumerate(codebook.get(\"codebook\", []))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55398eb-01cb-4983-8a07-0cc047d86c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c114cb6-d2eb-4617-a33e-7a7d51f747c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run codes on all posts\n",
    "coding_sample = mh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adc961-2bc4-4449-a3d4-a90b73f0b9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply codes and save results\n",
    "print(f\"Applying codes to {coding_sample.shape[0]} posts...\")\n",
    "coding_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe92e6-1a54-43d0-92c5-321ee58c13ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in tqdm(range(max(0, len(coding_results)), len(coding_sample))):\n",
    "    row = coding_sample.iloc[idx]\n",
    "    \n",
    "    title = row['title']\n",
    "    text = row['clean_text']\n",
    "    deduction_prompt = deduction_prompt_template.format(\n",
    "        codes_list=codes_list,\n",
    "        post_title=title,\n",
    "        post_text=text\n",
    "    )\n",
    "    \n",
    "    # Default empty result structure\n",
    "    parsed_result = {\n",
    "        \"assigned_codes\": [],\n",
    "        \"post_title\": title,\n",
    "        \"post_id\": idx,\n",
    "        \"uid\": coding_sample.index[idx]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Query LLM and parse response\n",
    "        response = query_llm(prompt=deduction_prompt)\n",
    "        result = response.json()\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post {idx}: {e}\")\n",
    "        print(response)\n",
    "        print(result)\n",
    "        time.sleep(5)\n",
    "        # Query LLM and parse response\n",
    "        response = query_llm(prompt=deduction_prompt)\n",
    "        result = response.json()\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "    try:\n",
    "        json_text = (\n",
    "            content.split(\"```json\")[1].split(\"```\")[0].strip() if \"```json\" in content\n",
    "            else content.split(\"```\")[1].split(\"```\")[0].strip() if \"```\" in content\n",
    "            else content\n",
    "        )\n",
    "\n",
    "        parsed_json = json.loads(json_text)\n",
    "\n",
    "        parsed_result[\"assigned_codes\"] = parsed_json[\"assigned_codes\"]    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post {idx}: {e}\")\n",
    "    \n",
    "    # Add result to collection\n",
    "    coding_results.append(parsed_result)\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(0.75)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0386b85-7f84-432c-889b-cfc0ef5b8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import json_normalize\n",
    "\n",
    "# Extract the post-level information\n",
    "codes_data = []\n",
    "\n",
    "for post_result in coding_results:\n",
    "    # Extract code information for this post\n",
    "    for code_info in post_result['assigned_codes']:\n",
    "        code_entry = {\n",
    "            'uid': post_result.get('uid'),  # Link back to original mh index\n",
    "            'code': code_info.get('code'),\n",
    "            'confidence': code_info.get('confidence'),\n",
    "            'reasoning': code_info.get('reasoning'),\n",
    "            'evidence': code_info.get('evidence')\n",
    "        }\n",
    "        codes_data.append(code_entry)\n",
    "\n",
    "codes_df = pd.DataFrame(codes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844f197-295f-4b26-8fc3-1f2c91e403a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = mh.merge(codes_df, on=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49505ff6-4188-4634-992f-b7bc56cbd27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b8307-c557-43db-8b3a-49796d49a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later analysis\n",
    "combined.to_csv(f'llm_coded_sample_{model}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
