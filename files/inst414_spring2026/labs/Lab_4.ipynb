{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INST414 — Lab 4: Performance Metrics + Naive Bayes (with Pandas)\n",
    "\n",
    "**How this notebook works:**\n",
    "- Everything **before** the **Lab Tasks** section is your **pre-lab** work to complete at home.\n",
    "- In class, we’ll focus on the **Lab Tasks** and I’ll call on people to walk through solutions.\n",
    "\n",
    "**Today’s goal:** compute performance metrics as **conditional probabilities**, see how metrics change when you change the **threshold**, and then use **Naive Bayes** to make a simple classification.\n",
    "\n",
    "## Learning goals\n",
    "By the end, you should be able to:\n",
    "- Turn predicted probabilities into predictions using a threshold.\n",
    "- Explain the confusion-matrix cells (TP, FP, TN, FN).\n",
    "- Compute a joint distribution of `( y , \\hat{y} )` with `value_counts`.\n",
    "- Compute **TPR (recall)** and **PPV (precision)** from data.\n",
    "- Use `sort_values`, `between`, and `map` for common data tasks.\n",
    "- Compare two Naive Bayes scores and classify.\n",
    "\n",
    "## How to work in this notebook\n",
    "- Run cells top-to-bottom. If something errors, re-run from the last successful cell.\n",
    "- When you see **Checkpoint** prompts, pause and try before scrolling.\n",
    "- After any big step, do a quick check with `.shape`, `.head()`, and `value_counts()`.\n",
    "\n",
    "## Common issues (quick fixes)\n",
    "- If you get a `KeyError`, check column names with `lab4_data.columns`.\n",
    "- If your probabilities look weird, double-check the denominator (what you’re conditioning on).\n",
    "- If `value_counts` output is hard to read, use `.sort_index()` to organize it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v7ti5o2s13",
   "source": "**Before you start:** Click **File → Save a copy in Drive** so you have your own version of this notebook. If you skip this step, your work will not be saved.\n\n**Turn off AI assistance:** Go to **Settings → AI Assistance** and uncheck everything. AI-generated code is not allowed on assignments in this course.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules and settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef4fa87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T19:39:41.828087Z",
     "start_time": "2022-09-19T19:39:41.818897Z"
    }
   },
   "outputs": [],
   "source": [
    "# first thing is to import pandas\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-lab (do before class)\n",
    "\n",
    "We’ll build the core workflow in a small example first, then repeat it on the Maryland dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Warm-up: metrics on a tiny table\n",
    "\n",
    "We’ll treat:\n",
    "- `y` as the true outcome (0/1)\n",
    "- `prediction` as a model’s predicted probability for `y = 1`\n",
    "\n",
    "Then we’ll create ŷ by choosing a threshold and compute TPR and PPV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: From a probability to a predicted label\n",
    "\n",
    "A model gives a number like `prediction = 0.73`. Think of it as: \n",
    "> the model’s estimated probability that `y = 1`\n",
    "\n",
    "To make a yes/no prediction, we pick a **threshold** `t` and define:\n",
    "- predict $$\\hat{y} = 1$$ if `prediction ≥ t`\n",
    "- predict $$\\hat{y} = 0$$ if `prediction < t`\n",
    "\n",
    "Changing `t` changes how many people we predict as positive, which changes the error types we make.\n",
    "\n",
    "### Step 1: Confusion matrix (four outcomes)\n",
    "\n",
    "Once you have `y` and $$\\hat{y}$$, every row is one of:\n",
    "- **TP**: `y = 1` and $$\\hat{y} = 1$$\n",
    "- **FP**: `y = 0` and $$\\hat{y} = 1$$\n",
    "- **TN**: `y = 0` and $$\\hat{y} = 0$$\n",
    "- **FN**: `y = 1` and $$\\hat{y} = 0$$\n",
    "\n",
    "In Pandas, `value_counts` on two columns is a fast way to compute those counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy = pd.DataFrame({\n",
    "    'prediction': [0.95, 0.80, 0.73, 0.68, 0.60, 0.55, 0.52, 0.49, 0.30, 0.18],\n",
    "    'y':          [1,    0,    1,    1,    0,    1,    0,    0,    0,    0],\n",
    "})\n",
    "df_toy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy table:\n",
    "- `y = 1` means the outcome happened.\n",
    "- `prediction` is the model’s score (higher means the model thinks `y = 1` is more likely).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy['yhat'] = df_toy['prediction'] >= 0.5\n",
    "df_toy['yhat01'] = df_toy['yhat'].map({True: 1, False: 0})\n",
    "df_toy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what we just did:\n",
    "- `yhat` is `True/False` (a boolean).\n",
    "- `yhat01` converts that to `1/0` using `map`, which is convenient for counting and probability calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_counts_toy = df_toy[['y', 'yhat01']].value_counts().sort_index()\n",
    "joint_counts_toy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value_counts` on `['y', 'yhat01']` gives counts for each pair `( y , \\hat{y} )`.\n",
    "\n",
    "Interpretation example: `( 1 , 1 )` is the number of rows where the outcome is 1 and the prediction is 1 (that’s **TP**).\n",
    "\n",
    "We’ll use those four counts to compute TPR and PPV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Metrics as conditional probabilities\n",
    "\n",
    "- **TPR / Recall** = $$P ( \\hat{y} = 1 \\mid y = 1 ) = \\frac{TP}{TP + FN}$$\n",
    "- **PPV / Precision** = $$P ( y = 1 \\mid \\hat{y} = 1 ) = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Notice the denominators:\n",
    "- TPR conditions on **actual positives** (`y = 1`).\n",
    "- PPV conditions on **predicted positives** (`\\hat{y} = 1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = joint_counts_toy.get((1, 1), 0)\n",
    "FP = joint_counts_toy.get((0, 1), 0)\n",
    "TN = joint_counts_toy.get((0, 0), 0)\n",
    "FN = joint_counts_toy.get((1, 0), 0)\n",
    "\n",
    "TPR = TP / (TP + FN)\n",
    "PPV = TP / (TP + FP)\n",
    "\n",
    "TP, FP, TN, FN, TPR, PPV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** lower the threshold to 0.3. What happens to TPR? What happens to PPV?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern is:\n",
    "- Lower threshold → predict more positives → **TPR tends to go up** (you miss fewer true positives)\n",
    "- But you may also predict more false positives → **PPV can go down**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy['yhat01_03'] = (df_toy['prediction'] >= 0.3).map({True: 1, False: 0})\n",
    "joint_counts_toy_03 = df_toy[['y', 'yhat01_03']].value_counts().sort_index()\n",
    "TP = joint_counts_toy_03.get((1, 1), 0)\n",
    "FP = joint_counts_toy_03.get((0, 1), 0)\n",
    "FN = joint_counts_toy_03.get((1, 0), 0)\n",
    "\n",
    "TPR_03 = TP / (TP + FN)\n",
    "PPV_03 = TP / (TP + FP)\n",
    "\n",
    "joint_counts_toy_03, TPR_03, PPV_03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Maryland dataset\n",
    "\n",
    "`prediction` is a model score (a predicted probability of violent rearrest within 1 year).\n",
    "`outcome_violent_rearrest` is what actually happened (0/1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the columns mean (for this lab)\n",
    "\n",
    "- `outcome_violent_rearrest` is the true label `y` (0/1).\n",
    "- `prediction` is the model’s predicted probability that `y = 1`.\n",
    "\n",
    "Our workflow is the same as the toy example: choose a threshold, build \\hat{y}, then compute metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e86cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umd_id</th>\n",
       "      <th>outcome_violent_rearrest</th>\n",
       "      <th>prediction</th>\n",
       "      <th>n_vio_convictions_last_4yrs</th>\n",
       "      <th>n_vio_convictions_last_180days</th>\n",
       "      <th>n_vio_arrests_last_4yrs</th>\n",
       "      <th>n_vio_arrests_last_180days</th>\n",
       "      <th>age_at_arrest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5974815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.060274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5835352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.420702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.717808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7222541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.427397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5031868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.181424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.186301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3532116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.764384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    umd_id  outcome_violent_rearrest  prediction  n_vio_convictions_last_4yrs  \\\n",
       "0  5974815                       0.0    0.084093                          0.0   \n",
       "1  5835352                       1.0    0.420702                          0.0   \n",
       "2  7222541                       0.0    0.124549                          0.0   \n",
       "3  5031868                       1.0    0.181424                          0.0   \n",
       "4  3532116                       0.0    0.114881                          0.0   \n",
       "\n",
       "   n_vio_convictions_last_180days  n_vio_arrests_last_4yrs  \\\n",
       "0                             0.0                      0.0   \n",
       "1                             0.0                      0.0   \n",
       "2                             0.0                      0.0   \n",
       "3                             0.0                      0.0   \n",
       "4                             0.0                      0.0   \n",
       "\n",
       "   n_vio_arrests_last_180days  age_at_arrest  \n",
       "0                         0.0      35.060274  \n",
       "1                         0.0      19.717808  \n",
       "2                         0.0      56.427397  \n",
       "3                         0.0      25.186301  \n",
       "4                         0.0      28.764384  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab4_data = pd.read_csv('https://www.dropbox.com/scl/fi/0nhbo32tdw7mi2v2g0vd0/lab5_dataset.csv?rlkey=e0oevkrciv91r53rricdesq00&dl=1')\n",
    "lab4_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ad9d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab4_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll mostly work with three columns for the metrics part:\n",
    "- `y`\n",
    "- `prediction`\n",
    "- `yhat01`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab4_data['y'] = lab4_data['outcome_violent_rearrest']\n",
    "lab4_data['yhat'] = lab4_data['prediction'] >= 0.5\n",
    "lab4_data['yhat01'] = lab4_data['yhat'].map({True: 1, False: 0})\n",
    "\n",
    "lab4_data[['y', 'yhat01']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_counts = lab4_data[['y', 'yhat01']].value_counts().sort_index()\n",
    "joint_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the toy example, `joint_counts` contains the four confusion-matrix counts (TP, FP, TN, FN), indexed by `( y , \\hat{y} )`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = joint_counts.get((1, 1), 0)\n",
    "FP = joint_counts.get((0, 1), 0)\n",
    "FN = joint_counts.get((1, 0), 0)\n",
    "\n",
    "TPR = TP / (TP + FN)\n",
    "PPV = TP / (TP + FP)\n",
    "\n",
    "TPR, PPV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 — New Pandas functions we’ll use\n",
    "\n",
    "These are three small functions that show up constantly in data science work:\n",
    "\n",
    "- `sort_values`: sort a DataFrame by a column (we’ll rank people by predicted risk).\n",
    "- `between`: create a boolean for whether a value falls in a range (we’ll define an age group).\n",
    "- `map`: recode values (we’ll convert `True/False` into `1/0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_values: rank people by predicted risk (highest first)\n",
    "ranked = lab4_data.sort_values('prediction', ascending=False)\n",
    "ranked[['prediction', 'y']].head()\n",
    "\n",
    "# Example: PPV among the top 100 highest-risk people\n",
    "ppv_top_100 = ranked.head(100)['y'].mean()\n",
    "ppv_top_100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea above is the same as the in-class “top 500” task — it’s just using a different K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between example\n",
    "lab4_data['young_adult'] = lab4_data['age_at_arrest'].between(18, 25)\n",
    "lab4_data['young_adult01'] = lab4_data['young_adult'].map({True: 1, False: 0})\n",
    "lab4_data[['age_at_arrest', 'young_adult', 'young_adult01']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`between(18, 25)` is often nicer than writing two conditions with `&`.\n",
    "\n",
    "It returns `True/False`. In this course we often map that to `1/0` so we can treat it like an indicator variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map example\n",
    "pd.Series([True, False, True]).map({True: 1, False: 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` is a general recoding tool. We’ll use it mostly to turn booleans into `1/0`, but you can also map categories (like `'M'` → `'Male'`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Tasks (in class)\n",
    "\n",
    "You should come to class having completed the pre-lab above.\n",
    "In class, we’ll work through these tasks together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Changing the threshold changes the metrics\n",
    "\n",
    "1) Compute the base rate `base_rate = P ( y = 1 )`.\n",
    "\n",
    "Then create `yhat_new` that is 1 if `prediction >= base_rate` and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute TPR and PPV for `yhat_new`, and compare them to the threshold-0.5 values you computed in the pre-lab.\n",
    "\n",
    "In 1–2 sentences: why might TPR go up while PPV goes down?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Rank-based thresholding (top K)\n",
    "\n",
    "3) Sort `lab4_data` by `prediction` from highest to lowest.\n",
    "\n",
    "Compute the PPV among the **top 500** rows (treat “top 500” as being predicted positive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Naive Bayes classification\n",
    "\n",
    "We’ll classify based on two features:\n",
    "- `young_adult` (18–25 inclusive)\n",
    "- `vio_conv_gt1` meaning `n_vio_convictions__last_4_years > 1`\n",
    "\n",
    "4) Create `vio_conv_gt1` and convert it to 0/1 if helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Compute the pieces you need for the `y = 1` score and compute:\n",
    "\n",
    "`score_1 = P ( y = 1 ) * P ( young_adult = 1 \\mid y = 1 ) * P ( vio_conv_gt1 = 1 \\mid y = 1 )`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Compute `score_0` similarly, then classify as `y = 1` if `score_1 > score_0` and `y = 0` otherwise.\n",
    "\n",
    "In 1 sentence: what does Naive Bayes assume when it multiplies those conditional probabilities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "5ade917c15395e7ed88b1e8a3f8554ff754a526a8e0b703e5d887afe50fc6c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}